---
layout: page
title: ハルシネーションとは？生成AIの限界と正しい向き合い方【DS検定】
permalink: /ds/hallucination/
tags: [ds, ai, business]
---

## まず結論

ハルシネーションとは、**生成AIが事実ではない内容を、あたかも正しいかのように出力してしまう現象**です。  
DS検定では「AIの限界を理解し、出力を鵜呑みにしない姿勢があるか」を問われます。

---

## 直感的な説明

とても自信ありげに話している人が、実は間違ったことを言っている――  
そんな場面を想像してください。

生成AIも同じです。

もっともらしい文章で回答してくれますが、  
**間違っていても“それらしく”答えてしまう**ことがあります。

例えば：

- 存在しない論文を引用する
- 架空の統計データを出す
- 実在しない法律条文を説明する

見た目では正誤が判断できない。  
これがハルシネーションの怖さです。

---

## 定義・仕組み

ハルシネーション（Hallucination）は、

> 大規模言語モデル（LLM）が、事実と異なる内容を自然な文章として生成してしまう現象

を指します。

### なぜ起きるのか？

生成AIは「正解を知っている」のではありません。

- 過去の大量データから
- 「次に来そうな単語」を確率的に予測している

つまり、

**意味理解ではなく、確率的な文章生成**をしているのです。

そのため：

- 学習データにない内容
- あいまいな質問
- 情報が不足している状況

では、もっともらしい推測をしてしまいます。

DS検定では、
「AIは誤りを根本的に避けられない」という前提を理解しているかが重要です。

---

## どんな場面で使う？

### 重要になる場面

- 生成AIを業務で利用する場合
- レポート作成をAIに補助させる場合
- AI出力をそのまま顧客に提示する場合

特にビジネスでは、

> 「AIが言ったから正しい」は通用しない

という判断が求められます。

### 適切な対応例

- 検索エンジンで裏取りする
- 他のLLMの出力と比較する
- 正確な追加情報を入力して再生成する
- 人間が最終確認する

DS検定では  
「ハルシネーションが起きたとき、どう対応するか」も問われます。

---

## よくある誤解・混同

### ❌ AIは大量データで学習しているから間違えない  
→ ⭕ 学習していても、事実保証はできない

---

### ❌ ハルシネーションはバグである  
→ ⭕ 構造的に避けられない特性である

---

### ❌ 精度が高いモデルなら起きない  
→ ⭕ 高性能モデルでも発生する

---

DS検定では、

- 「AIは誤りを出力しない」
- 「AIは常に正確である」

といった選択肢は、ほぼ誤りです。

選択肢では  
「出力を確認せず利用してよい」と書かれていたら注意です。

---

## まとめ（試験直前用）

- ハルシネーション＝AIが事実と異なる内容を生成する現象
- 原因は確率的文章生成の仕組み
- 根本的にゼロにはできない
- 対策は「検証・比較・人間確認」
- DS検定では“AIを盲信しない判断”が問われる

---

【対応スキル項目（AI利活用スキルシート）】

- アプローチ設計
- 生成AI活用
- ★ 大規模言語モデルにおいては、事実と異なる内容がさも正しいかのように生成されることがあること（Hallucination）、これらが根本的に避けることができないことを踏まえ、利用に際しては出力を鵜呑みにしない等の注意が必要であることを知っている
- ★ Hallucinationが起きていることに気づくための適切なアクションをとることができる（検索等によるリサーチ結果との比較や、他LLMの出力結果との比較、正確な追加情報を入力データに付与することによる出力結果の変化比較など）
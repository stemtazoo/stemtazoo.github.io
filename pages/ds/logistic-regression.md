---
layout: page
title: ロジスティック回帰とは？（オッズ・対数オッズから理解する）【DS検定】
permalink: /ds/logistic-regression/
tags: [ds, statistics, ai]
---

## まず結論

ロジスティック回帰とは、「ある事象が起こる確率」を予測するための分類手法です。  
DS検定では「回帰という名前だが分類に使う」という点と、「オッズ・対数オッズ・シグモイド関数」の関係を理解しているかが問われます。

---

## 直感的な説明

たとえば、

- この顧客は商品を買うか？
- このメールは迷惑メールか？
- この取引は不正か？

のように、「Yes / No」を予測したい場面があります。

ここで問題になるのは、  
**予測結果を“確率”として出したい**という点です。

単純な直線（通常の回帰）だと、  
予測値がマイナスになったり、1を超えたりしてしまいます。

そこで、

1. まず「確率」を「オッズ」に変換する  
2. さらに「対数」を取って直線で扱える形にする  
3. 最後にシグモイド関数で0〜1に戻す  

という流れを使います。

この流れ全体がロジスティック回帰です。

---

## 定義・仕組み

### ① オッズとは？

確率を p とすると、オッズは

$$
p / (1 - p)
$$

です。

意味は  
「起こる確率 ÷ 起こらない確率」

です。

たとえば、

- 起こる確率 0.8
- 起こらない確率 0.2

なら、オッズは 4 です。  
「4倍起こりやすい」という解釈になります。

---

### ② 対数オッズ（ロジット）

オッズに対数を取ったものを対数オッズといいます。

$$
log(p / (1 - p))
$$

これをロジット関数と呼びます。

なぜ対数を取るのか？

- オッズは 0〜∞
- 対数を取ると −∞〜＋∞

つまり、  
**直線モデルで扱える形に変換できる**のです。

---

### ③ シグモイド関数

ロジットの逆関数がシグモイド関数です。

$$
σ(x) = 1 / (1 + e^{-x})
$$

この関数の特徴は、

- 出力が必ず 0〜1 の範囲
- S字カーブになる
- 中心は 0.5

です。

ロジスティック回帰は、

- 入力を直線で計算
- それをシグモイド関数に通す
- 確率に変換する

という仕組みになっています。

DS検定では  
「ロジスティック回帰とシグモイド関数の関係」が問われることが多いです。

---

## どんな場面で使う？

### 使う場面

- 二値分類（Yes / No）
- 発生確率を出したいとき
- ビジネスで意思決定に使うとき

例：

- 購買確率
- 解約確率
- 不正発生確率

確率として出るため、  
**優先順位付けやリスク管理に使いやすい**のが特徴です。

---

### 使うと誤解しやすい場面

- 数値予測（売上金額など）には向かない
- 多クラス分類ではそのまま使えない（拡張が必要）

DS検定では  
「連続値予測に使う」と書いてあれば誤りです。

---

## よくある誤解・混同

### ① 回帰だから数値予測？

名称に「回帰」が入っていますが、  
実際は分類手法です。

DS検定ではここをよく混同させてきます。

---

### ② シグモイド関数＝ロジスティック回帰？

厳密には、

- シグモイド関数は“変換関数”
- ロジスティック回帰は“モデル全体”

です。

「シグモイド関数そのものが学習モデル」と書いてあれば誤りです。

---

### ③ オッズと確率の混同

- 確率は 0〜1
- オッズは 0〜∞

DS検定では  
「オッズは確率そのもの」と書いてあれば誤りです。

---

## まとめ（試験直前用）

- ロジスティック回帰は二値分類の手法
- 確率 → オッズ → 対数オッズで直線化する
- シグモイド関数で 0〜1 に戻す
- 「回帰」という名前だが分類で使う

迷ったら、

**確率を直線で扱うための変換モデル**

と覚えておくと判断できます。

---

【対応スキル項目（AI利活用スキルシート）】
- AIを理解する力
- 機械学習の基礎理解
- ★ 代表的な機械学習手法の特徴を理解している

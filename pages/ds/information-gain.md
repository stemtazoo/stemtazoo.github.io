---
layout: page
title: 情報利得とは？分岐の良さを判断する基準【DS検定】
permalink: /ds/information-gain/
tags: [ds, ai, evaluation]
---

## まず結論
情報利得とは、**ある特徴量で分岐したときに「どれだけ不確実性が減ったか」を表す指標**です。  
DS検定では、「どの特徴量で分けるのが良いかを判断する基準は何か？」を問う問題で使われます。

---

## 直感的な説明

たとえば、  
「売れる商品」と「売れない商品」を分類したいとします。

ある特徴量（例：価格帯）で分けたら、
片方はほぼ「売れる」だけ、もう片方はほぼ「売れない」だけになった。

→ これは**とても良い分け方**です。

なぜなら、分けた後は「ほぼ答えが決まっている」状態だからです。  
この「分けたことでどれだけスッキリしたか」を数値にしたものが**情報利得**です。

---

## 定義・仕組み

情報利得は、

- 分岐する前の「不確実さ」
- 分岐した後の「不確実さ」

の差で決まります。

不確実さには「エントロピー」という指標を使います。

難しく考えなくて大丈夫です。  
ポイントは次の1行です。

> **情報利得が大きい = 分けた結果、クラスがはっきりした**

つまり、

- 情報利得が大きい → 良い分岐
- 情報利得が小さい → 分けてもあまり意味がない

という判断になります。

DS検定では「情報利得が大きい特徴量を選ぶ」と理解していれば十分です。

---

## どんな場面で使う？

### ① 決定木の分岐基準
最も代表的な用途です。

決定木では、
「どの特徴量で分岐するか？」を決める必要があります。

このとき、

> 情報利得が最大になる特徴量を選ぶ

というルールが使われます。

---

### ② ランダムフォレストの内部理解

ランダムフォレストは決定木の集合です。  
それぞれの木の中で、情報利得が使われています。

DS検定では、

- 「分岐の基準は何か？」
- 「特徴量の重要度はどう決まるか？」

と問われることが多いです。

---

## よくある誤解・混同

### ❌ 情報利得が小さいほど重要
→ 逆です。**大きいほど良い分岐**です。

DS検定ではここをひっかけてきます。

---

### ❌ 情報利得 = 特徴量重要度
→ 完全に同じではありません。

情報利得は「1回の分岐」の評価です。  
特徴量重要度は「木全体・森全体での貢献度」です。

---

### ❌ 情報利得が高い = 因果関係がある
→ これは誤りです。

あくまで「分類に役立つ」だけで、
原因とは限りません。

---

### ❌ Gini不純度と同じもの
→ 似ていますが別物です。

- 情報利得 → エントロピーを使う
- Gini → ジニ不純度を使う

どちらも「どれだけ混ざっているか」を測る指標です。

DS検定では「分岐の基準として使われる指標」として整理しておきましょう。

---

## まとめ（試験直前用）

- 情報利得 = 分岐でどれだけ不確実性が減ったか
- 大きいほど良い分岐
- 決定木の分岐基準として使われる
- 因果を示す指標ではない
- Gini不純度と混同しない

---

## 対応スキル項目（AI利活用スキルシート）
- AIの理解
- 機械学習の基本理解
- ★ 機械学習モデルの基本的な仕組みを理解している

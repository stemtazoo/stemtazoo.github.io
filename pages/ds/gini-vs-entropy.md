---
layout: page
title: ジニ不純度とエントロピーの違いとは？分岐基準を整理【DS検定】
permalink: /ds/gini-vs-entropy/
tags: [ds, ai, evaluation]
---

## まず結論
ジニ不純度とエントロピーは、**どちらも「クラスの混ざり具合」を測る指標**です。  
DS検定では、「どちらが分岐基準として使われるか」「何を比較しているのか」を判断させる問題が出ます。

---

## 直感的な説明

どちらも考えていることは同じです。

- 1種類だけ → はっきりしている（良い）
- 半々に混ざる → 迷う（悪い）

つまり、

> 混ざりが大きいほど値が大きくなる  
> きれいに分かれるほど値が小さくなる  

この点は共通です。

違うのは「計算方法」です。

---

## 定義・仕組み

### ジニ不純度（Gini impurity）

- ランダムフォレストでよく使われる
- 計算が比較的シンプル
- 実務では標準設定で使われることが多い

考え方：
「ランダムに1つ選んだときに、間違う確率」

---

### エントロピー（Entropy）

- 情報理論ベース
- 情報利得の計算に使われる
- ID3アルゴリズムで有名

考え方：
「どれだけ不確実か（どれだけ迷うか）」

---

### 共通点

- 混ざるほど大きくなる
- 1クラスのみで最小
- 分岐評価に使われる

### 違い（DS検定で問われやすい）

| 観点 | ジニ不純度 | エントロピー |
|------|------------|--------------|
| ベース理論 | 確率的な誤分類 | 情報理論 |
| 主な用途 | ランダムフォレスト | 情報利得計算 |
| 計算コスト | やや軽い | やや重い |

ただし、実務では結果はほぼ似ることが多いです。

---

## どんな場面で使う？

### ① 決定木の分岐基準

どちらも「どの特徴量で分けるか」を決めるために使います。

DS検定では

- 「分岐基準として用いられる指標はどれか？」
- 「情報利得の計算に使われるのはどれか？」

といった形で問われます。

---

### ② 特徴量重要度の理解

ランダムフォレストでは、

> 不純度をどれだけ減らしたか

の合計が重要度になります。

このとき使われるのは通常ジニ不純度です。

---

## よくある誤解・混同

### ❌ エントロピーのほうが常に優れている
→ そんなことはありません。

DS検定では「どちらも分岐基準」と整理できれば十分です。

---

### ❌ ジニ不純度と情報利得は同じ
→ 違います。

- ジニ不純度 → 混ざり具合そのもの
- 情報利得 → エントロピーの減少量

ここは頻出の混同ポイントです。

---

### ❌ 値が大きいほうを選ぶ
→ 分岐後は「小さいほう」が良い状態です。

選択肢では  
「不純度が最大となる分岐を選ぶ」  
と書かれていたら誤りです。

---

## まとめ（試験直前用）

- 両方とも「混ざり具合」を測る指標
- 小さいほど良い状態
- ジニ不純度は実務でよく使われる
- エントロピーは情報利得とセット
- どちらも分岐基準として使われる

---

## 対応スキル項目（AI利活用スキルシート）
- AIの理解
- 機械学習の基本理解
- ★ 機械学習モデルの基本的な仕組みを理解している

---
layout: page
title: エントロピーとは？不確実さを測る指標【DS検定】
permalink: /ds/entropy/
tags: [ds, ai, evaluation]
---

## まず結論
エントロピーとは、**データの不確実さ（どれだけ混ざっているか）を数値で表す指標**です。  
DS検定では、「分岐の良さをどう判断するか？」という問題で使われます。

---

## 直感的な説明

たとえば、あるクラスに

- 全員「合格」だけ
- 合格と不合格が半々

この2つを比べると、

- 全員同じ → 予測は簡単（迷わない）
- 半々 → 予測は難しい（迷う）

この「どれだけ迷うか」を数値にしたものがエントロピーです。

> エントロピーが小さい = ほぼ決まっている  
> エントロピーが大きい = かなり混ざっている  

と理解しておけば十分です。

---

## 定義・仕組み

エントロピーは、  
**クラスの割合がどれだけ均等か**で決まります。

- 1種類だけ → 最小
- 均等に混ざる → 最大

決定木では、

> 分岐によってエントロピーがどれだけ減ったか

を使って、どの特徴量で分けるかを決めます。

この「減少量」が **情報利得** です。

つまり関係はこうです。

- エントロピー → 今の混ざり具合
- 情報利得 → 分岐でどれだけ混ざりが減ったか

DS検定ではこの関係を整理できているかが重要です。

---

## どんな場面で使う？

### ① 決定木（ID3など）の分岐基準

エントロピーは、決定木アルゴリズムの  
分岐評価に使われます。

「どの特徴量で分けると、よりはっきり分かれるか？」

を判断する材料です。

---

### ② 情報利得の計算

エントロピーは単体で問われることもありますが、  
多くは「情報利得」とセットで出題されます。

DS検定では

- エントロピーが小さい状態はどれか？
- 情報利得が最大になるのはどれか？

といった形で問われることが多いです。

---

## よくある誤解・混同

### ❌ エントロピーが大きいほど良い分岐
→ 逆です。

分岐後のエントロピーは  
**小さいほど良い状態**です。

---

### ❌ エントロピー = ジニ不純度
→ 似ていますが別物です。

どちらも「混ざり具合」を測りますが、
計算方法が違います。

DS検定では

> 「分岐基準の1つ」

と整理できればOKです。

---

### ❌ エントロピーが高い = モデル精度が高い
→ 関係ありません。

エントロピーはあくまで  
「ノード内の混ざり具合」を表すだけです。

---

## まとめ（試験直前用）

- エントロピー = 不確実さ（混ざり具合）
- 均等に混ざるほど大きい
- 1種類だけだと最小
- 決定木の分岐評価に使う
- 情報利得 = エントロピーの減少量

---

## 対応スキル項目（AI利活用スキルシート）
- AIの理解
- 機械学習の基本理解
- ★ 機械学習モデルの基本的な仕組みを理解している

---
layout: page
title: 中間層で使用される活性化関数とは？（ReLU・シグモイド・ソフトマックスの違い）【DS検定】
permalink: /ds/activation-functions-hidden-layer/
tags: [ds, ai]
---

## まず結論

中間層で使用される活性化関数とは、**ニューラルネットワークに「非線形性」を与えるための関数**です。  
DS検定では「どの関数が中間層向きか」を判断させる問題がよく出ます。

---

## 直感的な説明

ニューラルネットワークは、

入力 → 中間層 → 出力層

という構造になっています。

ここで中間層の役割は、  
**単純な直線では表せない複雑なパターンを表現すること**です。

もし中間層がすべて「線形（直線）」の変換だけなら、

- 何層あっても
- 深くしても

結局は「1回の直線変換」と同じになります。

だからこそ、中間層には  
**曲がった関数（非線形関数）**が必要になります。

---

## 定義・仕組み

活性化関数（Activation Function）とは、  
各層の出力をどのように変換するかを決める関数です。

### 中間層でよく使われるもの

- ReLU（レルー）
- シグモイド関数（現在は減少傾向だが歴史的に使用）

これらは「非線形」なので、  
モデルに複雑な表現力を持たせることができます。

### 中間層で基本的に使わないもの

- ソフトマックス関数

ソフトマックスは、  
**出力を確率（合計1）に変換する関数**です。

そのため、通常は  
**多クラス分類の出力層で使われます。**

---

## どんな場面で使う？

### 中間層

- ReLUが現在の主流
- 以前はシグモイドも利用されていた

### 出力層

- 二値分類 → シグモイド
- 多クラス分類 → ソフトマックス
- 回帰問題 → 線形関数

DS検定では、

「どの層でどの関数が使われるか」

を整理しておくことが重要です。

---

## よくある誤解・混同

### ① 線形関数は中間層では使えない？

理論上は使えます。  
しかしすべて線形にすると、層を重ねる意味がなくなります。

そのため、**実質的には非線形が必要**です。

### ② シグモイドは中間層では不適切？

現在はReLUが主流ですが、  
歴史的には中間層でも使われてきました。

DS検定では  
「絶対に使われないもの」を選ばせる問題が多いです。

この場合、より明確に不適切なのは  
**ソフトマックス（出力層専用）**です。

### ③ ソフトマックスは万能な活性化関数？

違います。  
確率に変換するための関数なので、中間層には通常使いません。

選択肢では  
「中間層でも一般的に使われる」と書かれていたら注意です。

---

## まとめ（試験直前用）

- 中間層には「非線形関数」が必要  
- ReLUは現在の主流  
- シグモイドも歴史的に使用されてきた  
- ソフトマックスは基本的に出力層専用  
- 「最も不適切」を選ばせる問題では、役割の違いで切る  

---

【対応スキル項目（AI利活用スキルシート）】
- AIを理解する力
- モデルの基本構造を理解している
- ★ AIの仕組みや基本構造を理解している

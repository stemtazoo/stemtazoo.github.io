---
layout: page
title: Embedding層とは？【単語IDを意味ベクトルに変換｜G検定対策】
permalink: /gk/embedding-layer/
tags: [gk, nlp, neural_network]
---

## まず結論
- **Embedding層**とは、単語IDなどの離散的な情報を、意味を持つベクトル（分散表現）に変換する層である。
- G検定では「**One-hotとの違い**」「**学習で更新されるか**」がよく問われる。

## 直感的な説明
Embedding層は、  
**単語に“意味の座標”を割り当てる翻訳機**のようなものです。

- 入力：単語ID（番号）
- 出力：意味を表すベクトル

👉 **番号 → 意味** に変換するのが役割。

## 定義・仕組み
- 単語は通常、ID（整数）として入力される
- Embedding層は  
  - 各IDに対応するベクトルを内部に持つ
  - IDに対応する行を取り出す

👉 **巨大な行列から1行を参照する処理**

### 数式イメージ（理解用）
- One-hot × 重み行列 ＝ 埋め込みベクトル  
（※G検定では数式理解不要）

## いつ使う？（得意・不得意）
### 得意な点
- 次元削減（One-hotより低次元）
- 意味的類似性を表現できる
- 学習によって改善される

### 注意点
- 初期値はランダムなことが多い
- 学習データに依存する
- 意味理解そのものではない（文脈は別）

## G検定ひっかけポイント
ここが頻出 👇

### ❌ Embedding層は単語の出現回数を扱う
- **誤り**
- それは BoW / TF-IDF

### ❌ Embedding層は固定で学習されない
- **誤り**
- Embedding層の重みは **学習で更新される**

### ⭕ 正しい判断基準
- 「IDをベクトルに変換」→ Embedding層
- 「意味的に近い単語が近くなる」→ Embedding層
- 「語彙数×次元の行列」→ Embedding層

## 他手法との関係
- One-hot：入力表現
- Embedding層：One-hotを低次元化
- Word2Vec：Embeddingを学習する方法
- BERT：Embedding＋文脈処理

👉 **Embedding層は“部品”**  
👉 **Word2Vec / BERTは“モデル”**

## まとめ（試験直前用）
- Embedding層＝ID → 意味ベクトル
- One-hotより低次元
- 重みは学習で更新
- 意味の近さを表現できる
- 文脈理解はしない

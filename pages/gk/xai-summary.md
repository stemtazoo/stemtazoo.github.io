---
layout: page
title: XAIまとめ（LIME・SHAP・CAM・Grad-CAM）
permalink: /gk/xai-summary/
tags: [gk, xai, neural_network, cheatsheet]
gk_section: ディープラーニングの応用例/モデルの解釈性
gk_order: 6
---

## まず結論

* **XAI（Explainable AI）は「なぜその判断か」を説明するための技術**
* **モデルの精度を上げるものではない**
* 手法ごとに「対象モデル」と「説明の仕方」が違う

---

## 直感的な説明

XAIは一言でいうと、

> **AIの考えを、人が理解できる形に翻訳する技術**

です。

G検定では特に、

* ブラックボックス問題
* 信頼性・説明責任
* 医療・金融・自動運転

とセットで問われます。

---

## XAIが必要な理由

* なぜその判断になったか分からないと  
  → **使えない・責任が取れない**
* 法規制・説明責任への対応
* バイアス・誤判断の検出

---

## 主なXAI手法の整理（最重要）

### 一覧比較表

| 手法 | 主な対象 | 特徴 | G検定頻度 |
|---|---|---|---|
| LIME | 任意モデル | 局所的・近似モデル | 高 |
| SHAP | 任意モデル | 寄与度を数値化 | 高 |
| CAM | CNN | GAP必須・構造依存 | 中 |
| Grad-CAM | CNN | 勾配利用・汎用 | 高 |

---

## 各手法のポイント

### LIME（Local Interpretable Model-agnostic Explanations）

* 任意のモデルに使える
* 対象データの**周辺を擬似的に線形モデルで近似**
* **局所的な説明**が得意

👉  
「この1件の判断を説明」

---

### SHAP（SHapley Additive exPlanations）

* ゲーム理論（シャープレイ値）に基づく
* 各特徴量の**寄与度を数値で表現**
* 理論的に一貫性がある

👉  
「どの特徴がどれくらい効いたか」

---

### CAM（Class Activation Map）

* CNN専用
* **GAPが必須**
* 特徴マップとクラスの対応を可視化

👉  
「構造に依存」

---

### Grad-CAM

* CNN専用
* **勾配情報を利用**
* 既存モデルにも適用可能

👉  
「CAMの進化版」

---

## どれを使う？（選び方）

* モデルを選ばず説明したい  
  → **LIME / SHAP**
* 画像分類CNNの判断根拠  
  → **CAM / Grad-CAM**
* 実務・実験で汎用  
  → **Grad-CAM / SHAP**

---

## G検定ひっかけポイント

### ❌ よくある誤解

* ❌ **「XAIは精度を上げる」**
* ❌ **「すべてのXAIはCNN専用」**
* ❌ **「GAPがあればGrad-CAMが必須」**

---

### ✅ 正しい理解

* XAIは **説明のため**
* モデルはそのまま
* 手法ごとに適用範囲が違う

---

## 試験での即断キーワード

* 「ブラックボックス」
* 「説明責任」
* 「なぜその判断か」

👉 **XAI**

---

## まとめ（試験直前用）

* XAI = 判断理由の説明
* 精度向上ではない
* LIME / SHAP：汎用
* CAM / Grad-CAM：CNN向け

👉 次は  
**AI倫理（公平性・透明性・説明責任）**  
とセットで完成します。

---
layout: page
title: Q学習と方策勾配法の違いとは？【G検定超重要対比】
permalink: /gk/q-learning-vs-policy-gradient/
tags: [gk, reinforcement_learning, neural_network]
gk_section: 機械学習の概要/代表的な手法/強化学習
gk_order: 18
---

## まず結論
**Q学習は「行動価値関数」を学習する手法**、  
**方策勾配法は「方策（行動ルール）」を直接学習する手法**である。  
G検定では「何を直接更新しているか」を見抜けるかが最大のポイント。

---

## 直感的な説明
まず考え方がまったく違う。

- **Q学習**  
  →「この行動、どれくらい得？」を数値で覚える  
- **方策勾配法**  
  →「この状況では、どの行動をどれくらいの確率で選ぶ？」を覚える  

つまり、

- Q学習＝**評価してから選ぶ**
- 方策勾配＝**選び方そのものを学ぶ**

ここを押さえると、問題文が一気に読みやすくなる。

---

## 定義・仕組み
### Q学習（Q-learning）
- 行動価値関数 **Q(s, a)** を学習
- 「状態sで行動aを取ると、どれだけ良いか」を数値で表す
- 最大のQ値を持つ行動を選択

👉 **価値ベース手法（Value-based method）**

---

### 方策勾配法（Policy Gradient）
- 方策 **π(a|s)** を直接最適化
- 行動を確率的に選択
- 勾配を使って「良い行動の確率を上げる」

👉 **方策ベース手法（Policy-based method）**

PPO・A2C・A3C はここに属する。

---

## いつ使う？（得意・不得意）
### Q学習が得意
- 行動が離散的
- 状態・行動空間が小さい
- シンプルな問題

### Q学習が苦手
- 連続行動空間
- 状態が複雑
- 不安定になりやすい

---

### 方策勾配法が得意
- 連続行動空間
- ロボット制御などの実問題
- 安定性が重要な場合

### 方策勾配法の注意点
- 分散が大きくなりやすい
- 安定化の工夫（PPOなど）が必要

---

## G検定ひっかけポイント
ここは**頻出の罠ゾーン**。

### よくある誤解
- ❌ PPOはQ学習の改良版  
- ❌ 方策勾配法は価値関数を使わない  
- ❌ Q学習も方策を直接更新する  

すべて **不正解**。

---

### 正しい判断基準（超重要）
- 「Q値を更新する」  
  → **Q学習**
- 「方策を直接最適化する」  
  → **方策勾配法**
- 「更新をクリッピングして安定化」  
  → **PPO**
- 「行動価値関数を直接更新」  
  → **Q学習系**

G検定では  
**“何を直接学習しているか”だけを見る**。

---

## まとめ（試験直前用）
- Q学習＝**行動価値関数を学習**
- 方策勾配法＝**方策を直接学習**
- PPOは方策勾配法の一種
- Q学習とPPOは別系統
- 「直接更新している対象」で切る

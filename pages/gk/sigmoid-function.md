---
layout: page
title: シグモイド関数（Sigmoid Function）とは？【G検定対策】
permalink: /gk/sigmoid-function/
tags: [gk, neural_network]
---

## まず結論
- **シグモイド関数は、入力を 0〜1 の範囲に変換する活性化関数**。
- G検定では **「出力範囲」「0入力時の値」「他の活性化関数との違い」**がよく問われる。

## 直感的な説明
- シグモイド関数は「**ONかOFFかをなだらかに決めるスイッチ**」。
- 入力が：
  - とても小さい → ほぼ 0
  - 0 のとき → 0.5
  - とても大きい → ほぼ 1
- グラフは **S字カーブ**。
- 「確率っぽい値」を出すのが得意。

## 定義・仕組み
- 数式：
  - σ(x) = 1 / (1 + e⁻ˣ)
- 性質：
  - 出力範囲：**0 < σ(x) < 1**
  - x = 0 のとき：**σ(0) = 0.5**
  - 出力の最大値：**1（に近づくが、1にはならない）**
  - 出力の最小値：**0（に近づくが、0にはならない）**
- 微分の性質：
  - 勾配の最大値は **0.25**
  - x = 0 のときに最大

## いつ使う？（得意・不得意）
**得意**
- 出力を確率として解釈したい場合
- 二値分類の出力層
- ロジスティック回帰

**不得意・注意**
- 勾配消失が起きやすい
- 隠れ層では ReLU 系が主流
- 出力が 0 中心ではない

## G検定ひっかけポイント
- **tanh と混同させる問題が多い**
- よくある誤解：
  - ❌ 出力の最小値は −1  
    → **それは tanh**
  - ❌ 出力は −1〜1  
    → **それも tanh**
- 正しい判断基準：
  - 出力が **0〜1** → シグモイド
  - 出力が **−1〜1** → tanh
- 選択肢での即断：
  - 「入力0で0.5」→ ⭕ シグモイド
  - 「微分最大0.25」→ ⭕ シグモイド
  - 「最小値−1」→ ❌

## まとめ（試験直前用）
- シグモイド関数＝**0〜1 に変換**
- 入力 0 → 出力 **0.5**
- 出力の最小値は **0**
- 微分の最大値は **0.25**
- **−1 が出てきたら tanh**

---
layout: page
title: NLP完全チートシート（最終まとめ）【G検定対策】
permalink: /gk/nlp-cheatsheet/
tags: [gk, nlp, cheatsheet]
---

## まず結論
- NLP（自然言語処理）は「**文章をどう数値化し、どう意味を扱うか**」が本質。
- G検定では「**頻度・意味・文脈**のどれを扱っているか」を見抜けば解ける。

---

## 直感的な全体像
NLP手法は、次の進化の流れで理解すると一発。

1. **数える**（BoW）
2. **重要度を付ける**（TF-IDF）
3. **意味を表す**（Word2Vec）
4. **文脈を理解する**（BERT）

👉 どこまで扱えるかで手法が決まる。

---

## 定義・仕組み（手法別まとめ）

### BoW（Bag of Words）
- 単語の**出現回数**で文章を表現
- 語順・意味は考慮しない

👉 「数えるだけ」

---

### TF-IDF
- BoWに**重要度**の重みを付与
- 文書内では多く、全体では少ない単語を重視

👉 「珍しくて重要な単語」

---

### One-hot表現
- 単語IDを **0/1 のベクトル**で表現
- 語彙数＝次元数、意味は持たない

👉 「番号札」

---

### 分散表現（Distributed Representation）
- 単語を **低次元ベクトル**で表現
- 意味的に近い単語は近い位置

👉 「意味を距離で表す」

---

### Word2Vec
- 単語の**意味的類似性**を学習
- 静的分散表現（文脈に依存しない）

#### CBOW / Skip-gram
- CBOW：周囲 → 中心
- Skip-gram：中心 → 周囲

👉 「単語の意味」

---

### Embedding層
- 単語ID → 意味ベクトル に変換
- 学習によって重みが更新される

👉 「IDを意味に変換する部品」

---

### Attention / Self-Attention
- 単語同士の**重要度**を計算
- 文脈理解の中核技術

👉 「どの単語を見るかを決める」

---

### Transformer
- Self-Attentionを中心とした構造
- 並列処理が可能

#### Encoder / Decoder
- Encoder：理解（双方向）
- Decoder：生成（一方向）

---

### BERT
- Transformer **Encoderのみ**
- 文脈を考慮した分散表現
- 意味理解が得意

👉 「文脈付き意味」

---

### GPT
- Transformer **Decoderのみ**
- 次単語予測による生成

👉 「文章生成」

---

## いつ使う？（使い分け早見）

| 目的 | 手法 |
|---|---|
| 単語数を使う | BoW |
| 単語の重要度 | TF-IDF |
| 単語の意味 | Word2Vec |
| 文脈理解 | BERT |
| 文章生成 | GPT |

---

## G検定ひっかけポイント総まとめ
ここを見れば切れます 👇

### ❌ よくある誤解
- TF-IDFは意味を理解する → ❌
- Word2Vecは文脈を理解する → ❌
- BoWは語順を考慮する → ❌
- BERTは出現回数を見る → ❌

### ⭕ 正しい判断基準
- 出現回数 → BoW
- 重要度 → TF-IDF
- 意味 → Word2Vec
- 文脈 → BERT
- 生成 → GPT

---

## まとめ（試験直前用）
- NLPは「数 → 重要度 → 意味 → 文脈」の進化
- BoW / TF-IDF：頻度系
- Word2Vec：意味系
- BERT：文脈系
- GPT：生成系
- **迷ったら「文脈を考慮するか？」で切る**

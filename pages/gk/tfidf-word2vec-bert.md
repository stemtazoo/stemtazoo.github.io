---
layout: page
title: TF-IDF・Word2Vec・BERTの違いとは？役割で整理するNLP基礎【G検定対策】
permalink: /gk/tfidf-word2vec-bert/
tags: [gk, nlp, neural_network]
---

## まず結論
- **TF-IDF・Word2Vec・BERTは「文章をどう数値化するか」のレベルが違うだけ**で、役割が明確に異なる。
- G検定では「**どこまで文脈を考慮できるか**」で切り分ける。

## 直感的な説明
3つを人間にたとえると、こうなります。

- **TF-IDF**：  
  👉 単語の「出現頻度」だけを見る人
- **Word2Vec**：  
  👉 単語の「意味の近さ」を考える人
- **BERT**：  
  👉 文全体の「文脈」を理解して読む人

つまり、
👉 **賢さの段階が違う**  
と考えると一気に整理できます。

## 定義・仕組み
### TF-IDF
- 単語の出現頻度に基づく特徴量
- 文脈・語順は考慮しない
- 古典的な手法

ポイント：
- シンプル
- 高速
- **意味は理解しない**

### Word2Vec
- 単語をベクトルとして表現
- 意味が似た単語は近いベクトルになる
- 文脈は「ある程度」考慮（局所的）

ポイント：
- 単語の意味を捉える
- 文全体の理解はできない
- 単語単位の表現

### BERT
- Transformerベースの言語モデル
- **双方向**に文脈を理解
- 文全体を考慮した表現を生成

ポイント：
- 文脈理解が可能
- 事前学習＋微調整
- 現在のNLPの中心技術

## いつ使う？（得意・不得意）
### TF-IDFが向く場面
- シンプルな文章分類
- 計算コストを抑えたい場合
- ベースラインモデル

### Word2Vecが向く場面
- 単語の意味類似度計算
- 特徴量として単語ベクトルを使いたい場合

### BERTが向く場面
- 感情分析
- 質問応答
- 文脈理解が重要なタスク全般

## G検定ひっかけポイント
G検定では、次の混同を狙ってきます。

### よくある誤解
- ❌「TF-IDFは意味を理解する」
- ❌「Word2Vecは文全体を理解する」
- ❌「BERTは単なる特徴量手法」

### 正しい判断基準
- **頻度ベース → TF-IDF**
- **意味の近さ → Word2Vec**
- **文脈理解 → BERT**

問題文に  
「出現頻度」「重み付け」  
とあれば **TF-IDF**。

「意味が近い単語」  
とあれば **Word2Vec**。

「文脈」「双方向」「事前学習モデル」  
とあれば **BERT**。

## まとめ（試験直前用）
- TF-IDF：頻度だけ
- Word2Vec：単語の意味
- BERT：文脈理解
- 文脈をどこまで扱うかで切る
- 賢さの段階で覚える

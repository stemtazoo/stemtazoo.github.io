---
layout: page
title: モデル圧縮まとめ（Pruning・Quantization・Distillation）
permalink: /gk/model-compression/
tags: [gk, model_compression, pruning, quantization, distillation]
---

## まず結論

* **モデル圧縮**は「軽く・速く・省メモリ」にするための技術群
* G検定で重要なのは **目的・手法・違い** を区別できること
* 代表的な手法は **Pruning / Quantization / Knowledge Distillation**

---

## 直感的な説明

モデル圧縮は、

> 「頭のいい人の考え方を、ムダを削ってコンパクトにする」

イメージです。

* 使っていない部分を削る → **Pruning**
* 数字をざっくり扱う → **Quantization**
* 賢い先生から学ばせる → **Distillation**

---

## 手法① Pruning（プルーニング）

### 何をする？

* **重要度の低い重み・ニューロンを削除**
* ネットワーク構造そのものを簡素化

### 特徴

* パラメータ数削減
* モデルサイズ削減
* 推論高速化が期待できる

### 注意点（ひっかけ）

* ❌ Dropoutではない（学習時のみ無効化）
* ❌ 正則化そのものではない

---

## 手法② Quantization（量子化）

### 何をする？

* 重みや活性値の **数値表現を低精度化**

  * 例：32bit浮動小数点 → 8bit整数

### 特徴

* メモリ使用量が大幅に減る
* エッジAI・組込み機器で重要

### 注意点（ひっかけ）

* ❌ 構造は変えない
* ❌ 学習アルゴリズムではない

---

## 手法③ Knowledge Distillation（蒸留）

### 何をする？

* **大きなモデル（Teacher）** の出力を
* **小さなモデル（Student）** に学習させる

### 特徴

* 精度を保ちつつ軽量化
* 出力の「確率分布」も学習

### 注意点（ひっかけ）

* ❌ モデルの単純コピーではない
* ❌ 教師あり学習とは異なる概念

---

## 手法の比較まとめ

| 手法           | 主な目的   | 構造変更 | G検定ポイント           |
| ------------ | ------ | ---- | ----------------- |
| Pruning      | 不要部分削除 | あり   | 重み・ニューロン削除        |
| Quantization | 数値低精度化 | なし   | bit数削減            |
| Distillation | 知識移転   | あり   | Teacher / Student |

---

## G検定ひっかけポイント

* ❌ Dropout＝モデル圧縮 → **誤り**
* ❌ 正則化＝モデル軽量化 → **誤り**
* ✅ Pruningは構造削減
* ✅ Quantizationは数値表現削減

---

## まとめ（試験直前用）

* モデル圧縮 = **軽量・高速・省メモリ**
* Pruning：不要な重みを削る
* Quantization：bit数を減らす
* Distillation：教師モデルから学ぶ

👉 次は **エッジAIとモデル圧縮の関係** を見ると理解が深まります。

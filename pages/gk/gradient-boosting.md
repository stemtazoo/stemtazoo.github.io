---
layout: page
title: 勾配ブースティング（Gradient Boosting）とは？G検定対策
permalink: /gk/gradient-boosting/
tags: [gk, ensemble]
---

## まず結論

* **勾配ブースティング（Gradient Boosting）**は、**予測誤差（損失）の勾配を利用して、弱い学習器を逐次的に追加していくアンサンブル学習手法**です。
* G検定では「**何が不適切な説明か**」を見抜けるかが問われます。

## 直感的な説明

* 勾配ブースティングは、
  「**前のモデルのミスを次のモデルが修正する**」ことを繰り返します。

* 先生がテストを採点して、

  * 間違いが多い部分だけを
  * 次の補習で重点的に教える

👉 この流れを何度も繰り返すイメージです。

## 定義・仕組み

* 勾配ブースティングは、

  * 複数の**弱い学習器（主に決定木）**を
  * **逐次的（直列）**に学習させ
  * 前の予測誤差を次で補正
    します。

* 「勾配」とは、

  * 損失関数をどちらに減らせばよいか
  * その方向（勾配）
    を意味します。

※ **ニューラルネットの勾配降下法とは別物**なので注意。

## いつ使う？（得意・不得意）

### 得意

* 分類タスク
* 回帰タスク
* 高精度が求められる表形式データ

### 不得意・注意点

* モデルが複雑になりやすい
* 過学習のリスク
* 学習に時間がかかる場合がある

## G検定ひっかけポイント

* ❌ **勾配降下法を用いてパラメータを更新する**

  * → ニューラルネットの説明

* ⭕ **複数の弱い学習器を組み合わせる**

* ⭕ **アンサンブル学習の一種**

* ❌ **分類では高精度だが、回帰では一般的に使われない**

  * → **誤り（回帰でも広く使われる）**

👉 「**回帰で使われない**」という表現が出たら×。

## まとめ（試験直前用）

* 勾配ブースティングはアンサンブル学習
* 弱い学習器を逐次的に追加
* 誤差（損失）の勾配を利用
* 分類・回帰の両方で使われる
* 勾配降下法と混同しない
* 

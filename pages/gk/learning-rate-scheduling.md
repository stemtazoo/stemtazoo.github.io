---
layout: page
title: 学習率スケジューリング（Learning Rate Scheduling）
permalink: /gk/learning-rate-scheduling/
tags: [gk, neural_network, cheatsheet]
gk_section: ディープラーニングの概要/ニューラルネットワークとディープラーニング
gk_order: 7
---

## まず結論

* **学習率は固定よりも変化させた方が安定して学習できる**
* 学習初期は **大きく**、後半は **小さく** が基本
* G検定では **代表的なスケジューリング手法の違い** を押さえる

---

## 直感的な説明

学習率は、

> パラメータをどれくらいの大きさで更新するか

を決める値です。

* 学習率が大きすぎる → 発散・振動する
* 学習率が小さすぎる → なかなか収束しない

そのため、

**「最初は大胆に、後半は慎重に」**

という考え方で学習率を変化させます。

---

## 定義・仕組み

### 学習率スケジューリングとは

* 学習の進行に応じて **学習率を変化させる手法**
* 主に以下を目的とする

  * 収束の安定化
  * 局所最適解からの脱出
  * 学習の高速化

---

## 代表的な手法

### 1. Step Decay（段階的減衰）

* 一定エポックごとに学習率を下げる
* シンプルで理解しやすい

**例**

```
100epochごとに learning rate を 1/10 にする
```

---

### 2. Exponential Decay（指数減衰）

* 学習率を指数関数的に減少させる
* なめらかに学習率が下がる

**特徴**

* Stepより自然
* ハイパーパラメータ調整が必要

---

### 3. Cosine Annealing（コサイン減衰）

* コサイン関数を使って学習率を変化させる
* 終盤で急激に小さくなる

**特徴**

* 近年よく使われる
* 深層学習と相性が良い

---

### 4. Warmup（ウォームアップ）

* 学習初期は **あえて学習率を小さく** する
* 徐々に通常の学習率まで上げる

**使われる場面**

* Transformer
* 大規模モデル

---

## いつ使う？（得意・不得意）

### 得意

* 深いニューラルネットワーク
* 学習が不安定なモデル
* 大規模データ

### 注意点

* 設計が複雑になる
* 効果はモデル依存

---

## G検定ひっかけポイント

* ❌「学習率は固定でよい」→ **誤り**
* ❌「後半ほど学習率を上げる」→ **誤り**
* ✅ 初期は大きく、後半は小さく
* ✅ Warmup は **最初に小さくする**

---

## まとめ（試験直前用）

* 学習率は **動かすのが基本**
* Step / Exponential / Cosine が代表例
* Warmup は Transformer で頻出

👉 次は **誤差逆伝播（Backpropagation）まとめ** に進むと理解が一気につながります。

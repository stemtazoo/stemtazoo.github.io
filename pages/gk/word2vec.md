---
layout: page
title: Word2Vec（CBOW / Skip-gram）とは？【分散表現の基本｜G検定対策】
permalink: /gk/word2vec/
tags: [gk, nlp, machine_learning]
gk_section: ディープラーニングの応用例/自然言語処理
gk_order: 6
---

## まず結論
- **Word2Vec**とは、単語を「意味の近さ」が反映されたベクトル（分散表現）として学習する手法である。
- **CBOW**と**Skip-gram**の違いは「何を予測するか」。
- G検定では「**意味を扱えるかどうか**」と「**CBOW / Skip-gram の方向**」が問われる。

## 直感的な説明
Word2Vecは、  
**意味が似ている単語ほど、近い位置に配置される地図**を作るイメージです。

- 「犬」と「猫」→ 近い  
- 「犬」と「自動車」→ 遠い  

👉 **単語の意味関係が距離として表現される**のが最大の特徴。

## 定義・仕組み
Word2Vecは、  
**周囲の単語（文脈）から学習するニューラルネットワーク**で、  
代表的な2つの方式がある。

---

### CBOW（Continuous Bag of Words）
- **周囲の単語 → 中心の単語** を予測
- 文脈から単語を当てる

例：  
「私は（　）を学ぶ」  
→ 周囲「私」「学ぶ」から「AI」を予測

#### 特徴
- 学習が速い
- 頻出語に強い
- 精度はやや控えめ

---

### Skip-gram
- **中心の単語 → 周囲の単語** を予測
- 単語から文脈を当てる

例：  
「AI」→「私は」「学ぶ」

#### 特徴
- 学習は遅い
- 低頻度語に強い
- 精度が高い

---

### 共通点
- 単語をベクトルで表現（分散表現）
- 単語の意味的な類似性を捉える
- 語順は直接は考慮しない

## いつ使う？（得意・不得意）
### 得意な場面
- 単語の意味類似度計算
- 文書分類の特徴量
- NLPタスクの前処理

### 不得意・注意点
- 文脈全体は扱えない
- 同音異義語の区別が苦手
- BERTのような文脈理解はできない

## G検定ひっかけポイント
ここが頻出 👇

### ❌ 単語の出現回数だけを見る
- **誤り**
- それは BoW / TF-IDF

### ❌ 文全体の文脈を理解する
- **誤り**
- それは BERT 系

### ⭕ 正しい判断基準
- 「意味的に近い単語」→ Word2Vec
- 「周囲 → 中心」→ CBOW
- 「中心 → 周囲」→ Skip-gram

### 他手法との位置づけ
- BoW / TF-IDF：頻度ベース
- Word2Vec：意味ベース
- BERT：文脈ベース

## まとめ（試験直前用）
- Word2Vec＝単語の分散表現
- 意味の近さを距離で表す
- CBOW：文脈 → 単語
- Skip-gram：単語 → 文脈
- 文脈理解はしない

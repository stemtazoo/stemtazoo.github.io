---
layout: page
title: 正則化（L1・L2）とは？バイアス・バリアンスとの関係【G検定対策】
permalink: /gk/regularization-l1-l2/
tags: [gk, neural_network]
---

## まず結論

正則化（L1・L2）は**モデルの複雑さを抑えてバリアンスを下げる手法**であり、G検定では「正則化を強くするとバイアスが上がり、バリアンスが下がる」という関係を正しく判断できるかが問われる。

## 直感的な説明

正則化は、

> モデルに「欲張りすぎるなよ」とブレーキをかける仕組み

です。

* 正則化なし：

  * データを細かく覚えすぎる（過学習）
* 正則化あり：

  * 重要なところだけ覚える

つまり、

* **過学習（バリアンス大）を抑える**
* その代わりに**単純になりすぎる可能性（バイアス増）**

が出てきます。

## 定義・仕組み

正則化とは、損失関数に

> 「重みを大きくしすぎないための罰則」

を加えることです。

### L2正則化（Ridge）

* 重みの**二乗和**を罰則として加える
* 重みを全体的に小さくする
* なめらかなモデルになる

### L1正則化（Lasso）

* 重みの**絶対値和**を罰則として加える
* 不要な重みを0にしやすい
* 特徴量選択の効果

## いつ使う？（得意・不得意）

### 正則化を強くすると

* モデルが単純になる
* **バリアンスが下がる**
* **バイアスが上がる**

### 正則化を弱くすると

* モデルが複雑になる
* バリアンスが上がる
* バイアスが下がる

## G検定ひっかけポイント

G検定では、

> 「正則化＝精度が上がる」

と短絡的に考えさせる選択肢が出ます。

### よくあるひっかけ

* 正則化を強くすると必ず精度が上がる → ✕
* 正則化はバイアスを下げる → ✕

### 正誤を切る判断基準

* **過学習を抑える？** → 正則化
* **バリアンスを下げる？** → 正則化
* **バイアスを下げる？** → 正則化ではない

## まとめ（試験直前用）

* 正則化は過学習対策
* L1/L2ともにバリアンスを下げる
* 正則化が強いほどバイアスは上がる
* L1は重みを0にしやすい
* G検定では「方向」を間違えない

---
layout: page
title: CLIP（画像とテキストの対応付けモデル）とは？G検定対策
permalink: /gk/clip/
tags: [gk, neural_network, attention]
---

## まず結論
- **CLIP（Contrastive Language–Image Pretraining）**とは、**画像とテキストの対応関係を学習するマルチモーダルモデル**である。
- G検定では「**生成モデルではない**」「**対応付けが目的**」という点がよく問われる。

## 直感的な説明
CLIPは、  
**「この画像に一番合う文章はどれ？」を当てるAI**です。

- 画像を見せる
- 複数の文章候補を出す
- **一番意味が合う組み合わせを選ぶ**

文章を“作る”のではなく、  
👉 **画像と文章の相性を判断するAI**  
と考えると分かりやすいです。

## 定義・仕組み
CLIPは OpenAI が提案した **マルチモーダル学習手法**です。

仕組みのポイント：
- 画像エンコーダ（画像をベクトル化）
- テキストエンコーダ（文章をベクトル化）
- **コントラスト学習（Contrastive Learning）** により  
  「正しい画像×文章は近く、間違いは遠く」なるよう学習

重要：
- **生成モデルではない**
- **分類・対応付けが主目的**
- Transformerが使われるが、NLP専用ではない

## いつ使う？（得意・不得意）
### 得意なこと
- ゼロショット画像分類（Zero-shot classification）
- 画像検索
- 画像と文章の対応判定
- 事前ラベルが少ない分類タスク

### 不得意・注意点
- 文章生成はできない
- 画像生成はできない
- 詳細な説明文を作る用途には不向き

## G検定ひっかけポイント
G検定では次の混同が頻出です。

- ❌「画像キャプションを生成するモデル」
- ❌「画像×テキストの質問応答モデル」
- ❌「Flamingoと同じ」

### 判断基準
- **生成する？ → CLIPではない**
- **対応付け・類似度？ → CLIP**
- **質問に文章で答える？ → Flamingo / PaLM-E**

選択肢に  
「対応関係」「コントラスト学習」「ゼロショット分類」  
があれば **CLIPが有力**です。

## まとめ（試験直前用）
- CLIPは **画像とテキストの対応付けモデル**
- 生成はしない
- コントラスト学習を使う
- ゼロショット分類が得意
- 「対応付け」ならCLIP

---
layout: page
title: Batch・Layer・Instance Normalizationの違いとは？【正規化まとめ】G検定対策
permalink: /gk/normalization-cheatsheet/
tags: [gk, neural_network, cnn, cheatsheet]
gk_section: ディープラーニングの要素技術/ネットワークの構成要素
gk_order: 11
---

## まず結論

* **正規化手法の違いは「どの単位で平均・分散を計算するか」**に集約されます。
* G検定では「**バッチか？サンプルか？チャネルか？全特徴か？**」を切り分けられるかが問われます。

## 直感的な説明

* 正規化は「**データのクセをそろえて学習しやすくする**」ための前処理です。
* ただし、

  * まとめて平均を取るのか
  * 1つずつ整えるのか
    で性質が大きく変わります。

## 定義・仕組み

### Batch Normalization（BN）

* **バッチ全体 × チャネルごと**に平均・分散を計算
* 学習を安定させ、高速化

### Layer Normalization（LN）

* **サンプルごと × 全特徴**で正規化
* バッチサイズに依存しない

### Instance Normalization（IN）

* **サンプルごと × チャネルごと**で正規化
* 画像処理・スタイル変換で多用

## いつ使う？（得意・不得意）

### Batch Normalization

* 得意：CNN、バッチサイズが十分大きい場合
* 注意：バッチサイズが小さいと不安定

### Layer Normalization

* 得意：NLP、Transformer
* 注意：CNNでは一般的ではない

### Instance Normalization

* 得意：画像認識、スタイル変換
* 注意：分類性能が下がる場合もある

## G検定ひっかけポイント

* **最大のひっかけ**

  * 「特徴間の相関を除去する」→ ❌（無相関化）
* よくある混同

  * BN と IN（どちらもチャネル単位）
  * LN と IN（どちらもサンプル単位）
* 選択肢で

  * 「バッチ全体」→ BN
  * 「サンプルごと・全特徴」→ LN
  * 「サンプルごと・チャネル」→ IN

## まとめ（試験直前用）

* 正規化は統計量を取る単位で決まる
* BN：バッチ×チャネル
* LN：サンプル×全特徴
* IN：サンプル×チャネル
* 

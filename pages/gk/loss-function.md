---
layout: page
title: 損失関数（Loss / Cost / Objective）とは？【違いと使い分け｜G検定対策】
permalink: /gk/loss-function/
tags: [gk, neural_network, optimization]
---

## まず結論
- **損失関数（Loss / Cost / Objective）**とは、モデルの予測と正解のズレを数値化する関数である。
- G検定では「**学習で最小化する対象は何か**」を理解していれば、3つはほぼ同義として扱ってよい。

## 直感的な説明
損失関数は、  
**「今のモデルはどれくらいダメか？」を点数で表す仕組み**です。

- 点数が大きい → 予測が外れている  
- 点数が小さい → 予測が正解に近い  

👉 学習では、この点数を **できるだけ小さくする** ようにモデルを調整します。

## 定義・仕組み
### 損失関数（Loss Function）
- **1つのデータ（サンプル）に対する誤差**
- ニューラルネットワークの学習で最も基本的に使われる用語

例：
- 二乗誤差
- 交差エントロピー誤差

---

### コスト関数（Cost Function）
- **複数データに対する損失の平均や合計**
- データセット全体の誤差を表す

👉 Loss をまとめたもの、と考えると分かりやすい。

---

### 目的関数（Objective Function）
- **最適化で最小化（または最大化）したい関数の総称**
- 損失関数やコスト関数を含む、より広い概念

👉 「学習のゴール」となる関数。

## いつ使う？（得意・不得意）
### 実務・論文での使い分け
- Loss：1サンプル単位の誤差
- Cost：全体の誤差
- Objective：最適化対象の総称

### G検定での扱い
- **基本的に同義として扱ってOK**
- 「勾配降下法で最小化するもの」＝損失関数

## G検定ひっかけポイント
ここが頻出です 👇

### よくある誤解
- ❌ 損失関数と目的関数は全く別物  
- ❌ コスト関数は測定誤差を表す  

### 正しい判断基準
- 「学習」「パラメータ更新」「最小化」  
  → **損失関数（Loss / Cost / Objective）**

### 関係まとめ（試験用）
- Loss ⊂ Cost ⊂ Objective  
（細かい違いはあるが、試験ではまとめてOK）

## まとめ（試験直前用）
- 損失関数＝予測と正解のズレ
- 勾配降下法は **損失関数を最小化**
- Loss / Cost / Objective はほぼ同義
- 測定誤差・偶然誤差とは別概念
- 「学習で最小化」→ 損失関数

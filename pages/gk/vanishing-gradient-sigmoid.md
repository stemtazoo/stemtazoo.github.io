---
layout: page
title: 勾配消失問題とは？（なぜシグモイドが減った？）【G検定対策】
permalink: /gk/vanishing-gradient-sigmoid/
tags: [gk, neural_network]
---

## まず結論
- **勾配消失問題とは、誤差逆伝播の途中で勾配が極端に小さくなり、学習が進まなくなる現象**。
- G検定では **「なぜシグモイドが隠れ層で使われなくなったか」**が問われる。

## 直感的な説明
- 勾配は「**どれくらい直せばいいかの指示量**」。
- それが層をさかのぼるごとに：
  - 0.1 → 0.01 → 0.001 → ほぼ0  
- 最初の層に届くころには  
  👉 **「ほとんど直さなくていい」**という誤った指示になる。
- その結果：
  - 学習が止まる
  - 深いネットワークが育たない

## 定義・仕組み
- ニューラルネットワークは **誤差逆伝播法**で学習する。
- 勾配は：
  - 各層の微分を **掛け算**で伝播
- シグモイド関数の特徴：
  - 出力が 0 または 1 に近づくと
  - **微分値がほぼ 0**
- これが積み重なると：
  - 勾配 × 勾配 × 勾配 → 0
- これが **勾配消失問題**。

## いつ使う？（得意・不得意）
**シグモイドが向いていない**
- 深いネットワークの隠れ層
- 勾配を安定して流したい場合

**シグモイドが今も使われる**
- 二値分類の出力層
- 確率として解釈したい場合

## G検定ひっかけポイント
- **「シグモイドは使えない関数」だと思わせる罠**
- よくある誤解：
  - ❌ シグモイドは表現力が低い
  - ❌ シグモイドは計算できない
- 正しい理由：
  - ⭕ **勾配が小さくなりやすい**
  - ⭕ 深層化に向かない
- 代替として登場：
  - **ReLU 系活性化関数**
    - 微分が 0 or 1
    - 勾配が伝わりやすい

## まとめ（試験直前用）
- 勾配消失問題＝**勾配が途中で消える**
- 原因：微分の掛け算
- シグモイドは端で微分がほぼ0
- 深層学習に不向き
- 隠れ層では **ReLU 系が主流**

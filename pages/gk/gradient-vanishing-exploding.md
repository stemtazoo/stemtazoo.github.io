---
layout: page
title: 勾配消失・勾配爆発（ひっかけ対策）
permalink: /gk/gradient-vanishing-exploding/
tags: [gk, neural_network, cheatsheet]
---

## まず結論

* **勾配消失**：誤差が伝わらず、学習が進まない
* **勾配爆発**：誤差が大きくなりすぎて学習が不安定
* **深いネットワーク・RNNで起きやすい**
* G検定では  
  👉「どの手法が *対策* か」を問われる

---

## 直感的な説明

誤差逆伝播は、

> **後ろから前へ誤差を掛け算で伝える**

仕組みです。

### 勾配消失

* 1より小さい値を何度も掛ける
* → だんだん **0に近づく**
* → 前の層に誤差が届かない

### 勾配爆発

* 1より大きい値を何度も掛ける
* → 一気に **巨大な値**
* → 学習が発散する

---

## 定義・仕組み（数式なし）

### 勾配消失が起きる原因

* 深いニューラルネットワーク
* RNNの長い時系列
* シグモイド・tanh などの活性化関数

👉 **微分が小さい関数が原因**

---

### 勾配爆発が起きる原因

* 重みが大きすぎる
* RNNで長い系列を扱う
* 初期値設定が不適切

👉 **誤差が制御不能になる**

---

## いつ起きやすい？

### 勾配消失

* 深いCNN
* RNN（特に長期依存）
* シグモイドを多用

### 勾配爆発

* RNN
* 学習率が大きすぎる場合

---

## 主な対策（ここが最重要）

### 勾配消失への対策

* **ReLU系活性化関数**
* **LSTM / GRU**
* **Batch Normalization**
* **Residual Connection（ResNet）**

---

### 勾配爆発への対策

* **勾配クリッピング**
* 学習率を下げる
* 適切な初期値

---

## G検定ひっかけポイント

### ① 勾配消失と爆発を逆にする

* ❌「勾配消失 → 値が大きくなる」
* ❌「勾配爆発 → 学習が止まる」

👉 正しくは  
* 勾配消失 → **0に近づく**
* 勾配爆発 → **大きくなりすぎる**

---

### ② LSTMの役割を誤解させる

* ❌「LSTMは爆発を防ぐためのモデル」
* ❌「LSTMはCNNの改良」

👉 正解  
* **LSTMは勾配消失（長期依存）対策**

---

### ③ 活性化関数の罠

* シグモイド：微分最大 **0.25**
* tanh：微分最大 **1.0**
* ReLU：正の領域で **1**

👉 **ReLUは消失しにくい**

---

## まとめ（試験直前用）

* 勾配消失：誤差が伝わらない
* 勾配爆発：誤差が大きくなりすぎる
* LSTM / GRU → **消失対策**
* 勾配クリッピング → **爆発対策**
* ReLU系 → **消失しにくい**

👉 「どの問題に、どの対策か」を結びつける

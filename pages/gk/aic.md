---
layout: page
title: 赤池情報量基準（AIC）とは？G検定対策
permalink: /gk/aic/
tags: [gk, metrics]
---

## まず結論

赤池情報量基準（AIC: Akaike Information Criterion）は、**モデルの複雑さと当てはまり（予測性能）のバランスを評価する指標**です。
G検定では「**精度指標ではなく、モデル選択の基準**」である点が問われます。

## 直感的な説明

AICは「**賢すぎるモデルに罰点を与える仕組み**」だと考えると分かりやすいです。

* パラメータを増やすほど、学習データにはよく合う
* しかし、複雑すぎるモデルは過学習しやすい

AICは、

> 「**よく当てはまるほど評価は上がるが、複雑すぎると減点する**」

という考え方で、ちょうどよいモデルを選ぶための基準です。

## 定義・仕組み

赤池情報量基準（AIC）は、次の式で定義されます。

```
AIC = -2 log L + 2k
```

* **L**：モデルの尤度（データへの当てはまりの良さ）
* **k**：モデルのパラメータ数

ポイントは次の2つです。

* `-2 log L`：当てはまりが良いほど小さくなる
* `2k`：パラメータが多いほど大きくなる（＝複雑さへのペナルティ）

👉 **AICは「小さいほど良いモデル」** と判断します。

## いつ使う？（得意・不得意）

**得意な場面**

* 複数のモデル候補から1つを選びたいとき
* 線形回帰や一般化線形モデルなどのモデル選択

**注意が必要な点**

* 絶対的な性能評価ではない（モデル間の相対比較）
* 分類精度（Accuracyなど）を直接評価する指標ではない

## G検定ひっかけポイント

G検定では、次の混同を狙われやすいです。

* **AIC と 正解率（Accuracy）を混同させる**
* **AIC と ROC曲線を並べて選ばせる**

選択肢で次のように書かれていたら注意です。

* ❌「分類性能を評価する指標」
* ❌「ROC曲線の一種」

**判断基準**

* モデルの複雑さと精度のバランス → **AIC**
* 分類の当たり外れ → Accuracy / Recall
* 閾値を変えた性能評価 → ROC曲線

## まとめ（試験直前用）

* AICは**モデル選択の指標**
* 複雑さ（パラメータ数）にペナルティを与える
* **値は小さいほど良い**
* 精度指標（Accuracy・ROC）とは目的が違う
* 「モデルの良さを総合的に比較」→ AIC

---
layout: page
title: SGD / Momentum / Adam 系の系譜まとめ【1枚理解｜G検定対策】
permalink: /gk/optimizer-lineage/
tags: [gk]
---

## まず結論

最適化手法は、**SGD → Momentum → RMSProp → Adam → AdaBound** という流れで発展しており、G検定では「何を改良した手法か」「どの問題を解決しようとしたか」を系譜で理解できているかが問われる。

## 直感的な説明

最適化手法の進化は、

> 勾配降下を「どれだけ賢く進めるか」の工夫の歴史

です。

* SGD：毎回その場の勾配だけを見る
* Momentum：過去の勢いも考える
* RMSProp：勾配の大きさの違いに対応
* Adam：勢い＋大きさの両取り
* AdaBound：Adamを安定させる

## 定義・仕組み（系譜で理解）

### SGD（確率的勾配降下法）

* 最も基本的な手法
* 学習率は固定
* シンプルだが不安定になりやすい

### Momentum

* SGDに「慣性（勢い）」を追加
* 振動を抑え、収束を速める

### RMSProp

* 勾配の2次モーメントを利用
* パラメータごとに学習率を調整

### Adam

* Momentum（1次）＋RMSProp（2次）
* 高速で安定しやすい
* ディープラーニングのデフォルト

### AdaBound

* Adamに学習率の上限・下限を追加
* 後半はSGDに近づく
* 汎化性能を改善

## いつ使う？（得意・不得意）

| 手法       | 強み    | 注意点        |
| -------- | ----- | ---------- |
| SGD      | 汎化性能  | 調整が難しい     |
| Momentum | 収束が速い | 学習率依存      |
| RMSProp  | 非定常問題 | 汎化はやや弱い    |
| Adam     | 高速・安定 | 過学習傾向      |
| AdaBound | 安定＋汎化 | 常に最良とは限らない |

## G検定ひっかけポイント

### よくある誤解

* Adamは全ての問題で最良 → ✕
* RMSPropはMomentumの改良版 → ✕

### 正誤を切る判断基準

* **勢い（慣性）？** → Momentum
* **勾配の大きさ調整？** → RMSProp
* **両方使う？** → Adam
* **学習率に境界？** → AdaBound

## まとめ（試験直前用）

* SGDがすべての出発点
* Momentum＝勢い追加
* RMSProp＝勾配スケール調整
* Adam＝両取り
* AdaBound＝Adamを安定化

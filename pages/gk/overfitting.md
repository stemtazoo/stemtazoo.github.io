---
layout: page
title: 過学習（Overfitting）
permalink: /gk/overfitting/
tags: [gk, machine_learning, overfitting, regularization]
gk_section: 機械学習の概要/よくあるつまずき（過学習など）
gk_order: 1
---

## まず結論

* **過学習とは、学習データに過度に適合しすぎた状態**
* 学習データでは高精度だが、**未知データで性能が落ちる**
* モデルが「覚えすぎてしまう」ことが原因

---

## 直感的な説明

過学習は、

> **過去問を丸暗記したけど、少し問題文が変わると解けない状態**

に近いです。

* 学習データ → 完璧
* テストデータ → ボロボロ

というギャップが生まれます。

---

## 定義・仕組み

### 過学習とは？

モデルが、

* データの本質的な傾向だけでなく
* **ノイズや偶然のパターンまで学習してしまう**

状態を **過学習（Overfitting）** といいます。

---

### 過学習が起きやすい条件

* モデルが複雑すぎる
* 学習データが少ない
* 学習回数（エポック）が多すぎる

---

### 学習曲線の特徴

* 学習誤差：どんどん下がる
* 検証誤差：途中から上がり始める

👉 **検証誤差が悪化し始めたら過学習のサイン**

---

## どうやって防ぐ？（代表的な対策）

### 正則化（Regularization）

* 重みが大きくなりすぎないよう制約を加える
* L1正則化 / L2正則化

---

### 早期終了（Early Stopping）

* 検証誤差が悪化し始めた時点で学習を止める

---

### ドロップアウト（Dropout）

* 学習中にランダムでニューロンを無効化
* モデルの依存関係を弱める

---

### データを増やす

* 学習データが多いほど過学習しにくい
* データ拡張（Data Augmentation）も有効

---

## G検定ひっかけポイント

* ❌「バッチ学習は過学習を防ぐ」→ **誤り**
* ❌「学習誤差が低い＝良いモデル」→ **誤り**
* ✅ 学習誤差と検証誤差の **差** が重要
* ✅ 正則化・早期終了・ドロップアウトは過学習対策

---

## まとめ（試験直前用）

* 過学習＝**覚えすぎ**
* 学習データでは強いが汎化性能が低い
* 対策は  
  **正則化 / 早期終了 / ドロップアウト / データ増加**

👉 次は **未学習（Underfitting）** と比較すると理解が深まります。

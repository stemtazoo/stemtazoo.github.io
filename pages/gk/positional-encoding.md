---
layout: page
title: 位置エンコーディング（Positional Encoding）とは？【Transformerの弱点補完｜G検定対策】
permalink: /gk/positional-encoding/
tags: [gk, attention, transformer]
gk_section: ディープラーニングの要素技術/トランスフォーマー (Transformer)
gk_order: 4
---

## まず結論
- **位置エンコーディング（Positional Encoding）とは、系列データの順序・時間情報をTransformerに明示的に与えるための技術**である。
- G検定では「**Transformerは順序を持たないため必要**」という点が問われる。

## 直感的な説明
Transformerは、RNNやCNNと違って

- 時系列構造を持たない  
- 入力を並列に処理する  

という特徴があります。

そのため、  
「この単語が**何番目に出てきたか**」  
「前後関係はどうなっているか」  
が **そのままでは分からない**のです。

👉 そこで  
**「これは1番目」「これは2番目」**  
という位置情報を数値として足し込む  
＝ 位置エンコーディング。

## 定義・仕組み
### 定義
- トークンの埋め込みベクトルに  
  **位置情報を表すベクトルを加算**する手法
- 主に **Transformer系モデル**で使用される

### なぜ必要か
- Transformerは自己注意（Self-Attention）を使う
- Self-Attentionは **順序を考慮しない**
- → 順序情報を別途与える必要がある

### 代表的な方法
- **固定（非学習）型**
  - 正弦波（sin）・余弦波（cos）を使用
- **学習可能型**
  - 位置ベクトルを学習パラメータとして持つ

## いつ使う？（得意・不得意）
### 使われる場面
- Transformer
- BERT / GPT などの言語モデル
- 時系列データを扱うAttentionモデル

### 使われない／不要な場面
- RNN / LSTM（内部で順序を扱える）
- CNN（局所構造を持つ）

## G検定ひっかけポイント
ここはかなり狙われます。

### よくある誤解
- ❌「重みの初期化手法である」
- ❌「ノイズ除去のフィルタである」
- ❌「画像の回転量を表す」
- ❌「Attentionそのもの」

### 正しい判断基準
- **順序情報を明示的に与える → 位置エンコーディング**
- **Transformerで順序を扱う → 位置エンコーディング**
- **初期値設計 → Xavier / He**
- **ノイズ除去 → フィルタリング**

問題文に  
「Transformer」「順序」「系列データ」  
があれば **位置エンコーディング**。

## まとめ（試験直前用）
- Transformerは順序を持たない
- 位置情報は別途与える必要あり
- その役割が位置エンコーディング
- sin/cos or 学習可能ベクトル
- 「順序を補う」が判断基準

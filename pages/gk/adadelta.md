---
layout: page
title: AdaDelta（適応的学習率最適化）とは？【G検定対策】
permalink: /gk/adadelta/
tags: [gk]
---

## まず結論

* **AdaDelta** とは、学習率を事前に決めず、**過去の勾配情報から自動的に調整する最適化手法**である
* G検定では「**AdaGradの欠点を改善した手法**」「**学習率を明示的に設定しない**」点が問われる

---

## 直感的な説明

AdaDeltaは、
「最近の学習の様子を見て、
今どれくらいパラメータを動かすべきかを決める」方法。

* 学習が進みすぎていれば → 動きを小さく
* 学習が足りなければ → 動きを大きく

人間で言うと、
**過去の失敗・成功の“記憶”を見ながら慎重さを調整する**イメージ。

---

## 定義・仕組み

### AdaGrad の問題点

* 学習が進むにつれて
* 学習率がどんどん小さくなり
* **途中で学習が止まりやすい**

---

### AdaDelta の工夫

* 勾配の累積和を使わず
* **指数移動平均（直近の情報を重視）**を使う
* その結果：

  * 学習率の極端な減少を防ぐ
  * 学習率を事前に設定しなくてよい

※ G検定では数式不要。
「**AdaGradの改良版**」で十分。

---

## いつ使う？（得意・不得意）

### 得意

* 学習率調整が難しい場合
* ハイパーパラメータを減らしたい場合

### 不得意

* Adam など、より高性能な手法が使える場合
* 現代の大規模深層学習（実務では使用頻度低め）

---

## G検定ひっかけポイント

### よくある混同①：AdaGrad

* ❌ AdaDelta＝AdaGradと同じ
* ✅ **AdaGradの学習率減少問題を改善**

---

### よくある混同②：Adam

* ❌ AdaDelta＝Adam
* ✅ Adamは

  * モーメント（1次・2次）を使う
  * AdaDeltaとは別系統

---

### 選択肢の判断基準

* 「**学習率を事前に決めない**」→ AdaDelta
* 「**AdaGradの欠点を改善**」→ AdaDelta
* 「**モーメント＋適応学習率**」→ Adam

---

## まとめ（試験直前用）

* AdaDelta＝**AdaGrad改良版**
* 学習率を明示的に設定しない
* 直近の勾配情報を重視
* Adamとは別
* 「学習率を決めない」が出たらAdaDelta

---
layout: page
title: バギングとは？（Boostingとの違い）【G検定対策】
permalink: /gk/bagging/
tags: [gk, neural_network]
---

## まず結論

バギング（Bagging）とは、**訓練データをランダムに再抽出して複数のモデルを並列に学習し、その予測を平均や多数決で統合するアンサンブル学習手法**で、G検定では「ブースティングとの違い（並列か逐次か）」を正確に区別できるかが問われる。

## 直感的な説明

バギングは、

> 同じ問題を、複数人がそれぞれ独立に解いて、最後に多数決を取る

イメージです。

* 各人は同じ教材を使うが、見る問題は少しずつ違う
* 誰かのミスを他の人が補う

この結果、

* 予測のブレ（分散）が小さくなる

という効果が得られます。

## 定義・仕組み

バギングは **Bootstrap Aggregating** の略です。

仕組みは次の通りです。

1. 元データから**重複を許してランダム抽出（ブートストラップ）**
2. 抽出したデータごとに**独立したモデルを学習**
3. 各モデルの出力を**平均（回帰）や多数決（分類）**で統合

重要なポイントは、

* **各モデルは互いに独立**
* **学習は並列に行える**

という点です。

## いつ使う？（得意・不得意）

### 得意なケース

* 単体モデルの予測が不安定（分散が大きい）
* 決定木など、データに敏感なモデル

### 苦手・注意点

* バイアスが大きいモデルには効果が限定的
* 計算資源を多く使うことがある

## G検定ひっかけポイント

G検定では、**ブースティングの説明と混ぜる**選択肢が頻出です。

### よくあるひっかけ

* 誤分類データを重視する → ✕（ブースティング）
* モデルを逐次的に作成する → ✕（ブースティング）

### 正誤を切る判断基準

* **並列学習？** → バギング
* **逐次学習？** → ブースティング
* **平均・多数決？** → バギング

## まとめ（試験直前用）

* バギングは並列学習
* データを再抽出して独立に学習
* 分散を下げるのが目的
* ブースティングは逐次・誤り重視
* G検定では「並列か順次か」で即断

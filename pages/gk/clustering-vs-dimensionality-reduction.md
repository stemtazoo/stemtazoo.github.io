---
layout: page
title: クラスタリングと次元削減の違い【最終混同対策・G検定】
permalink: /gk/clustering-vs-dimensionality-reduction/
tags: [gk, unsupervised_learning]
gk_section: 機械学習の概要/代表的な手法/教師なし学習
gk_order: 4
---

## まず結論
- **クラスタリング**は「データをグループ分けする手法」、  
  **次元削減**は「データの次元（特徴量数）を減らす手法」であり、  
  **目的も出力もまったく異なる**。  
- G検定では「PCAやSVDをクラスタリングと誤認させる」選択肢が頻出する。

## 直感的な説明
### クラスタリング
- 「**似ているもの同士をまとめる**」
- 人をグループ分けするイメージ
- **結果：グループ（クラスタ）番号**

### 次元削減
- 「**情報を保ったまま圧縮する**」
- 写真を軽量化するイメージ
- **結果：特徴量の少ない新しいデータ**

👉  
- **分けるか？** → クラスタリング  
- **圧縮するか？** → 次元削減  

## 定義・仕組み
### クラスタリングの定義
- データ間の類似度に基づき
- データを複数のグループに分割する手法
- 教師なし学習に分類される

代表例：
- k-means
- 階層型クラスタリング
- DBSCAN

### 次元削減の定義
- 多次元の特徴量を
- 情報をできるだけ保ったまま低次元に変換する手法
- 教師なし学習に分類される

代表例：
- 主成分分析（PCA）
- 特異値分解（SVD）
- オートエンコーダ

## いつ使う？（得意・不得意）
### クラスタリングを使う場面
- データを分類・整理したい
- 顧客セグメンテーション
- グループ構造を知りたい

### 次元削減を使う場面
- 特徴量が多すぎる
- 可視化したい（2次元・3次元）
- 学習を高速化したい
- ノイズを減らしたい

👉  
次元削減 → **前処理**  
クラスタリング → **分析目的そのもの**

## G検定ひっかけポイント
- ❌「PCAはクラスタリング手法である」
- ❌「SVDでクラスタ分けを行う」
- ❌「デンドログラムは次元削減の結果」

- ✅「PCA / SVDは次元削減」
- ✅「クラスタリングはグループ番号を出力」
- ✅「次元削減は新しい特徴量を出力」

**超重要な判断基準**
- 出力が **ラベル（グループ）** → クラスタリング  
- 出力が **座標・特徴量** → 次元削減  

## まとめ（試験直前用）
- クラスタリング＝分ける
- 次元削減＝減らす
- 両方とも教師なし学習だが目的が違う
- PCA / SVD / オートエンコーダは次元削減
- k-means / 階層型はクラスタリング
- **「何を出力するか」で即判断**

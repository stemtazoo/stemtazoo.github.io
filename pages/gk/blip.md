---
layout: page
title: BLIP（画像キャプション生成モデル）とは？CLIPとの違い【G検定対策】
permalink: /gk/blip/
tags: [gk, neural_network, attention]
---

## まず結論
- **BLIP（Bootstrapping Language-Image Pre-training）**とは、**画像と言語を用いて「生成タスク」と「理解タスク」の両方を扱えるマルチモーダルモデル**である。
- G検定では **CLIPとの違い（生成できるかどうか）** が頻出で問われる。

## 直感的な説明
BLIPは、  
**「画像を見て、ちゃんと文章で説明できるAI」**です。

- 写真を見て説明文を書く
- 質問されたら文章で答える
- 画像と言葉を双方向に扱える

CLIPが  
👉「この画像に合う文章はどれ？」  
だとすると、

BLIPは  
👉「この画像について説明して」  
に答えられるAIです。

## 定義・仕組み
BLIPは Salesforce が提案した **マルチモーダル事前学習モデル**です。

主な特徴：
- 画像エンコーダ + テキストエンコーダ
- **画像キャプション生成**
- **画像質問応答（VQA）**
- 理解タスクと生成タスクを統合

また、
- 擬似ラベル（自動生成キャプション）を使った **Bootstrapping** により学習データを拡張
- TransformerベースのAttention機構を使用

重要ポイント：
- **生成ができる**
- **CLIPより汎用的**
- **FlamingoほどFew-shot特化ではない**

## いつ使う？（得意・不得意）
### 得意なこと
- 画像キャプション生成
- 画像に関する質問応答
- 画像内容の自然言語説明
- マルチモーダル理解＋生成タスク

### 不得意・注意点
- ゼロショット分類特化ではない
- Few-shot性能はFlamingoほど強調されない
- 単純な類似度判定ならCLIPの方が向く

## G検定ひっかけポイント
G検定では次の混同を狙ってきます。

- ❌「CLIPと同じモデル」
- ❌「画像とテキストの対応付けだけを行う」
- ❌「生成はできない」

### 判断基準（超重要）
- **文章を生成する？ → BLIP**
- **対応関係を判断するだけ？ → CLIP**
- **Few-shot強調？ → Flamingo**

選択肢に  
「キャプション生成」「質問応答」「説明文を生成」  
と書かれていたら **BLIPを選ぶ**。

## まとめ（試験直前用）
- BLIPは **画像と言語の生成・理解ができるモデル**
- 画像キャプション生成が可能
- CLIPは生成できない点が最大の違い
- FlamingoはFew-shot特化
- 「説明文を作る」→ BLIP

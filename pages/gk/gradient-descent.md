---
layout: page
title: 勾配降下法（Gradient Descent）とは？【訓練誤差との関係｜G検定対策】
permalink: /gk/gradient-descent/
tags: [gk, neural_network, optimization]
---

## まず結論
- **勾配降下法（Gradient Descent）**とは、損失関数（誤差）を最小化する方向にパラメータを更新する最適化手法である。
- G検定では「**何を最小化しているか**」を正確に答えられるかが問われる。

## 直感的な説明
勾配降下法は、  
**山を下るときに、一番急な下り坂を少しずつ下っていく方法**です。

- 今いる場所で  
- どっちに行けば下がるか（勾配）を見て  
- 少しだけ移動する  

👉 これを何度も繰り返して、**一番低いところ（誤差が最小）**を目指します。

## 定義・仕組み
- ニューラルネットワークの学習では  
  **損失関数（Loss）** を定義する
- 勾配降下法は  
  この損失関数の **勾配（微分）** を使ってパラメータを更新する

### 最小化しているもの
- ⭕ **訓練誤差（損失関数）**
- ❌ 測定誤差
- ❌ 偶然誤差
- ❌ 系統誤差

👉 **学習中に直接使われるのは「訓練誤差」だけ**

## いつ使う？（得意・不得意）
### 使われる場面
- ディープラーニングの学習全般
- 重み・バイアスの更新
- 誤差逆伝播法とセットで使用

### 注意点
- 局所最適解に陥ることがある
- 学習率の設定が重要
- データの誤差そのものは扱わない

## G検定ひっかけポイント
今回の問題の**ひっかけ構造**はここ👇

### ❌ 測定誤差
- センサーや計測機器の精度の問題
- **学習アルゴリズムとは無関係**

### ❌ 偶然誤差
- ノイズなどによるランダムな誤差
- **統計的な概念**

### ❌ 系統誤差
- 偏りのある測定による誤差
- **データ側の問題**

### ⭕ 訓練誤差
- 正解データとモデル予測の差
- **勾配降下法が最小化する対象**

👉 「**パラメータを更新する**」と書いてあったら  
👉 **訓練誤差 一択**

## まとめ（試験直前用）
- 勾配降下法は **損失関数を最小化**
- 最小化対象は **訓練誤差**
- 測定誤差・偶然誤差・系統誤差は無関係
- 勾配（微分）を使って更新
- 「学習」「更新」と書いてあったら訓練誤差

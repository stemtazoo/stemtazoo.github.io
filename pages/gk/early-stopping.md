---
layout: page
title: Early Stoppingとは？（暗黙的正則化）【G検定対策】
permalink: /gk/early-stopping/
tags: [gk, neural_network]
gk_section: ディープラーニングの概要/正則化
gk_order: 4
---

## まず結論

Early Stopping（早期終了）は、**検証誤差が悪化し始めた時点で学習を止めることで過学習を防ぐ手法**で、G検定では「損失関数を変更しない暗黙的正則化」である点を理解しているかが問われる。

## 直感的な説明

Early Stoppingは、

> テスト前日に「もうこれ以上やると混乱するから勉強をやめる」

ようなものです。

* 勉強を続けすぎる → 覚えすぎて本番で失敗
* ちょうど良いところで止める → 一番成績が良い

学習も同じで、

* 学習を続けすぎると訓練データに**過度に適合**
* テストデータでの性能が落ちる

その手前で止めるのが Early Stopping です。

## 定義・仕組み

Early Stoppingでは、

* 学習データでの誤差（訓練誤差）
* 検証データでの誤差（検証誤差）

を同時に監視します。

一般的な流れは次の通りです。

1. 学習を進める
2. 検証誤差が改善する間は継続
3. **検証誤差が悪化し始めたら停止**

重要なのは、

* モデル構造は変えない
* 損失関数にも項を追加しない

という点です。

## いつ使う？（得意・不得意）

### 得意なケース

* 深層ニューラルネットワーク
* 学習回数（エポック）が多いモデル
* 過学習が起きやすい場合

### 注意点

* 検証データが必要
* 止めるタイミングの判断が重要

## G検定ひっかけポイント

G検定では、**Early Stoppingを明示的正則化と混同させる**選択肢が出ます。

### よくあるひっかけ

* Early Stoppingは損失関数に罰則項を加える → ✕
* Early Stoppingは重みを直接小さくする → ✕

### 正誤を切る判断基準

* **学習を途中で止める？** → Early Stopping
* **損失関数を変える？** → L1 / L2
* **学習中に構造を変える？** → Dropout

## まとめ（試験直前用）

* Early Stoppingは過学習対策
* 検証誤差を見て学習を止める
* 暗黙的正則化の代表例
* 損失関数やモデル構造は変えない
* G検定ではL1/L2・Dropoutとの違いを整理

---
layout: page
title: Leaky ReLU（リーキーReLU）
permalink: /gk/leaky-relu/
tags: [gk, neural_network]
gk_section: ディープラーニングの概要/活性化関数
gk_order: 3
---

## まず結論

* **Leaky ReLUは、入力が負でも0にせず「小さな負の値」を出力する活性化関数**
* **ReLUの欠点（Dying ReLU問題）を緩和するために考案された**
* **負の入力でも勾配が完全に0にならない**

👉 G検定では **「ReLUとの違い」** を正確に言えるかが重要。

---

## 直感的な説明

Leaky ReLUは、

> 「マイナスの入力も、ちょっとだけ通すReLU」

です。

通常のReLUは👇

* 入力 < 0 → **出力 0**

となるため、

* 一度0になると学習が進まなくなる

という問題（**Dying ReLU**）があります。

Leaky ReLUでは👇

* 入力 < 0 → **入力 × 小さな係数（例：0.01）**

とすることで、
**完全に止まらない** ようにしています。

---

## 定義・仕組み

Leaky ReLU関数は次のように定義されます。

* 入力 ≥ 0 ：出力 = 入力
* 入力 < 0 ：出力 = **α × 入力**（α は小さな正の定数）

👉 重要なのは

* **負の入力でも出力は負になる**
* **0ではない**

という点です。

---

## ReLUとの違い（必須）

| 観点         | ReLU  | Leaky ReLU |
| ---------- | ----- | ---------- |
| 入力 < 0     | 出力 0  | 出力 α×入力    |
| 勾配         | 0     | α（0ではない）   |
| Dying ReLU | 起きやすい | 起きにくい      |

---

## いつ使う？（得意・不得意）

### 得意な場面

* ReLUで学習が止まる場合
* 負の入力も多少は活かしたい場合

---

### 注意点

* α の値は固定（例：0.01）が多い
* 学習で最適化されるわけではない

👉 学習でαを決めたい場合は **PReLU**

---

## G検定ひっかけポイント

* ❌「負の入力は0を出力する」→ **ReLUの説明**

* ❌「常に一定の値を出力する」→ **誤り**

* ❌「入力と常に同じ値を出力する」→ **誤り**

* ✅ **負の入力でも負の値を出力**

* ✅ **0にはならない**

---

## なぜ今回の問題で間違えやすい？

多くの人が、

> 活性化関数 = ReLU

というイメージを強く持っているため、

* Leaky ReLU
* ReLU

を **無意識に同一視** してしまいます。

👉 「Leaky（漏れる）」という名前通り、

* **負の領域が完全に0に切られない**

と覚えると切り分けやすい。

---

## まとめ（試験直前用）

* Leaky ReLUは **負の入力も通す**
* 出力は **小さな負の値**
* ReLUとの最大の違いは **0にしない点**

👉 迷ったら

> **Leaky = 漏れる → 0にならない**

---
layout: page
title: Word2Vec・FastText・GloVeの違いとは？単語分散表現の整理【G検定対策】
permalink: /gk/word2vec-fasttext-glove/
tags: [gk, nlp, neural_network]
---

## まず結論
- **Word2Vec / FastText / GloVe はすべて「単語分散表現」だが、学習の考え方が異なる。**
- G検定では「**局所文脈か／文字情報を使うか／全体統計か**」で切り分ける。

## 直感的な説明
3つは「単語の意味をどうやって学ぶか」が違います。

- **Word2Vec**  
  👉 周りの単語から意味を学ぶ
- **FastText**  
  👉 単語を文字の集まりとしても見る
- **GloVe**  
  👉 単語同士がどれくらい一緒に出るかを全体で見る

つまり、
👉 **見る範囲と粒度が違う**  
と覚えると混乱しません。

## 定義・仕組み
### Word2Vec
- 周囲の単語（局所文脈）から単語ベクトルを学習
- モデル：
  - CBOW
  - Skip-gram
- 単語は **1つのIDとして扱う**

ポイント：
- シンプル
- 高速
- 未知語（OOV）に弱い

### FastText
- Word2Vecを拡張
- 単語を **文字 n-gram の集合** として扱う
- 未知語でも文字情報からベクトル生成可能

ポイント：
- 表記ゆれ・新語に強い
- 日本語など形態変化のある言語と相性が良い

### GloVe
- **単語共起行列（全体統計）** を使って学習
- グローバル（Global）な情報を重視
- 行列分解に基づく手法

ポイント：
- コーパス全体の統計を反映
- Word2Vecとは学習の発想が異なる

## いつ使う？（得意・不得意）
### Word2Vecが向く場面
- 基本的な意味表現
- シンプルなEmbeddingが欲しいとき

### FastTextが向く場面
- 未知語が多い
- 表記ゆれが多い
- 日本語・多言語処理

### GloVeが向く場面
- 大規模コーパス
- 単語共起関係を重視したい場合

## G検定ひっかけポイント
ここが超重要です。

### よくある誤解
- ❌「FastTextは全く別の概念」
- ❌「GloVeは文脈を使わない」
- ❌「3つは性能差だけの違い」

### 正しい判断基準
- **局所文脈 → Word2Vec**
- **文字 n-gram → FastText**
- **共起行列・全体統計 → GloVe**

問題文に  
「未知語に強い」  
とあれば **FastText**。

「共起行列」「全体統計」  
とあれば **GloVe**。

## まとめ（試験直前用）
- 3つとも単語分散表現
- Word2Vec：周囲の単語
- FastText：文字情報も使う
- GloVe：全体の共起統計
- 見る範囲で切り分ける

---
layout: page
title: one-hot と Embedding の違いとは？G検定対策
permalink: /gk/onehot-vs-embedding/
tags: [gk, nlp]
gk_section: ディープラーニングの応用例/自然言語処理
gk_order: 15
---

## まず結論

* **one-hot** は単語を「1か0」だけで表す **局所表現**、**Embedding** は意味を複数次元に分散して表す **分散表現** です。
* G検定では「**どちらが類似度を表現できるか**」「**学習が必要なのはどちらか**」がよく問われます。

## 直感的な説明

* one-hot は「**出席番号**」のようなものです。番号が違えば、関係性は一切わかりません。
* Embedding は「**性格チャート**」のようなものです。複数の軸で表すので、似た単語は近くに配置されます。
* 「犬」と「猫」が近く、「犬」と「自動車」が遠くなるのが Embedding です。

## 定義・仕組み

* **one-hot 表現**

  * 単語数と同じ次元数を持つベクトル
  * 該当する単語の位置だけが1、他は0
  * 単語間の意味的な関係は表現できない

* **Embedding**

  * 単語を **低次元の実数ベクトル** に変換
  * 学習によって意味や文脈を反映
  * ベクトル間の距離で類似度を扱える

## いつ使う？（得意・不得意）

### one-hot

* 得意：

  * 実装が簡単
  * 理解しやすい
* 不得意：

  * 次元数が大きくなる
  * 類似度を扱えない

### Embedding

* 得意：

  * 単語の意味・類似性を表現できる
  * NLPモデルの性能向上に必須
* 不得意：

  * 学習や事前学習モデルが必要

## G検定ひっかけポイント

* 「**Embedding は one-hot を変換したもの**」→ ⭕（実装上はそう）
* 「**Embedding は学習によって得られる**」→ ⭕
* 「**one-hot でも単語の類似度がわかる**」→ ❌
* 選択肢で

  * 「該当要素が1、他が0」→ one-hot
  * 「低次元・実数ベクトル・類似度」→ Embedding

## まとめ（試験直前用）

* one-hot：局所表現、1か0だけ、類似度なし
* Embedding：分散表現、実数ベクトル、類似度あり
* Embeddingは学習が必要
* 類似度・意味を扱えたらEmbedding

---
layout: page
title: 確率的勾配降下法（SGD）とは？G検定対策
permalink: /gk/sgd/
tags: [gk, optimization]
gk_section: ディープラーニングの概要/最適化手法
gk_order: 3
---

## まず結論

* **確率的勾配降下法（SGD：Stochastic Gradient Descent）**とは、訓練データの**一部をランダムに抽出**して勾配を近似的に計算し、パラメータを更新する最適化手法。
* G検定では**「全データを使うかどうか」**を正しく区別できるかが問われる。

## 直感的な説明

* 山を下るとき、

  * 毎回地形全体を完璧に測って進むのが「バッチ勾配降下法」
  * 近くの足元だけを見て、少しずつ進むのが「SGD」
* SGDは多少フラつきますが、
  👉 **軽く・速く・何度も更新できる**のが強みです。

## 定義・仕組み

* SGDでは、

  * 全訓練データではなく
  * **1サンプルまたはミニバッチ**を用いて
  * 勾配を近似計算し、更新を行います。

* 特徴：

  * 計算コストが低い
  * ノイズを含む更新
  * 大規模データに向く

## いつ使う？（得意・不得意）

### 使われる場面（得意）

* データ量が非常に多い場合
* ニューラルネットワークの学習
* オンライン学習

### 注意点・不得意

* 更新が不安定になりやすい
* 学習率の設定が重要

## G検定ひっかけポイント

* よくある誤り表現：

  * ❌ 「すべての学習データを毎回使用する」
  * ❌ 「計算が安定する」

* 正しい理解：

  * ランダムサンプリング
  * 近似的な勾配計算

* 判断基準：

  * **一部データ・ランダム → SGD**
  * **全データ → バッチ勾配降下法**

## まとめ（試験直前用）

* SGD＝ランダムに一部データで更新
* 計算効率が高い
* ノイズを含む更新
* 全データ使用ではない
* 大規模学習向け

---
layout: page
title: 正則化・データ拡張・正規化の違いとは？【L1/L2/Dropout vs BatchNorm｜G検定対策】
permalink: /gk/regularization-vs-augmentation-vs-normalization/
tags: [gk, neural_network, cnn]
---

## まず結論
- **正則化・データ拡張・正規化は、すべて「過学習を防ぐ」方向に働くが、作用する場所と方法が全く異なる。**
- G検定では「**何に作用しているか**」を見れば一瞬で切れる。

## 直感的な説明
この3つは「対策している対象」が違います。

- **正則化**  
  👉 モデルの重みを縛る
- **データ拡張**  
  👉 学習データを増やす
- **正規化**  
  👉 学習の流れを安定させる

たとえると、
- 正則化＝**解き方に制限をかける**
- データ拡張＝**問題集を増やす**
- 正規化＝**勉強しやすい環境を整える**

です。

## 定義・仕組み
### 正則化（Regularization）
- モデルが複雑になりすぎるのを防ぐ手法
- **重み（パラメータ）に制約をかける**

代表例：
- **L1正則化**：重みの絶対値にペナルティ（スパース化）
- **L2正則化**：重みの二乗にペナルティ（重みを小さく）
- **Dropout**：学習中に一部のノードを無効化

ポイント：
- モデル内部に作用
- 過学習対策が主目的

### データ拡張（Data Augmentation）
- 学習データに変換を加えて **データ数・多様性を増やす**
- 主に画像データで使用

例：
- 回転、反転、色変換
- RandAugment / AutoAugment

ポイント：
- **入力データに作用**
- データが増える
- 汎化性能向上が目的

### 正規化（Normalization）
- ネットワーク内部の値の分布を整える手法
- 学習を安定させる

代表例：
- **Batch Normalization**

ポイント：
- 各層の出力分布を安定化
- 勾配消失・爆発の抑制
- 学習の高速化

## いつ使う？（得意・不得意）
### 正則化が効く場面
- モデルが複雑
- 過学習が起きている
- 重みが大きくなりすぎる

### データ拡張が効く場面
- データが少ない
- 特に画像認識
- 汎化性能を高めたい

### 正規化が効く場面
- 深いネットワーク
- 学習が不安定
- 勾配消失・爆発が問題

## G検定ひっかけポイント
ここが最重要です。

### よくある誤解
- ❌「正規化はデータを増やす」
- ❌「データ拡張は重みに作用する」
- ❌「Dropoutはデータ拡張」
- ❌「全部前処理で同じ」

### 正しい判断基準
- **重みにペナルティ → 正則化**
- **ノードを無効化 → Dropout（正則化）**
- **データを増やす → データ拡張**
- **分布を安定化 → 正規化**

## 最終比較表（これだけ見ればOK）
| 観点 | 正則化 | データ拡張 | 正規化 |
|---|---|---|---|
| 作用対象 | 重み・ノード | 入力データ | 層の出力 |
| タイミング | 学習中 | 学習前 | 学習中 |
| データ数 | 増えない | 増える | 増えない |
| 主目的 | 過学習防止 | 汎化向上 | 学習安定化 |
| 代表例 | L1/L2/Dropout | RandAugment | BatchNorm |

## まとめ（試験直前用）
- 正則化：モデルを縛る
- データ拡張：データを増やす
- 正規化：学習を安定させる
- Dropoutは正則化
- 「どこに作用？」で即判断

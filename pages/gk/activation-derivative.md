---
layout: page
title: 活性化関数の微分（ひっかけ対策）
permalink: /gk/activation-derivative/
tags: [gk, neural_network, cheatsheet]
---

## まず結論

* **tanhの微分の最大値は 1.0**
* **シグモイドの微分の最大値は 0.25**
* **「1.0」と「0.5」を混同させる選択肢が頻出**

👉 G検定では  
**「どの関数の微分が一番大きいか」**を聞いてくる。

---

## 直感的な説明

活性化関数の微分は、

> **誤差がどれだけ強く伝わるか**

を表しています。

* 微分が大きい → 勾配が伝わりやすい  
* 微分が小さい → 勾配消失しやすい  

つまり、

* **tanhはシグモイドより勾配が強い**
* **シグモイドは勾配が弱くなりやすい**

という違いがあります。

---

## 定義・仕組み（数式は最小限）

### tanh 関数

* 出力範囲：-1 ～ 1
* 微分の最大値：**1.0**
* 最大値は **入力0付近** で取る

👉 **「tanh = 1.0」だけ覚えればOK**

---

### シグモイド関数

* 出力範囲：0 ～ 1
* 微分の最大値：**0.25**
* 最大値は **出力が0.5のとき**

👉 **「シグモイド = 0.25」**

---

## いつ使う？（得意・不得意）

### tanh

* RNN / LSTM でよく使われる
* 勾配消失が比較的起きにくい
* 出力がゼロ中心で学習が安定

### シグモイド

* 出力層（確率として使う）
* 中間層では勾配消失が起きやすい
* 深いネットワークには不向き

---

## G検定ひっかけポイント

### ① 最大値の混同（最頻出）

* ❌「シグモイドの微分の最大値は1.0」
* ❌「tanhの微分の最大値は0.5」

👉 **正しくは**
* ✅ tanh → **1.0**
* ✅ シグモイド → **0.25**

---

### ② ReLUと混ぜてくる罠

* ReLUの微分は 0 または 1
* しかし **今回の問題はReLUではない**

👉 「1.0」が出てきたら  
**tanhかReLUかを文脈で判断**

---

### ③ 数式を覚えさせようとする錯覚

G検定では

* 微分式そのもの ❌
* **最大値・大小関係 ⭕**

が問われます。

---

## まとめ（試験直前用）

* **tanhの微分最大値：1.0**
* **シグモイドの微分最大値：0.25**
* 「1.0 vs 0.5」は罠
* 勾配が強いのは **tanh**

👉 「微分の最大値」＝ **勾配の伝わりやすさ**

---

必要なら次は👇も自然につながります。

* ReLU / Leaky ReLU / ELU の微分比較
* なぜReLUは勾配消失に強いのか
* 活性化関数まとめチートシート

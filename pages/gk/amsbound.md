---
layout: page
title: AMSBound（AMSGrad＋SGDの最適化手法）とは？【G検定対策】
permalink: /gk/amsbound/
tags: [gk, neural_network]
gk_section: ディープラーニングの概要/最適化手法
gk_order: 11
---

## まず結論
- **AMSBound**とは、**学習初期は AMSGrad のように速く収束し、学習後半は SGD のような汎化性能を得る最適化手法**である。
- G検定では「**序盤と終盤で性質が切り替わる**」点が問われる。

## 直感的な説明
AMSBoundは、  
**「最初は賢く速く学んで、最後は地道に仕上げる」最適化手法**です。

- 学習の最初  
  👉 AMSGradのように学習率を自動調整して **速く収束**
- 学習の後半  
  👉 学習率に上限・下限を設けて **SGDに近い挙動**

つまり、
- **速さ（Adaptive系）**
- **汎化性能（SGD系）**

の **いいとこ取り** を狙っています。

## 定義・仕組み
AMSBoundは、**AMSGrad をベース**にした最適化手法です。

仕組みのポイント：
- 適応的学習率（AMSGrad）
- 学習率に **上限・下限（Bound）** を設定
- 学習が進むにつれて、その範囲が狭まる

結果として：
- 初期：AMSGradの挙動
- 終盤：学習率が固定に近づき、**SGD的挙動**

重要：
- **Adamではない**
- **AMSGradがベース**
- SGDに「切り替える」のではなく「近づく」

## いつ使う？（得意・不得意）
### 得意な点
- 学習初期の収束が速い
- 終盤での汎化性能が高い
- Adam系の不安定さを抑えたい場合

### 注意点
- RMSpropの改良ではない
- TF-IDFとは全く無関係
- ハイパーパラメータ設定がやや複雑

## G検定ひっかけポイント
この問題は **超ひっかけ典型**です。

### よくある誤解
- ❌「RMSpropを使う」
- ❌「Adamを使う」
- ❌「最初から最後まで同じ最適化」

### 正しい判断基準
- **AMSGradが出てくる → AMSBound**
- **Adamが出てくる → AdaBound**
- **序盤速い＋終盤SGD → Bound系**

問題文に  
「序盤はAMSGrad」「終盤はSGD」  
と書いてあれば **AMSBound一択**。

## まとめ（試験直前用）
- AMSBound＝AMSGrad＋SGDの発想
- 学習率に上限・下限を設定
- 初期は速く、終盤は安定
- Adamではない
- 「AMSGrad → SGD」ならAMSBound

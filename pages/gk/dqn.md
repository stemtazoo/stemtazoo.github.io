---
layout: page
title: DQN（Deep Q-Network）とは？Experience Replay【G検定対策】
permalink: /gk/dqn/
tags: [gk]
gk_section: ディープラーニングの応用例/深層強化学習
gk_order: 2
---

## まず結論

* **DQN（Deep Q-Network）** とは、Q学習にニューラルネットワークを組み合わせ、**Experience Replay（経験再生）** を用いて学習を安定させた深層強化学習手法である
* G検定では「**経験再生を使う代表例**」「**Q学習との関係**」がよく問われる

---

## 直感的な説明

人は、

* 昔の失敗や成功を
* 思い出しながら
* 今の判断に活かす

DQNも同じで、

* 過去の行動と結果を
* メモ帳に保存しておき
* ランダムに思い出しながら学習する

この仕組みが **Experience Replay（経験再生）**。
G検定では「**過去の経験を再利用する強化学習**」と来たら DQN を疑う。

---

## 定義・仕組み

### DQNの基本構成

* ベース：**Q学習**
* Q値（状態・行動の価値）を

  * テーブルではなく
  * **ニューラルネットワーク**で近似

### Experience Replay

* （状態, 行動, 報酬, 次状態）の組を保存
* ランダムにサンプリングして学習

効果：

* データの相関を減らす
* 学習を安定化

※ G検定では数式は不要。
「**Q値をNNで近似＋経験再生**」で十分。

---

## いつ使う？（得意・不得意）

### 得意

* 状態空間が大きい強化学習
* ゲームAI（Atari など）
* ルールは明確だが状態が多い問題

### 不得意

* 連続行動空間（基本のDQNは離散行動）
* 環境が大きく変化する問題

---

## G検定ひっかけポイント

### よくある混同①：VAE

* ❌ DQN＝生成モデル
* ✅ **DQNは強化学習**

### よくある混同②：GPU

* ❌ 経験再生を行うハードウェア
* ✅ **アルゴリズム（学習手法）**

### よくある混同③：HMM

* ❌ 強化学習モデル
* ✅ **確率モデル（系列モデル）**

---

### 選択肢の判断基準

* 「**経験再生（Experience Replay）**」→ DQN
* 「**Q学習＋ニューラルネットワーク**」→ DQN
* 「**生成モデル**」→ VAE
* 「**確率的状態遷移モデル**」→ HMM

---

## まとめ（試験直前用）

* DQN＝**Q学習＋NN**
* 経験再生で学習を安定化
* 強化学習の代表例
* VAE・HMM・GPUとは無関係
* 「Experience Replay」が見えたら即DQN

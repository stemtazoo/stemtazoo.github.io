---
layout: page
title: Q学習（オフポリシー強化学習）とは？G検定対策
permalink: /gk/q-learning/
tags: [gk, reinforcement_learning]
---

## まず結論

* **Q学習（Q-learning）**とは、**実際に選択した行動とは無関係に「将来取りうる最適行動」を用いてQ値を更新するオフポリシー型の強化学習アルゴリズム**。
* G検定では **「SARSAとの違い（オンポリシー vs オフポリシー）」** を正しく区別できるかが最重要ポイント。

## 直感的な説明

* 強化学習では、

  * 実際に取った行動から学ぶ方法
  * 「もし最善の行動を取ったら？」と仮定して学ぶ方法
    があります。
* Q学習は、
  👉 **実際の行動は探索用、学習は常に理想的な最適行動を仮定して行う**
  という考え方です。
* 人で言えば、
  👉 **多少失敗しても、常にベストな行動を基準に反省するタイプ**です。

## 定義・仕組み

* Q学習では、次の更新式に基づいてQ値を更新します。

  Q(s, a) ← Q(s, a) + α [ r + γ maxₐ Q(s', a) − Q(s, a) ]

* 特徴：

  * **オフポリシー型**
  * 更新には「次状態での最大Q値」を使用
  * 探索方策（ε-greedyなど）とは独立

## いつ使う？（得意・不得意）

### 得意な場面

* 最適方策を重視したい問題
* 十分な探索が許される環境

### 苦手・注意点

* 危険な探索行動を取りやすい
* 実環境では不安定になることがある

## G検定ひっかけポイント

* よくある誤解：

  * ❌ 「実際に選んだ行動で更新する」
  * ❌ 「オンポリシー型である」

* 正しい理解：

  * **最適行動に基づいて更新**
  * **オフポリシー**

* 判断基準：

  * 「max Q(s', a) を使う」→ Q学習
  * 「次に選んだ行動を使う」→ SARSA

## まとめ（試験直前用）

* Q学習＝オフポリシー
* 最適行動で更新
* 実際の行動とは独立
* SARSAと対になる存在
* 判断軸は「更新に使う行動」

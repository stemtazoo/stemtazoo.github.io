---
layout: page
title: ε-greedy方策（epsilon-greedy policy）とは？【G検定対策】
permalink: /gk/epsilon-greedy/
tags: [gk, reinforcement_learning, policy]
gk_section: 機械学習の概要/代表的な手法/強化学習
gk_order: 15
---

## まず結論
- **ε-greedy方策とは、確率 ε でランダム行動（探索）を行い、確率 1−ε で最も良い行動（活用）を選ぶ方策**である。  
- G検定では「**探索と活用のバランス**」や「**UCBなど他の方策との違い**」がよく問われる。

---

## 直感的な説明
ε-greedy方策は、  
**「たまに冒険するけど、基本は一番よさそうな行動を選ぶ」**  
という考え方です。

- いつも同じ行動だけを選ぶ → 新しい良い行動を見逃す  
- いつもランダム → いつまでも賢くならない  

そこで、
- **ε の確率**でランダムに行動（探索）
- **1−ε の確率**で今まで一番良かった行動を選ぶ（活用）

というシンプルな仕組みで、学習を安定させます。

---

## 定義・仕組み
ε-greedy方策（epsilon-greedy policy）は、  
強化学習における **行動選択ルール（方策）** の一つです。

- ε（イプシロン）：  
  - ランダム行動を選ぶ確率
- 行動選択の流れ：
  1. 確率 ε → ランダムな行動を選択（探索）
  2. 確率 1−ε → 価値が最大の行動を選択（活用）

※ G検定では  
**「ε は固定の場合もあれば、学習が進むにつれて小さくする場合もある」**  
という点も押さえておくと安心です。

---

## いつ使う？（得意・不得意）
### 得意な場面
- Q学習などの基本的な強化学習
- シンプルに探索と活用を両立したいとき
- 実装を簡単にしたい場合

### 苦手・注意点
- 完全にランダムな探索なので効率が悪いことがある
- 行動回数が少ない行動を優先する仕組みはない
- UCBのような「不確実性を考慮した探索」はできない

---

## G検定ひっかけポイント
ここが **今回の模擬試験そのもの** です。

### ひっかけ①  
**「方策を事前に決め、その通りに行動する」**  
❌ 不正解  

→ ε-greedyは  
**学習結果（価値）に応じて行動を変える方策**

---

### ひっかけ②  
**「選択回数が少ない行動を優先する」**  
❌ 不正解  

→ これは **UCB（Upper Confidence Bound）方策** の説明

---

### ひっかけ③  
**「初めて行う行動を重視して選択する」**  
❌ 不正解  

→ ε-greedyは  
**あくまでランダム or 最大価値** の二択

---

### 正解の判断基準
選択肢に  
- 「ランダムに行動する」  
- 「学習結果に基づいて行動する」  

この **両方が含まれていたら ε-greedy**。

---

## まとめ（試験直前用）
- ε-greedy方策は「探索」と「活用」を確率で切り替える  
- **ε：ランダム行動、1−ε：最良行動**  
- 選択回数や不確実性は考慮しない  
- UCBとの違いは「探索の仕方」  

👉 **「ランダム＋最良」なら ε-greedy**

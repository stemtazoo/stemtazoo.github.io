---
layout: page
title: ミニバッチ学習（Mini-batch Gradient Descent）とは？G検定対策
permalink: /gk/mini-batch/
tags: [gk, optimization]
gk_section: ディープラーニングの概要/最適化手法
gk_order: 4
---

## まず結論

* **ミニバッチ学習（Mini-batch Gradient Descent）**とは、訓練データの**一部（ミニバッチ）**を使って勾配を計算し、パラメータを更新する最適化手法。
* G検定では**SGD（1サンプル）とバッチ勾配降下法（全データ）との違い**を正しく区別できるかが問われる。

## 直感的な説明

* 山を下るとき、

  * 全体地形を毎回調べる → バッチ勾配降下法
  * 1歩ごとに足元だけ見る → SGD
  * **数歩分まとめて地形を見る** → ミニバッチ学習
* 👉 **速さと安定性のバランス型**がミニバッチ学習です。

## 定義・仕組み

* ミニバッチ学習では、

  * 全データを使わず
  * **小さなデータ集合（例：32、64、128件）**
  * をランダムに抽出して勾配を計算します。

* 特徴：

  * SGDより安定
  * バッチ勾配降下法より計算効率が良い
  * GPU計算と相性が良い

## いつ使う？（得意・不得意）

### 使われる場面（得意）

* 深層学習の学習全般
* 大規模データセット
* GPUを用いた学習

### 注意点・不得意

* バッチサイズの選択が重要
* 大きすぎるとバッチ学習に近づく
* 小さすぎるとSGDに近づく

## G検定ひっかけポイント

* よくある誤解：

  * ❌ 「全データを使用する」
  * ❌ 「1サンプルのみを使う」

* 判断基準：

  * **少量データのまとまり → ミニバッチ**
  * **1サンプル → SGD**
  * **全データ → バッチ勾配降下法**

## まとめ（試験直前用）

* ミニバッチ学習＝少量データで更新
* 速さと安定性のバランス型
* SGDとバッチの中間
* 深層学習で標準的
* 「バッチサイズ」がキーワード

---
layout: page
title: MLM（Masked Language Model）とは？【BERTの学習方法｜G検定対策】
permalink: /gk/mlm/
tags: [gk, nlp, transformer]
gk_section: ディープラーニングの応用例/自然言語処理
gk_order: 11
---

## まず結論
- **MLM（Masked Language Model）**とは、文章中の一部の単語を隠し、その単語を予測させる学習手法である。
- G検定では「**次単語予測との違い**」「**双方向文脈が使える理由**」が問われる。

## 直感的な説明
MLMは、  
**文章の一部を黒塗りにして、前後の文脈から当てさせる問題**です。

例：
> 「私は **[MASK]** を勉強している」

- 前だけでなく  
- 後ろの文脈も使って  

👉 「AI」を予測する。

**文章全体を見られる**のが最大の特徴。

## 定義・仕組み
- 入力文の一部の単語を **[MASK]** に置き換える
- モデルは、そのマスクされた単語を予測する
- 学習時のみマスクを使う（推論時は使わない）

👉 **単語単位の穴埋め問題**

## いつ使う？（得意・不得意）
### 得意
- 文脈理解
- 意味理解
- 双方向の情報活用

### 不得意・注意点
- 文章生成には不向き
- 次単語予測とは目的が違う

## G検定ひっかけポイント
ここが**最頻出**👇

### ❌ MLMは次の単語を予測する
- **誤り**
- それは GPT 系（自己回帰モデル）

### ❌ MLMは一方向モデルである
- **誤り**
- 前後両方向の文脈を使う

### ⭕ 正しい判断基準
- 「文章中の単語を隠す」→ MLM
- 「前後の文脈を使う」→ MLM
- 「BERTの学習方法」→ MLM

### GPTとの決定的違い
- MLM（BERT）：穴埋め型・双方向
- 次単語予測（GPT）：生成型・一方向

## Transformerとの関係
- MLMは **Transformer Encoder** を使用
- Self-Attentionにより全文脈を参照可能

👉 **MLMがあるからBERTは文脈理解が得意**

## まとめ（試験直前用）
- MLM＝マスクした単語を予測
- BERTの事前学習手法
- 双方向文脈を利用
- 次単語予測とは別物
- 「穴埋め」＝MLM

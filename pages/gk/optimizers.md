---
layout: page
title: 最適化手法まとめ（SGD / Adam など）
permalink: /gk/optimizers/
tags: [gk, neural_network, optimizer]
---

## まず結論

* **最適化手法は「どうやって重みを更新するか」を決める方法**
* 誤差逆伝播法で求めた **勾配をどう使うか** が違い
* G検定では **特徴・違い・名前の対応関係** がよく問われる

---

## 直感的な説明

学習とは、

> 「間違いを少しずつ減らす方向に、重みを動かすこと」

です。

最適化手法は、

* 一気に動かすか
* ゆっくり動かすか
* 過去の動きを覚えるか

といった **動かし方のルール** を決めています。

---

## 定義・仕組み

### 勾配降下法の基本

すべての最適化手法の基本は、

[
w := w - \eta \frac{\partial L}{\partial w}
]

です。

* (\eta)：学習率
* 勾配の方向に逆らって重みを更新

---

## 主な最適化手法

### SGD（確率的勾配降下法）

* ミニバッチごとに重みを更新

**特徴**

* シンプル
* 実装が簡単

**欠点**

* 学習が不安定になりやすい
* 収束が遅い場合がある

---

### Momentum

* 過去の更新方向を考慮

**特徴**

* 振動を抑える
* 収束が速くなる

---

### AdaGrad

* パラメータごとに学習率を調整

**特徴**

* あまり更新されない重みを大きく更新

**欠点**

* 学習が進むと学習率が小さくなりすぎる

---

### RMSProp

* AdaGradの改良版

**特徴**

* 学習率が極端に小さくならない
* 非定常な問題に強い

---

### Adam

* Momentum + RMSProp の考え方を組み合わせ

**特徴**

* 学習が安定
* デフォルト設定でも性能が出やすい
* 現在の主流

---

## いつ使う？（得意・不得意）

| 手法       | 特徴        |
| -------- | --------- |
| SGD      | 基本・理論理解向け |
| Momentum | SGDの安定化   |
| AdaGrad  | 疎な特徴量     |
| RMSProp  | 学習率調整     |
| Adam     | まず試す定番    |

---

## G検定ひっかけポイント

* ❌「最適化手法は誤差を計算する方法」→ **誤り**
* ❌「Adamは勾配消失を解決する」→ **誤り**
* ✅ 誤差逆伝播法は **勾配を計算**
* ✅ 最適化手法は **勾配の使い方**
* ✅ Adam = Momentum + RMSProp

---

## まとめ（試験直前用）

* 最適化手法は **重み更新ルール**
* 基本は **SGD**
* 実務・試験ともに **Adamが定番**
* Backprop と役割を混同しない

👉 次は **正則化・過学習対策** をまとめると理解が完成します。

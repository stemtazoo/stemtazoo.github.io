---
layout: page
title: RNNエンコーダ・デコーダ（Seq2Seq）とは？【G検定対策】
permalink: /gk/rnn-encoder-decoder/
tags: [gk, rnn, nlp]
gk_section: ディープラーニングの要素技術/リカレントニューラルネットワーク (RNN)
gk_order: 5
---

## まず結論
- **RNNエンコーダ・デコーダ（Seq2Seq）** とは、**入力系列をRNNで圧縮（エンコード）し、別のRNNで出力系列を生成（デコード）するモデル構造**である。
- G検定では **Attention・Transformer・教師強制との違い**がよく問われる。

## 直感的な説明
- 「文章を一度頭の中で要約してから、別の文章として話す」イメージ。
- 例：
  - 日本語文 →（理解）→ 英語文
- 最初のRNNが **意味をまとめる係（エンコーダ）**
- 次のRNNが **文章を作る係（デコーダ）**

👉 **入力と出力の長さが違ってもOK**なのが特徴。

## 定義・仕組み
- 構成は **2つのRNN**
  1. **エンコーダ**
     - 入力系列を順に処理
     - 最後の隠れ状態に情報を圧縮
  2. **デコーダ**
     - エンコーダの最終状態を受け取り
     - 出力系列を1ステップずつ生成
- 初期のSeq2Seqでは
  - 情報は「固定長ベクトル」に圧縮される
  - 長文になると情報が欠落しやすい

👉 この欠点を補うのが **Attention機構**。

## いつ使う？（得意・不得意）
**得意**
- 機械翻訳
- 要約
- 音声認識 → テキスト変換
- 入力と出力の長さが異なる問題

**不得意**
- 長文（Attentionなしの場合）
- 並列処理が必要な大規模学習
- 高速処理が求められる場合

## G検定ひっかけポイント
- ❌「RNNエンコーダ・デコーダはAttentionを必ず使う」
- ❌「TransformerはRNNエンコーダ・デコーダの一種」
- ❌「Skip-gramの発展形である」

👉 **Attentionは後付け拡張**  
👉 **TransformerはRNNを使わない別系統**

### 判断基準
- **2つのRNNで系列→系列** → Seq2Seq
- **重み付けで参照** → Attention
- **RNNなし・並列処理** → Transformer

## まとめ（試験直前用）
- RNNエンコーダ・デコーダ = Seq2Seq
- 入力系列をRNNで圧縮し、別RNNで生成
- 機械翻訳が代表例
- Attentionは必須ではない
- Transformerとは別物

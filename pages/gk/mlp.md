---
layout: page
title: 多層パーセプトロン（MLP）
permalink: /gk/mlp/
tags: [gk, neural_network, mlp]
---

## まず結論

* **多層パーセプトロン（MLP）は中間層を持つニューラルネットワーク**
* **非線形問題（XORなど）を解ける**
* **誤差逆伝播法（Backpropagation）で学習する**

---

## 直感的な説明

単層パーセプトロンは、

> 「一直線で分けられるか？」

しか判断できませんでした。

MLPでは **中間層（隠れ層）** が入ることで、

* 小さな判定をいくつも組み合わせる
* 境界線を“折り曲げる”ような表現ができる

ようになります。

イメージとしては、

* 単層：定規で1本線を引く
* MLP：折り紙を何回も折って分ける

と考えると分かりやすいです。

---

## 定義・仕組み

### ネットワーク構造

MLPは、次の層で構成されます。

* 入力層
* **中間層（1層以上）**
* 出力層

各層では、

[
\text{出力} = f\left( \sum_i w_i x_i + b \right)
]

という計算を行い、
**活性化関数には非線形関数**（ReLU、Sigmoid など）が使われます。

### なぜXORが解ける？

* 中間層が **特徴の組み合わせ** を表現できる
* 単純な判定を重ねることで、非線形な境界を作れる

これにより、

> **線形分離できない問題も扱える**

ようになります。

### 学習方法

MLPは **誤差逆伝播法（Backpropagation）** により学習します。

1. 順伝播で出力を計算
2. 正解との差（誤差）を計算
3. 誤差を後ろから伝えて重みを更新

---

## いつ使う？（得意・不得意）

### 得意なこと

* 非線形な分類・回帰問題
* XORのような単層では解けない問題
* 表形式データ（特徴量が整理されたデータ）

### 苦手なこと

* 画像や時系列の構造を直接扱うのは不得意
* 層やノードを増やしすぎると **過学習** しやすい

> 画像 → CNN、時系列 → RNN がよく使われる理由です。

---

## G検定ひっかけポイント

* ❌「MLPは線形モデル」 → **誤り**
* ❌「中間層がなくてもXORを解ける」 → **誤り**
* ✅ **非線形活性化関数 + 中間層** がポイント
* ✅ 学習方法は **誤差逆伝播法**
* ✅ MLP = 全結合ニューラルネットワーク（Fully Connected NN）

---

## まとめ（試験直前用）

* MLPは **中間層を持つNN**
* **非線形問題を解ける**（XORが代表例）
* 学習は **誤差逆伝播法**
* 画像や系列では専用構造（CNN/RNN）が使われる

👉 次は **活性化関数** をまとめると理解が一気につながります。

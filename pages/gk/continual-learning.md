---
layout: page
title: 継続学習（Continual Learning）とは？破壊的忘却と対策まとめ【G検定対策】
permalink: /gk/continual-learning/
tags: [gk, neural_network]
gk_section: ディープラーニングの応用例/転移学習・ファインチューニング
gk_order: 6
---

## まず結論
- **継続学習（Continual Learning）**とは、**新しいタスクを順番に学習しながら、過去の知識を維持することを目指す学習方法**である。
- G検定では「**破壊的忘却がなぜ起き、どう防ぐか**」がセットで問われる。

## 直感的な説明
継続学習とは、  
**「一度覚えたことを忘れずに、新しいことを覚え続けるAI」**を作る考え方です。

普通のニューラルネットワークは、
- 新しいことを覚えると
- 前のことを **上書きして忘れてしまう**

👉 これを防ごう、というのが継続学習です。

人間でいうと、
- 学校で学年が上がっても
- 前の学年の内容を忘れない  
状態を目指しています。

## 定義・仕組み
継続学習では、次のような状況を想定します。

- タスクが **時間順に追加** される
- 過去データをすべて保持できない
- モデルは1つのまま使い続ける

このときに問題になるのが  
👉 **破壊的忘却（Catastrophic Forgetting）**です。

### 破壊的忘却とは
- 新しいタスクの学習によって
- 過去タスクの性能が急激に低下する現象
- 重みの上書きが主な原因

## いつ使う？（得意・不得意）
### 継続学習が必要な場面
- ロボットが作業を追加で覚える
- 長期間運用されるAIシステム
- データが順次到着する環境

### 向いていない場面
- 全タスクのデータを最初から集められる
- 一括学習（同時学習）が可能

## G検定ひっかけポイント
継続学習は、次の概念と一緒に出題されます。

### 混同しやすい用語
- ❌ マルチタスク学習  
  → 複数タスクを**同時に**学習
- ❌ 転移学習  
  → 事前学習モデルを**別タスクに適用**
- ✅ 継続学習  
  → タスクを**順番に**学習

### 転移との関係
- 正の転移：過去知識が役に立つ
- 負の転移：過去知識が邪魔になる
- 破壊的忘却：新学習で過去を忘れる

## 破壊的忘却への代表的対策
### EWC（Elastic Weight Consolidation）
- 過去タスクで重要な重みを特定
- その重みが変わらないよう制約
- **重みを守るアプローチ**

### リプレイ手法
- 過去データ（または生成データ）を保存
- 新タスク学習時に一緒に学習
- **復習するアプローチ**

## まとめ（試験直前用）
- 継続学習＝順番に学ぶ
- 最大の問題は破壊的忘却
- 正の転移は理想、負の転移は注意
- EWC：重みを固定
- リプレイ：過去データで復習
- 「順番・忘却・対策」でセット理解

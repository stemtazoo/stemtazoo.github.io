---
layout: page
title: 事前学習とファインチューニングは何が違う？転移学習との関係【G検定対策】
permalink: /gk/pretraining-finetuning-transfer/
tags: [gk, neural_network]
---

## まず結論
- **事前学習は「知識の土台づくり」、ファインチューニング（Fine-tuning）は「目的タスクへの適応」**である。
- G検定では「**どの段階で転移が起きているか**」を理解しているかが問われる。

## 直感的な説明
転移学習は、次の2段階で考えると分かりやすいです。

1. **事前学習**  
   👉 いろんな問題を解いて「基礎力」を身につける
2. **ファインチューニング（Fine-tuning）**  
   👉 本番の試験に合わせて「対策」する

つまり、
- 事前学習＝**広く浅く覚える**
- ファインチューニング＝**狭く深く合わせる**

という役割分担です。

## 定義・仕組み
### 事前学習（Pre-training）
- 大量の汎用データで学習
- 特定タスクに依存しない特徴表現を獲得
- 例：
  - ImageNetで学習したCNN
  - 大規模コーパスで学習した言語モデル

役割：
- **転移学習の出発点**
- 正の転移を起こしやすくする

### ファインチューニング（Fine-tuning）
- 事前学習済みモデルを、目的タスク用データで再学習
- 全層または一部の層の重みを更新
- 少量データでも高性能を実現

役割：
- **タスク適応**
- 転移の「仕上げ」

## いつ使う？（得意・不得意）
### 事前学習が効く場面
- データが大量にある
- 汎用的な特徴が重要
- 似たタスクが多数存在する

### ファインチューニングが効く場面
- タスク固有の違いが重要
- データが少ない
- 高精度が求められる

### 注意点
- タスクが大きく異なると **負の転移** が起きることがある
- ファインチューニングしすぎると **破壊的忘却** が起きることもある

## G検定ひっかけポイント
G検定では、次の混同を狙ってきます。

### よくある誤解
- ❌「事前学習＝転移学習そのもの」
- ❌「ファインチューニングしなくても転移は完了」
- ❌「ファインチューニングは必ず全層更新」

### 正しい判断基準
- **知識をためる段階 → 事前学習**
- **目的に合わせて調整 → ファインチューニング**
- **性能が向上 → 正の転移**
- **性能が下がる → 負の転移**

問題文に  
「大規模データで事前に学習」  
とあれば **事前学習**。

「少量データで調整」  
とあれば **ファインチューニング**。

## まとめ（試験直前用）
- 事前学習：汎用的な知識を獲得
- ファインチューニング：目的タスクに適応
- 転移学習はこの2段階で成立
- 正の転移が理想、負の転移に注意
- 「土台」と「仕上げ」で覚える

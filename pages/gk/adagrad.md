---
layout: page
title: AdaGradとは？G検定対策
permalink: /gk/adagrad/
tags: [gk, neural_network]
---

## まず結論

AdaGrad（Adaptive Gradient）は、**各パラメータごとに学習率を自動調整する最適化手法**です。
G検定では、**「過去すべての勾配の二乗和を使う」点**が問われます。

## 直感的な説明

AdaGradは、

> 「たくさん動いたパラメータは慎重に、あまり動いていないパラメータは大きく動かす」

という考え方の最適化手法です。

* 更新が多いパラメータ → 学習率がどんどん小さくなる
* 更新が少ないパラメータ → 学習率があまり下がらない

その結果、**特徴量ごとに最適な更新量**になります。

## 定義・仕組み

AdaGradは、確率的勾配降下法（SGD）を改良した手法です。

ポイントは次の1点です。

* **過去すべての勾配の二乗和を累積する**

式のイメージは次の通りです。

```
G_t = G_{t-1} + g_t^2
```

* `g_t`：時刻tの勾配
* `G_t`：これまでの勾配二乗和

この `G_t` を使って、パラメータごとに学習率を調整します。

## いつ使う？（得意・不得意）

**得意な点**

* 特徴量ごとのスケール差が大きい問題
* 自然言語処理などの疎な特徴量

**不得意・注意点**

* 勾配を累積し続けるため、
  学習が進むと**学習率が極端に小さくなる**

👉 この欠点を改良したのが **RMSprop** や **Adam** です。

## G検定ひっかけポイント

G検定では、次の混同が頻出です。

* **AdaBoost と混同させる**
* **RMSprop の説明と入れ替える**

選択肢の切り方は次の通りです。

* 「過去すべての勾配の二乗和」→ **AdaGrad**
* 「過去を忘れる」→ RMSprop
* 「Boost」→ AdaBoost（最適化手法ではない）

## まとめ（試験直前用）

* AdaGradは**最適化手法**
* SGDを改良した方法
* 勾配の**二乗和を累積**する
* 学習率は自動調整されるが0にはならない
* 過去を忘れないのが特徴

---
layout: page
title: SVD（特異値分解）とは？G検定対策
permalink: /gk/svd/
tags: [gk, linear_algebra]
gk_section: 機械学習の概要/代表的な手法/教師なし学習
gk_order: 12
---

## まず結論

* **SVD（特異値分解, Singular Value Decomposition）**とは、行列を**3つの行列に分解し、データの本質的な構造を取り出す線形代数の手法**。
* G検定では**PCAとの関係**や**次元削減・ノイズ除去に使われる点**が問われる。

## 直感的な説明

* 大量のデータを「重要な情報」と「それほど重要でない情報」に分けたい場面を想像してください。
* SVDは、

  * データの中で**影響の大きい方向**を見つけ
  * 重要度の低い部分を切り落とす
* つまり、👉 **データを“圧縮して要点だけ残す”方法**です。

## 定義・仕組み

* SVDでは、任意の行列 X を次の3つに分解します：

  * 左特異ベクトル行列
  * 特異値（重要度）
  * 右特異ベクトル行列

* 特徴：

  * 行列が正方でなくても分解可能
  * 特異値の大きさが情報量を表す
  * 小さい特異値を捨てることで次元削減が可能

## いつ使う？（得意・不得意）

### 使われる場面（得意）

* 次元削減
* ノイズ除去
* 潜在意味解析（LSA）
* PCAの内部計算

### 注意点・不得意

* 計算コストが高い
* 非線形構造は表現できない

## G検定ひっかけポイント

* よくある誤解：

  * ❌ 教師あり学習アルゴリズム
  * ❌ 非線形次元削減手法

* PCAとの関係：

  * PCAは**SVDを用いて計算される**
  * 目的は分散最大化、仕組みはSVD

* 判断基準：

  * **行列分解 → SVD**
  * **分散最大化 → PCA**

## まとめ（試験直前用）

* SVD＝行列を3つに分解
* 特異値が重要度を表す
* 次元削減・ノイズ除去に利用
* PCAの基盤技術
* 非線形ではない

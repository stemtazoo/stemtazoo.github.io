---
layout: page
title: PPO（Proximal Policy Optimization）とは？【G検定対策】
permalink: /gk/ppo/
tags: [gk, reinforcement_learning, neural_network]
gk_section: 機械学習の概要/代表的な手法/強化学習
gk_order: 17
---

## まず結論
**PPO（Proximal Policy Optimization）**とは、  
**方策（Policy）の更新量を制限することで、学習を安定させる強化学習アルゴリズム**である。  
G検定では「なぜ学習が安定するのか」「Q学習との違い」がよく問われる。

---

## 直感的な説明
PPOは一言でいうと、  
**「一気に方針を変えすぎない強化学習」**。

強化学習では、
- 良さそうだからといって  
- 方策（行動ルール）を大きく変えると  

👉 かえって学習が壊れることがある。

PPOは  
「**前の方策から、ちょっとだけ更新しよう**」  
とブレーキをかけることで、  
安定して学習を進める仕組み。

---

## 定義・仕組み
**PPO（Proximal Policy Optimization）**は、  
**方策勾配法**に属する強化学習アルゴリズム。

特徴は次の2点。

- 方策（Policy）を直接最適化する  
- 方策の更新幅を **クリッピング（clipping）** によって制限する  

これにより、

- 大きすぎる更新を防ぐ  
- 学習が不安定になるのを防止  

G検定では数式は不要で、  
**「更新を制限して安定化」**が理解できていればOK。

---

## いつ使う？（得意・不得意）
### 得意な場面
- 連続行動空間の問題
- 方策ベースの強化学習が必要な場合
- 安定性が重要なタスク

### 注意・不得意な点
- Q学習のように行動価値関数を直接更新する手法ではない
- シンプルな問題では過剰な場合もある

👉 **PPO＝安定性重視の方策勾配法**。

---

## G検定ひっかけポイント
ここが一番重要。

### よくある誤解
- ❌ 古い方策を完全に破棄して学習する  
- ❌ ランダムに行動する探索手法  
- ❌ Q学習と同じく行動価値関数を直接更新する  

👉 すべて **不正解**。

### 正しい判断基準
- 「方策の更新量を制限する」  
  → **PPO**
- 「学習を安定させる」  
  → **PPO**
- 「Q値を直接更新する」  
  → **Q学習系**

G検定では  
**「方策」か「行動価値関数」か** を必ず見る。

---

## まとめ（試験直前用）
- PPO＝**方策更新を制限する強化学習**
- 目的は学習の安定化
- クリッピングが特徴
- Q学習とはアプローチが異なる
- 「更新を抑える」→ PPO

---
layout: page
title: バッチ勾配降下法とは？【G検定対策】
permalink: /gk/batch-gradient-descent/
tags: [gk, neural_network]
gk_section: ディープラーニングの概要/最適化手法
gk_order: 2
---

## まず結論

* **バッチ勾配降下法（Batch Gradient Descent）**とは、**学習データ全体を一度に使って勾配を計算し、パラメータを更新する最適化手法**である。
* G検定では「SGDとの違い」「計算コスト」「安定性」が問われる。

## 直感的な説明

* クラス全員のテスト結果を集計してから、次の授業方針を決めるイメージ。
* データ全体を見てから判断するので、**更新は慎重で安定**している。

## 定義・仕組み

* 1回の更新で使うデータ：**全学習データ**
* 手順：

  1. 全データに対して損失を計算
  2. その平均勾配を求める
  3. パラメータを更新
* ミニバッチやSGDとは異なり、更新頻度は低い。

## いつ使う？（得意・不得意）

**得意**

* データ数が少ない場合
* 損失関数が滑らかで、安定した収束が欲しい場合

**不得意・注意点**

* 大規模データでは計算量・メモリ負荷が大きい
* 1回の更新に時間がかかる

## G検定ひっかけポイント

* **「毎回ランダムに一部データを使う」→ ✕**（それはSGD）
* **「更新が不安定」→ ✕**（むしろ安定）
* **「高速に学習できる」→ △**（データ量次第）
* 判断軸は「全データを使うかどうか」

## まとめ（試験直前用）

* バッチ勾配降下法＝全データで更新
* 更新は安定だが重い
* 大規模データには不向き
* SGD・ミニバッチとの違いが頻出
* G検定では定義の切り分けが重要

---
layout: page
title: Word2VecとBERTの決定的な違いとは？【意味と文脈の違い｜G検定対策】
permalink: /gk/word2vec-vs-bert/
tags: [gk, nlp, transformer]
gk_section: ディープラーニングの応用例/自然言語処理
gk_order: 8
---

## まず結論
- **Word2Vec**は「単語そのものの意味」を表す手法。
- **BERT**は「文脈を考慮した単語の意味」を表すモデル。
- G検定では「**文脈を考慮するかどうか**」が最大の判断ポイント。

## 直感的な説明
同じ単語でも、文脈で意味が変わる例👇

> 「**銀行**に行く」  
> 「**川の銀行**が決壊した」

- Word2Vec：  
  👉 「銀行」は **いつも同じ意味ベクトル**
- BERT：  
  👉 文脈に応じて **別の意味ベクトル**

👉 **同じ単語でも意味が変わるかどうか**が決定的な違い。

## 定義・仕組み
### Word2Vec
- 単語ごとに **1つの固定ベクトル**
- 文脈に依存しない
- CBOW / Skip-gram で学習

#### 特徴
- 軽量・高速
- 単語の意味的類似を捉える
- 同音異義語を区別できない

---

### BERT
- 文中の単語ごとに **文脈依存ベクトル**
- Transformer（Encoder）を使用
- 双方向に文脈を考慮

#### 特徴
- 文脈理解が可能
- 同音異義語を区別できる
- 計算コストが高い

## いつ使う？（得意・不得意）
### Word2Vecが向く
- 単語類似度計算
- 軽量な前処理
- 単語レベルの意味表現

### BERTが向く
- 文書分類
- 質問応答
- 感情分析
- 文の意味理解が重要なタスク

## G検定ひっかけポイント
ここが頻出 👇

### ❌ Word2Vecは文脈を理解する
- **誤り**
- 文脈は考慮しない

### ❌ BERTは単語の出現回数を使う
- **誤り**
- それは BoW / TF-IDF

### ⭕ 正しい判断基準
- 「単語ごとに1ベクトル」→ Word2Vec
- 「文脈で意味が変わる」→ BERT
- 「Transformer・双方向」→ BERT

### 分散表現としての違い
- Word2Vec：**静的分散表現**
- BERT：**文脈付き分散表現**

## まとめ（試験直前用）
- Word2Vec：単語の意味（固定）
- BERT：文脈込みの意味
- 同音異義語：BERTはOK、Word2VecはNG
- 軽さ重視 → Word2Vec
- 理解力重視 → BERT

---
layout: page
title: Actor–Critic とは？（オン／オフポリシーの位置づけ）【G検定対策】
permalink: /gk/actor-critic/
tags: [gk, neural_network]
---

## まず結論
- **Actor–Critic は、「行動を決める Actor」と「評価する Critic」を分けて学習する強化学習手法**。
- G検定では **「オンポリシーにもオフポリシーにもなり得る」点**が重要。

## 直感的な説明
- Actor–Critic は「**プレイヤーとコーチの分業**」。
- Actor（行動役）：
  - 次に何をするか決める
- Critic（評価役）：
  - その行動がどれくらい良かったか評価
- イメージ：
  - Actor：プレイする人
  - Critic：横でアドバイスするコーチ
- DQN と違い、  
  **行動決定と評価を分けて考える**のがポイント。

## 定義・仕組み
- Actor–Critic の基本構造：
  - **Actor**：方策（policy）を学習
  - **Critic**：価値関数（V値やQ値）を学習
- 学習の流れ：
  1. Actor が行動を選択
  2. 環境から報酬を受け取る
  3. Critic が「良さ」を評価
  4. その評価を使って Actor を更新
- 特徴：
  - Policy Gradient の不安定さを改善
  - DQN のような価値関数学習も活用

## いつ使う？（得意・不得意）
**得意**
- 連続行動空間
- 方策を直接学習したい場合
- 安定性と効率のバランスを取りたいとき

**不得意・注意**
- 実装がやや複雑
- Critic の精度に依存しやすい

## オン／オフポリシーの位置づけ
- **Actor–Critic は枠組みであって、学習方式は派生アルゴリズム次第**。
- 代表例：
  - **A2C / A3C**：オンポリシー
  - **DDPG / SAC**：オフポリシー
- 判断基準：
  - Replay を使う → オフポリシー
  - 今の方策だけ → オンポリシー

## G検定ひっかけポイント
- **「Actor–Critic ＝ オンポリシー」と決めつけさせる罠**
- よくある誤解：
  - ❌ Actor–Critic は 1 種類のアルゴリズム
  - ❌ 必ずオンポリシー
- 正しい理解：
  - Actor–Critic は **設計思想**
  - オン／オフは派生次第
- 即断キーワード：
  - 「Actor と Critic」→ Actor–Critic
  - 「Replay」→ オフポリシー型 Actor–Critic

## まとめ（試験直前用）
- Actor–Critic = **行動役と評価役を分離**
- Actor：方策、Critic：価値
- 安定性と効率を両立
- オン／オフはアルゴリズム次第
- **A2C/A3C＝オン、DDPG/SAC＝オフ**

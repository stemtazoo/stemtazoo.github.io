---
layout: page
title: 単語分散表現とは？TF-IDFが含まれない理由【G検定対策】
permalink: /gk/word-embedding/
tags: [gk, nlp, neural_network]
---

## まず結論
- **単語分散表現（Word Embedding）とは、単語を「意味を反映した連続値ベクトル」で表現する手法**である。
- **TF-IDFは単語分散表現ではない**。これはG検定の超頻出ひっかけポイント。

## 直感的な説明
まず、言葉をどう数値にするかを考えます。

### 単語分散表現のイメージ
単語を **座標（ベクトル）として配置** します。

- 「王」と「女王」は近い  
- 「犬」と「猫」は近い  
- 「犬」と「机」は遠い  

👉 **意味が近い単語ほど、ベクトルも近くなる**  
これが「分散表現」です。

### TF-IDFはどう違う？
TF-IDFは、

- その文書に何回出たか
- 他の文書ではどれくらい珍しいか

だけを数値にします。

👉 **意味の近さは一切考えません。**

## 定義・仕組み
### 単語分散表現（Word Embedding）
- 単語を **低次元の連続値ベクトル** に変換
- ベクトル間の距離や方向が **意味を表す**
- 文脈や共起関係から学習

代表例：
- Word2Vec
- FastText
- GloVe

### TF-IDF
- 単語の出現頻度に基づく重み付け
- 各次元は「単語そのもの」
- ベクトルは **疎（スパース）**

👉 **意味空間を作らない**

## いつ使う？（得意・不得意）
### 単語分散表現が向く場面
- 意味類似度計算
- 文書分類
- 感情分析
- 深層学習モデルの入力

### TF-IDFが向く場面
- 単純な文書分類
- 高速なベースライン
- 意味理解が不要なタスク

## G検定ひっかけポイント
ここが今回の問題の核心です。

### よくある誤解
- ❌「単語を数値化していれば単語分散表現」
- ❌「ベクトルなら全部Embedding」
- ❌「TF-IDFも意味を表している」

### 正しい判断基準
- **意味の近さを表す → 単語分散表現**
- **頻度だけ → TF-IDF**

問題文に  
「単語分散表現手法として最も不適切」  
とあれば、

👉 **TF-IDFを選ぶ**。

## まとめ（試験直前用）
- 単語分散表現＝意味を持つベクトル
- Word2Vec / FastText / GloVe はOK
- TF-IDFは頻度ベース
- 意味空間を作らない
- 「意味が近い？」で判断する

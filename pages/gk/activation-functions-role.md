---
layout: page
title: 活性化関数（ReLU / Sigmoid / tanh）の役割とは？G検定対策
permalink: /gk/activation-functions-role/
tags: [gk, neural_network]
gk_section: 人工知能をめぐる動向/人工知能をめぐる動向
gk_order: 9
---

## まず結論

* **活性化関数の役割は、ニューラルネットワークに「非線形性」を与えること**です。
* G検定では「**なぜ必要か**」「**ReLU・sigmoid・tanhの違い**」がよく問われます。

## 直感的な説明

* 活性化関数がないと、
  ニューラルネットワークは**どれだけ層を重ねても直線の組み合わせ**にしかなりません。
* 活性化関数は、
  **出力を曲げるスイッチ**のような役割を持ちます。

👉 この「曲がり」があるから、
XORのような複雑な問題を解けるようになります。

## 定義・仕組み

* ニューロンでは、

  1. 重み付き和を計算
  2. **活性化関数に通す**
     という処理を行います。

* 代表的な活性化関数：

### Sigmoid

* 出力範囲：0〜1
* 確率として解釈しやすい
* 欠点：**勾配消失が起きやすい**

### tanh

* 出力範囲：-1〜1
* Sigmoidより勾配が大きい
* それでも勾配消失の問題あり

### ReLU

* 出力範囲：0以上
* 計算が簡単
* **勾配消失が起きにくい**
* 現在最もよく使われる

## いつ使う？（得意・不得意）

| 関数      | 主な用途      | 注意点           |
| ------- | --------- | ------------- |
| Sigmoid | 出力層（二値分類） | 中間層では使われにくい   |
| tanh    | 旧来の中間層    | 深い層では不利       |
| ReLU    | 中間層（主流）   | Dead ReLU に注意 |

## G検定ひっかけポイント

* ❌ **活性化関数は出力を制限するためだけのもの** → 誤り

* ⭕ **非線形性を導入するため** → 正解

* ❌ **層を増やせば活性化関数はいらない** → 誤り

* ReLUは

  * 勾配消失を完全に防ぐ → ❌
  * 勾配消失を起こしにくい → ⭕

## まとめ（試験直前用）

* 活性化関数の役割は非線形性の導入
* 非線形がないとXORは解けない
* Sigmoid / tanh は勾配消失しやすい
* ReLUは現在の主流
* G検定では「なぜ必要か」を答えられることが重要
* 

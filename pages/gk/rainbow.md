---
layout: page
title: Rainbow（強化学習アルゴリズム）とは？【G検定対策】
permalink: /gk/rainbow/
tags: [gk, reinforcement_learning]
---

## まず結論

* **Rainbow**とは、DQNをベースに複数の改良手法を統合した、**高性能なモデルフリー型の強化学習アルゴリズム**である。
* G検定では「何を組み合わせたか」「モデルベースかどうか」「Q学習を使うか」が問われる。

## 直感的な説明

* DQNを「全部盛り」にして強化したのがRainbow。
* DQNの弱点（学習不安定・過大評価・探索不足など）を、複数の工夫でまとめて改善している。

## 定義・仕組み

* Rainbowは次のDQN系改良手法を組み合わせたアルゴリズム：

  * **Double DQN**：Q値の過大評価を抑制
  * **Prioritized Experience Replay**：重要な経験を優先学習
  * **Dueling Network**：状態価値と行動価値を分離
  * **Noisy Nets**：探索性能を向上
  * **Distributional RL**：報酬分布を学習
  * **Multi-step Learning**：n-step報酬を利用
* ベースは **Q学習（DQN）** であり、方策勾配法ではない。

## いつ使う？（得意・不得意）

**得意**

* ゲーム環境（Atariなど）
* 離散行動空間での高性能制御

**不得意・注意点**

* 実装が複雑
* 連続行動空間にはそのまま使えない

## G検定ひっかけポイント

* **「モデルベース強化学習」→ ✕**
* **「方策勾配法のみを使用」→ ✕**
* **「Q学習を使わない」→ ✕**
* 「DQNの改良を組み合わせたアルゴリズム」が判断軸

## まとめ（試験直前用）

* Rainbow＝DQN改良の全部盛り
* Double DQN・Prioritized Replayなどを統合
* モデルフリー型・Q学習ベース
* 高性能だが構造は複雑
* G検定では“何を組み合わせたか”を問われる

---
layout: page
title: アンサンブル学習とは？（バギング・ブースティング・スタッキング）【G検定対策】
permalink: /gk/ensemble-learning/
tags: [gk, machine_learning, ensemble]
---

## まず結論
- **アンサンブル学習**とは、複数のモデルを組み合わせて予測性能を向上させる手法である。
- G検定では「**バギング・ブースティング・スタッキングの違いと対応関係**」が頻出。

## 直感的な説明
アンサンブル学習は、  
**「1人の意見より、複数人の意見をまとめた方が正確」**という考え方。

- 同時に学習する → バギング  
- 順番に学習する → ブースティング  
- 上にもう一段モデルを乗せる → スタッキング  

👉 **どう組み合わせるか**がポイント。

## 定義・仕組み
### アンサンブル学習の定義
- 複数のモデル（弱学習器）を学習させ
- 推論時にそれらの出力を統合する手法

---

### バギング（Bagging）
- 全体データから **一部をランダムサンプリング**
- 各モデルを **独立・並列** に学習
- 出力を平均・多数決で統合

例：
- ランダムフォレスト

👉 **分散を下げるのが目的**

---

### ブースティング（Boosting）
- 弱学習器を **直列（逐次）** に学習
- 前のモデルの **誤りを次で重点的に学習**
- 徐々に精度を高める

例：
- AdaBoost
- Gradient Boosting
- XGBoost

👉 **バイアスを下げるのが目的**

---

### スタッキング（Stacking）
- 複数のモデルの **予測結果を入力**
- それを **別のモデル（メタモデル）** が学習
- **二層構造のアンサンブル**

👉 **バギングを応用した高度な手法**

## いつ使う？（得意・不得意）
### バギングが得意
- 過学習しやすいモデル
- データのばらつきが大きい場合

### ブースティングが得意
- 単体モデルの性能が低い場合
- 精度を極限まで高めたい場合

### スタッキングが得意
- 複数モデルの長所を活かしたい場合
- 上級者向け・設計が難しい

## G検定ひっかけポイント（★最重要）
ここが**今回の誤答ポイント**👇

### ❌ バギングは逐次学習する
- **誤り**
- 逐次学習は **ブースティング**

### ❌ スタッキングはブースティングの一種
- **誤り**
- スタッキングは **バギングを応用した二層構造**

### ⭕ 正しい対応関係（試験用）
- 「一部データで別々に学習」→ **バギング**
- 「直列・誤りを重視」→ **ブースティング**
- 「二層のアンサンブル」→ **スタッキング**

## まとめ（試験直前用）
- アンサンブル学習＝複数モデルの統合
- バギング：並列・分散低減
- ブースティング：逐次・誤り重視
- スタッキング：二層構造
- **並列／逐次／二層で即判断**

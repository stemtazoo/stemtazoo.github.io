---
layout: page
title: AIの公平性とバイアス（Fairness）
permalink: /gk/fairness-bias/
tags: [gk]
gk_section: AIの法律と倫理/AIの法律と倫理
gk_order: 1
---

## まず結論

**AI の公平性（Fairness）とは、「特定の属性によって不利益が生じないようにする考え方」です。**

G検定では、
👉 **AI は中立ではなく、データや設計次第でバイアスを持つ**
という点を理解しているかが問われます。

---

## 直感的な説明

AI は「自分で善悪を判断」しているわけではありません。

* 過去のデータ
* 人が決めたルール
* 学習方法

をそのまま学習します。

そのため、

👉 **データに偏りがあれば、判断結果にも偏りが出る**

という問題が起きます。

---

## 定義・仕組み

### 公平性（Fairness）

* 年齢・性別・人種などの属性によって
  不当な差別が生じないこと
* AI の判断結果が **特定の集団に不利にならない** 状態

---

### バイアス（Bias）

* データやモデルに含まれる偏り
* 学習データの分布の偏りが原因になることが多い

👉 **AI の偏りの多くは「人間由来」**

---

## いつ問題になる？（G検定視点）

### 問題になりやすい分野

* 採用・人事評価
* 融資・与信判断
* 医療診断
* 犯罪予測・監視

👉 社会的影響が大きい分野ほど重要

---

### 公平性を確保する難しさ

* 公平性の定義は 1 つではない
* 全員に完全に公平な AI は難しい

👉 **公平性はトレードオフ**

---

## G検定ひっかけポイント

### ① AI は客観的で公平である

❌ 不正解。

* AI は学習データの影響を受ける
* 人の偏りをそのまま学ぶことがある

---

### ② バイアスはモデルが悪い

❌ 不正解。

* 原因は **データ収集・設計・運用** にあることが多い

---

### ③ 公平性を高めればすべて解決

❌ 不正解。

* 精度・説明可能性・コストとのトレードオフがある

---

## まとめ（試験直前用）

* AI は **中立ではない**
* 公平性（Fairness）は重要な社会的要請
* バイアスの多くは **データ由来**

👉 G検定では

> **AI は偏る可能性があることを前提に設計・運用する**

と覚えておく

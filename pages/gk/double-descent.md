---
layout: page
title: 二重降下現象とは？（Double Descent）【G検定対策】
permalink: /gk/double-descent/
tags: [gk, neural_network]
gk_section: ディープラーニングの概要/最適化手法
gk_order: 15
---

## まず結論

二重降下現象（Double Descent）とは、**モデルの複雑さや学習の進行に応じて誤差が「下がる→上がる→再び下がる」挙動を示す現象**で、G検定では「深層学習特有の現象であり、単純なモデルでは起きない」ことを見抜けるかが問われる。

## 直感的な説明

普通は、

* モデルを複雑にするほど誤差は下がる

と考えがちです。

しかし二重降下現象では、

1. モデルがまだ小さい → 誤差が下がる
2. モデルが中途半端に大きい → **誤差が一度悪化**
3. モデルが非常に大きい → **再び誤差が下がる**

という、

> 下がる → 上がる → 下がる

という**2回目の降下**が起こります。

## 定義・仕組み

二重降下現象は、

* モデルの表現能力
* パラメータ数
* 学習データ量

のバランスが変化することで発生します。

特に、

* パラメータ数がデータ数と同程度になる付近

で、

* 過学習が強くなり
* 一時的に汎化性能が悪化

します。

しかし、さらにモデルを大きくすると、

* 最適化が進み
* 汎化性能が改善

することがあります。

## いつ使う？（得意・不得意）

### 発生し得るモデル

* 深層ニューラルネットワーク
* CNN
* Transformer
* ResNet などの深いモデル

### 発生しないモデル

* **単純パーセプトロン**
* 極めて小規模な線形モデル

## G検定ひっかけポイント

G検定では、

> 「どのモデルで起きないか」

を問われることが多いです。

### よくある誤解

* ResNetは二重降下を防ぐ → ✕
* CNNでは起きない → ✕

### 正誤を切る判断基準

* **深層学習モデルか？** → 起きうる
* **単層・単純モデルか？** → 起きない

選択肢に

> 「単純パーセプトロン」

があれば、それが**最も不適切**な選択肢になります。

## まとめ（試験直前用）

* 二重降下現象は誤差が二度下がる現象
* 深層学習モデル特有
* モデルが大きくなる途中で一度悪化
* CNN・Transformer・ResNetでも起きうる
* 単純パーセプトロンでは起きない

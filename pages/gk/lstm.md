---
layout: page
title: LSTM（長短期記憶）
permalink: /gk/lstm/
tags: [gk, neural_network, rnn, lstm]
gk_section: ディープラーニングの要素技術/リカレントニューラルネットワーク (RNN)
gk_order: 2
---

## まず結論

* **LSTMは長期依存関係を学習できるRNNの改良モデル**
* **ゲート機構** により、情報を「覚える・忘れる」を制御
* **勾配消失問題を緩和** するために設計された

---

## 直感的な説明

LSTMは、

> 「覚えておく情報」と「忘れていい情報」を自分で選べるRNN

です。

普通のRNNは、

* 重要でない情報もずっと引きずる
* 重要な情報を途中で失う

という問題がありました。

LSTMでは **ゲート** によって、

* 必要な情報だけを長く保持
* 不要な情報は適切に捨てる

ことができます。

---

## 定義・仕組み

### LSTMの基本構造

LSTMには次の3つのゲートがあります。

1. **忘却ゲート（Forget Gate）**
2. **入力ゲート（Input Gate）**
3. **出力ゲート（Output Gate）**

さらに、

* **セル状態（Cell State）** が
  長期記憶を担います。

---

### 各ゲートの役割

#### 忘却ゲート

* 過去の情報をどれだけ残すか決める
* 不要な情報を忘れる

---

#### 入力ゲート

* 新しい情報をどれだけ記憶するか決める

---

#### 出力ゲート

* 現在の状態をどれだけ出力に反映するか決める

---

### なぜ勾配消失が起きにくい？

* セル状態がほぼ **線形に情報を伝達**
* 勾配が途中で消えにくい

これにより、
**長期依存関係の学習** が可能になります。

---

## いつ使う？（得意・不得意）

### 得意なこと

* 長い文章の理解
* 時系列データの予測
* 文脈が重要なタスク

### 注意点

* 構造が複雑
* 計算量が大きい

---

## G検定ひっかけポイント

* ❌「LSTMは勾配爆発を完全に防ぐ」→ **誤り**
* ❌「LSTMにはゲートが1つだけ」→ **誤り**
* ✅ ゲートは **3種類**
* ✅ セル状態が長期記憶

---

## まとめ（試験直前用）

* LSTMは **長期依存対応RNN**
* ゲートで記憶を制御
* 勾配消失を緩和

👉 次は **GRU（Gated Recurrent Unit）** を見ていきます。

---
layout: page
title: Permutation Importance（置換重要度）とは？【G検定対策】
permalink: /gk/permutation-importance/
tags: [gk, metrics]
gk_section: ディープラーニングの応用例/モデルの解釈性
gk_order: 4
---

## まず結論
- **Permutation Importance は、特徴量の値をランダムに並び替えたときの「性能の下がり具合」で重要度を測る手法**。
- G検定では **「前処理ではない」「公平性評価ではない」ことを切れるか**が問われる。

## 直感的な説明
- 発想はとてもシンプル。
- ある特徴量を **わざとぐちゃぐちゃにする**  
  → それで **モデルがどれだけ困るか**を見る。
- たとえ話：
  - テスト中に「視力」を無効化したら成績が大きく下がる  
    → 視力は重要
  - 「靴の色」を無効化しても成績が変わらない  
    → 靴の色は重要でない

## 定義・仕組み
- 手順：
  1. 学習済みモデルの性能を測る（Accuracy など）
  2. **ある特徴量だけをランダムにシャッフル**
  3. 再度性能を測る
  4. **性能低下量＝その特徴量の重要度**
- ポイント：
  - **モデルは再学習しない**
  - 学習後に評価する **モデル非依存（ブラックボックス）手法**
- XAI（説明可能AI）の一種として使われる

## いつ使う？（得意・不得意）
**得意**
- 学習済みモデルの解釈
- どの特徴量が効いているかの確認
- モデルに依存しない評価

**不得意・注意**
- 特徴量同士が強く相関していると誤解しやすい
- 計算コストがやや高い

## G検定ひっかけポイント
- **「何ではないか」を問う問題が多い**
- よくある誤解：
  - ❌ 変数選択・前処理の手法  
  - ❌ モデルの公平性（バイアス）評価  
  - ❌ PCA のような次元削減
- 正しい判断基準：
  - 「シャッフル」「性能低下」→ **Permutation Importance**
  - 「学習前に削除」→ 前処理 → ❌
  - 「主成分」→ PCA → ❌
- 選択肢での即断ルール：
  - **ランダムに入れ替える → ⭕**
  - **削除・再構成 → ❌**

## まとめ（試験直前用）
- Permutation Importance = **シャッフルして性能低下を見る**
- 学習後に使う（前処理ではない）
- 公平性評価ではない
- PCA・変数選択と混同しない
- **「並び替え × 性能低下」がキーワード**

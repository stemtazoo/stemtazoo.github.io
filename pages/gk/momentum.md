---
layout: page
title: モーメンタム（Momentum）
permalink: /gk/momentum/
tags: [gk, neural_network, optimization]
---

## まず結論

* **モーメンタムは最適化に「慣性」を持たせる手法**
* **1990年代に提案**
* **Adamなどの最適化手法の基礎**
* 振動を抑え、**学習停滞を防ぐ**

👉 G検定では  
**「Adamで使われている」「慣性」「1990年代」** がキーワード。

---

## 直感的な説明

モーメンタムは、

> **これまで進んできた方向を、少し信じて進み続ける**

考え方です。

坂を転がるボールをイメージすると、

* 毎回止まって進路を決め直す → **SGD**
* 勢いを保って進む → **モーメンタム**

となります。

---

## 定義・仕組み（数式なし）

### 何をしている？

* **過去の勾配の移動平均** を保持
* 現在の勾配と合成して更新
* 更新方向が **なめらか** になる

---

### 何がうれしい？

* 谷での **振動を抑制**
* 平坦な領域を **速く抜ける**
* 局所的な揺れに引っ張られにくい

---

## いつ使う？（得意・不得意）

### 得意な場面

* 損失関数が **ジグザグな形**
* SGDがなかなか収束しないとき
* 学習が途中で止まりやすい場合

---

### 注意点

* 学習率や係数の調整が必要
* 単独では最先端ではない

👉 **Adamなどの土台として使われる**

---

## Adamとの関係（超重要）

* **Adam = モーメンタム + 学習率の自動調整**
* モーメンタム：**1次モーメント**
* RMSProp系：**2次モーメント**

👉 G検定では  
**「Adamの構成要素は？」** と聞かれることが多い。

---

## G検定ひっかけポイント

### ① SGDと混同させる

* ❌「SGDは慣性を使う」
* ❌「ミニバッチ＝モーメンタム」

👉 慣性・移動平均 → **モーメンタム**

---

### ② 新しい手法だと思わせる

* ❌「最近提案された最適化手法」

👉 **1990年代に提案された古典的手法**

---

### ③ Adamそのものと誤認

* ❌「モーメンタム＝Adam」

👉 正しくは  
* モーメンタ*

---
layout: page
title: 勾配消失問題とは？回避するための手法【G検定対策】
permalink: /gk/vanishing-gradient/
tags: [gk, neural_network]
gk_section: ディープラーニングの概要/ニューラルネットワークとディープラーニング
gk_order: 11
---

## まず結論
- **勾配消失問題**とは、誤差逆伝播の過程で勾配が極端に小さくなり、深い層ほど学習が進まなくなる問題である。
- G検定では **「過学習対策」と混同していないか** が最重要チェックポイント。

## 直感的な説明
- ニューラルネットワークが深くなるほど、
  - 後ろの層の誤差情報が
  - 前の層に伝わる途中で
  - どんどん弱くなる
- その結果、
  👉 **初期層がほとんど学習しなくなる**。

## 定義・仕組み
- 主な原因：
  - シグモイドやtanhなどの活性化関数
  - 重みの連続的な掛け算
- 勾配が 1 未満の値で何度も掛け算されることで
  👉 **指数的に小さくなる**

## いつ使う？（得意・不得意）
**問題が起きやすい**
- 深いニューラルネットワーク
- RNN（特に長い系列）

**起きにくい**
- 浅いネットワーク
- 勾配対策が施されたモデル

## G検定ひっかけポイント
- ❌「ドロップアウトは勾配消失対策である」
- ❌「正則化すれば勾配消失は解決する」
- ❌「過学習対策＝勾配消失対策」

👉 **ドロップアウトは過学習対策**  
👉 **勾配消失は学習が進まない問題**

## 勾配消失を回避する代表的手法
- **ReLU系活性化関数**
- **バッチ正規化（Batch Normalization）**
- **残差接続（ResNet）**
- **LSTM / GRU（ゲート構造）**
- **勾配クリッピング**

## まとめ（試験直前用）
- 勾配消失＝勾配が小さくなる問題
- 深いネットワークで発生
- 学習が進まなくなる
- 回避策は構造・活性化・正規化
- ドロップアウトは別問題

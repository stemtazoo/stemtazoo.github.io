---
layout: page
title: Multi-Head Attentionとは？複数視点で関係性を学ぶ仕組み【G検定対策】
permalink: /gk/multi-head-attention/
tags: [gk, attention, transformer]
---

## まず結論
- **Multi-Head Attentionとは、異なる表現空間で注意機構を並列に適用し、入力情報の多様な関係性を同時に学習できる仕組みである。**
- G検定では「**なぜヘッドを複数にするのか**」が問われる。

## 直感的な説明
Multi-Head Attentionは一言で言うと、

> **「同じ文章を、複数の視点で同時に見る」**

仕組みです。

例：
- あるヘッドは「主語と動詞の関係」に注目
- 別のヘッドは「長距離の単語関係」に注目
- さらに別のヘッドは「文法構造」に注目

👉 **1つの注意だけでは見落とす関係を拾える**  
これが最大の強みです。

## 定義・仕組み
### 定義
- Attention機構を複数（Head）用意し、それぞれ異なる線形変換後の空間で注意を計算する手法
- Transformerの中核技術

### 仕組みのポイント
1. 入力を複数の低次元空間に射影
2. 各空間で Attention を計算
3. 結果を結合（concat）
4. 最終的な表現を生成

重要：
- **各ヘッドは異なる関係性を学習**
- **1ヘッドでは表現力が不足**

## いつ使う？（得意・不得意）
### 得意な点
- 文脈理解の向上
- 長距離依存関係の把握
- 多様な特徴の同時抽出

### 注意点
- 計算量は減らない（むしろ増える）
- パラメータ削減が目的ではない
- 並列＝高速化が本質ではない

## G検定ひっかけポイント
ここが頻出です。

### よくある誤解
- ❌「入力系列を削除して高速化する」
- ❌「1つのヘッドですべて処理する」
- ❌「パラメータ数を削減する仕組み」

### 正しい判断基準
- **複数の視点 → Multi-Head Attention**
- **異なる表現空間 → Multi-Head Attention**
- **関係性の多様性 → Multi-Head Attention**

問題文に  
「異なる

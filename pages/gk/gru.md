---
layout: page
title: GRU（Gated Recurrent Unit）
permalink: /gk/gru/
tags: [gk, neural_network, rnn, gru]
---

## まず結論

* **GRUはLSTMを簡略化したRNNの一種**
* ゲート数を減らして **構造をシンプルに** した
* **勾配消失を緩和** しつつ、計算効率が良い

---

## 直感的な説明

GRUは、

> 「LSTMほど細かく制御しなくても、十分うまく覚えられる」

という発想で作られたモデルです。

* 覚えるか、忘れるか
* どれくらい反映するか

を **少ないゲート** で判断します。

---

## 定義・仕組み

### GRUの基本構造

GRUには次の **2つのゲート** があります。

1. **更新ゲート（Update Gate）**
2. **リセットゲート（Reset Gate）**

※ LSTMの「セル状態」は明示的に持ちません。

---

### 各ゲートの役割

#### 更新ゲート

* 過去の情報をどれだけ残すか決める
* 新しい情報をどれだけ取り入れるか調整

---

#### リセットゲート

* 過去の情報をどれだけ無視するか決める
* 短期的な依存関係を扱いやすくする

---

### LSTMとの違い

| 項目   | LSTM | GRU  |
| ---- | ---- | ---- |
| ゲート数 | 3    | 2    |
| セル状態 | あり   | なし   |
| 構造   | 複雑   | シンプル |

---

## いつ使う？（得意・不得意）

### 得意なこと

* 時系列・系列データ
* モデルを軽量にしたい場合

### 注意点

* 表現力はLSTMよりやや劣る場合がある

---

## G検定ひっかけポイント

* ❌「GRUはLSTMよりゲートが多い」→ **誤り**
* ❌「GRUはセル状態を持つ」→ **誤り**
* ✅ GRUは **ゲート2つ**
* ✅ LSTMより構造がシンプル

---

## まとめ（試験直前用）

* GRUは **軽量なLSTM**
* ゲート数は2つ
* 勾配消失を緩和

👉 これで **RNN系ブロックは完成** です。

---
layout: page
title: Adam（Adaptive Moment Estimation）
permalink: /gk/adam/
tags: [gk, neural_network, optimization]
---

## まず結論

* **Adamは最も広く使われている最適化手法**
* **モーメンタム + RMSProp** を組み合わせた手法
* 学習が安定しやすく、**初期設定のままでも強い**
* G検定では  
  👉「構成要素」「モーメント」「自動調整」が問われる

---

## 直感的な説明

Adamは、

> **勢いを保ちつつ、場所ごとに学習率を自動で変える**

最適化手法です。

イメージすると：

* **モーメンタム**：進む方向の勢いを記憶
* **RMSProp**：勾配が大きい所では慎重に、小さい所では大胆に

👉 この2つを **同時に使う** のがAdam。

---

## 定義・仕組み（数式なし）

Adamでは、次の2つを同時に管理します。

### ① 1次モーメント（平均）

* 勾配の **移動平均**
* 👉 **モーメンタムに相当**
* 更新方向を安定させる

---

### ② 2次モーメント（分散）

* 勾配の **二乗の移動平均**
* 👉 **RMSPropに相当**
* 学習率を自動調整

---

### Adamの正体（試験用）

* Adam = **モーメンタム + RMSProp**
* 「モーメント推定」という名前がヒント

---

## いつ使う？（得意・不得意）

### 得意なこと

* 深いニューラルネットワーク
* 勾配が不安定な問題
* 学習率調整が難しい場合

👉 **とりあえずAdam** が通用する理由。

---

### 注意点

* 必ずしも最良とは限らない
* 収束後の性能ではSGDが勝つ場合もある

（※ G検定では深掘り不要）

---

## SGD・モーメンタムとの違い（超重要）

| 手法 | 特徴 |
|---|---|
| SGD | シンプル・不安定 |
| モーメンタム | 慣性を導入 |
| RMSProp | 学習率を自動調整 |
| **Adam** | **両方を組み合わせ** |

---

## G検定ひっかけポイント

### ① Adam＝新しい理論だと思わせる

* ❌「Adamは全く新しい考え方」

👉 実際は  
* **既存手法の組み合わせ**

---

### ② モーメントの意味を誤解

* ❌「モーメント＝物理の話」

👉 正しくは  
* **勾配の統計量（平均・分散）**

---

### ③ 学習率を固定だと思わせる

* ❌「Adamも学習率は固定」

👉 **自動で調整される**

---

## まとめ（試験直前用）

* Adam = **モーメンタム + RMSProp**
* 1次モーメント：勢い
* 2次モーメント：学習率調整
* デフォルトでも強い
* G検定頻出

👉 「モーメント・自動調整・Adam」  
このセットで即答

---
layout: page
title: SARSA（オンポリシー強化学習）とは？G検定対策
permalink: /gk/sarsa/
tags: [gk, reinforcement_learning]
gk_section: 機械学習の概要/代表的な手法/強化学習
gk_order: 13
---

## まず結論

* **SARSA**とは、**行動選択と同じ方策（ポリシー）に基づいてQ値を更新するオンポリシー型の強化学習アルゴリズム**。
* G検定では **「オンポリシーか／オフポリシーか」** を正しく切り分けられるかが最大のポイント。

## 直感的な説明

* 強化学習には、

  * 実際に選んだ行動で学習する方法
  * 理想的な行動を仮定して学習する方法
    があります。
* SARSAは、
  👉 **実際にエージェントが選んだ行動そのものを使って学習する**
  方法です。
* 人で言えば、
  👉 **失敗も含めて、実体験から学ぶタイプ**。

## 定義・仕組み

* SARSAは次の5要素から名前が付いています。

  * State（状態）
  * Action（行動）
  * Reward（報酬）
  * State（次の状態）
  * Action（次の行動）

* 特徴：

  * **オンポリシー型**
  * 行動選択と同じ方策でQ値を更新
  * 探索の影響をそのまま学習に反映

## いつ使う？（得意・不得意）

### 得意な場面

* 安全性を重視する環境
* 探索行動のリスクを抑えたい場合

### 苦手・注意点

* 学習速度はQ学習より遅いことがある
* 最適方策への収束が遅くなる場合がある

## G検定ひっかけポイント

* よくある誤解：

  * ❌ 「最適行動に基づいて更新する」
  * ❌ 「探索とは無関係に学習する」

* 正しい理解：

  * **実際に選択した行動で更新**
  * **オンポリシー**

* 判断基準：

  * 「同じ方策で行動＋更新」→ SARSA
  * 「最適行動で更新」→ Q学習

## まとめ（試験直前用）

* SARSA＝オンポリシー
* 実際の行動で学習
* 探索の影響を含む
* Q学習とは対になる存在
* 判断軸は「更新に使う行動」

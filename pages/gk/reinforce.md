---
layout: page
title: REINFORCEとは？（方策勾配法）G検定対策
permalink: /gk/reinforce/
tags: [gk, reinforcement_learning]
gk_section: 機械学習の概要/代表的な手法/強化学習
gk_order: 4
---

## まず結論

* **REINFORCE**とは、価値関数を使わずに、**方策（ポリシー）そのものを勾配で最適化する強化学習アルゴリズム**です。
* G検定では「**価値関数を最適化するか／方策を直接最適化するか**」が問われます。

## 直感的な説明

* REINFORCEは「**行動してみて、良かった行動は起こりやすく、悪かった行動は起こりにくくする**」学習方法です。
* どの行動が良かったかを、

  * 価値関数で評価するのではなく
  * 実際にもらった報酬を使って
    直接ポリシーを調整します。
* つまり「**反省して行動のクセを直す**」イメージです。

## 定義・仕組み

* REINFORCEは**方策勾配法（Policy Gradient Method）**の代表例です。
* 方策 ( \pi(a|s; \theta) ) のパラメータ (\theta) を、
  **報酬の期待値が大きくなる方向**に更新します。
* 重要な点：

  * **価値関数（Value Function）を使わない**
  * モンテカルロ法で報酬を推定

## いつ使う？（得意・不得意）

### 得意な点

* 行動が確率的な問題に向いている
* 実装がシンプル

### 不得意・注意点

* 分散が大きく、学習が不安定
* サンプル効率が悪い
* 実務では改良版（Actor-Criticなど）が使われる

## G検定ひっかけポイント

* **最大のひっかけ**

  * 「REINFORCEは価値関数を最適化する」→ ❌
* 正しい理解

  * 価値関数を最適化する → Q学習 / DQN
  * 方策を直接最適化する → **REINFORCE**
* よくある混同

  * 方策勾配法 ＝ Actor-Critic（※REINFORCEはその原型）
* 選択肢で

  * 「確率的ポリシー勾配法」→ ⭕
  * 「価値関数を用いずに学習」→ ⭕

## まとめ（試験直前用）

* REINFORCEは方策勾配法
* 価値関数を使わない
* ポリシーを直接更新
* 「価値関数を最適化する」は誤り

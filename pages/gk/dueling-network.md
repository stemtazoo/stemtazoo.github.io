---
layout: page
title: デュエリングネットワーク（Dueling Network）とは？G検定対策
permalink: /gk/dueling-network/
tags: [gk, reinforcement_learning, neural_network]
---

## まず結論
- **デュエリングネットワーク（Dueling Network）**は、状態価値とアドバンテージを別々に推定し、そこから行動価値を計算する強化学習手法である。
- G検定では「**何を予測（算出）しているか**」を正確に区別できるかが問われる。

## 直感的な説明
デュエリングネットワークは、  
**「今の状況がどれくらい良いか」と「どの行動がどれくらい有利か」を分けて考える**仕組みです。

人間で例えると、

- 状態価値：  
  👉 **今の状況そのものは良さそう？**
- アドバンテージ：  
  👉 **この状況で、この行動はどれくらい得？**

この2つを組み合わせて、  
👉 **どの行動を取るべきか（行動価値）**  
を判断します。

## 定義・仕組み
デュエリングネットワークは、Qネットワークを次の2つに分解する。

### 状態価値関数（Value）
- V(s)：その状態がどれだけ良いか
- 行動に依存しない

### アドバンテージ関数（Advantage）
- A(s, a)：その状態で特定の行動がどれだけ有利か
- 行動ごとの違いを表す

### 行動価値関数（Q値）
- Q(s, a) は次のように計算される：

  Q(s, a) = V(s) + A(s, a)

👉 **ネットワークは V と A を推定し、Q値を算出する**

## いつ使う？（得意・不得意）
### 得意
- 行動による差が小さい状態
- 状態の良し悪しを先に判断したい問題
- DQNの性能改善

### 注意点
- 報酬そのものはネットワークが出力しない
- あくまで価値関数の推定が目的

## G検定ひっかけポイント
この問題の核心はここ👇

### ❌ 報酬（Reward）
- 報酬は **環境から与えられる**
- **ネットワークが予測・算出するものではない**

👉 これが「最も不適切」になる理由

### ⭕ 正しく算出するもの
- 状態価値（V）
- アドバンテージ（A）
- 行動価値（Q）

### よくある誤解
- ❌ アドバンテージ = 報酬
- ❌ Q値 = 報酬

👉 **報酬は教師信号、価値は推定結果**

## まとめ（試験直前用）
- デュエリングネットワークは **V と A を別々に推定**
- 行動価値 Q を計算するための構造
- **報酬は予測しない**
- 状態価値・アドバンテージ・行動価値は算出対象
- 「環境が与える」＝ 報酬

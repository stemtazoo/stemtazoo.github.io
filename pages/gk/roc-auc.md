---
layout: page 
title: ROC曲線・AUC 
permalink: /gk/roc-auc/ 
tags: [gk, 機械学習, 評価指標, 頻出]
---

# ROC曲線・AUC

## まず結論

ROC曲線は、分類モデルの性能を「しきい値を変えながら」評価するグラフです。
AUCは、その ROC曲線の下の面積を数値で表したものです。

👉 AUCが大きいほど、良い分類モデル といえます。


---

## ROC曲線とは？

ROC曲線（Receiver Operating Characteristic）は、

横軸：FPR（偽陽性率）

縦軸：TPR（真陽性率＝Recall）


をプロットしたグラフです。

縦軸：TPR（Recall）
↑
|        ●
|     ●
|  ●
|●
+----------------→ 横軸：FPR


---

## 軸の意味（ここ重要）

真陽性率（TPR）

TPR = TP / (TP + FN)
→ Recall（再現率）と同じ

偽陽性率（FPR）

FPR = FP / (FP + TN)
→ 正常を誤って陽性とした割合（誤検知率）


---

## 直感的な説明

しきい値を厳しくする
→ 誤検知は減るが、見逃しが増える

しきい値を緩くする
→ 見逃しは減るが、誤検知が増える


👉 ROC曲線は
このトレードオフ全体を1枚で見る方法です。


---

## AUC（Area Under the Curve）とは

AUCは、ROC曲線の下の面積を表します。

AUC = 1.0：完璧な分類

AUC = 0.5：ランダム予測と同等

AUC < 0.5：予測が逆（ラベル反転で改善可能）


👉 しきい値に依存しない指標なのが強みです。


---

## いつ使う？（得意・不得意）

得意

クラス不均衡データ

モデル同士の性能比較

しきい値を後で決めたい場合


苦手

実運用の「誤検知 vs 見逃し」の具体的なバランスは分からない
→ Precision / Recall と併用する



---

## Accuracy・F1-scoreとの違い

Accuracy：1つのしきい値での正解率

F1-score：Precision と Recall のバランス

ROC/AUC：しきい値全体での性能評価


👉 視点がまったく違う。


---

## G検定ひっかけポイント

ROC曲線の縦軸は Recall（TPR）

横軸は FPR（偽陽性率）

AUC = 0.5 は ランダム

AUC は しきい値に依存しない



---

## よくある勘違い

❌ AUC が高ければ誤検知が少ない
→ ⭕ 全体的に性能が良いだけ（しきい値次第）

❌ ROC曲線は分類結果そのもの
→ ⭕ 評価用のグラフ



---

## まとめ（試験直前用）

ROC曲線：TPR と FPR の関係を見る

AUC：ROC曲線の下の面積

AUC = 0.5 はランダム予測

モデル比較に強い指標

---
layout: page
title: 強化学習チートシート（完全版）【G検定対策まとめ】
permalink: /gk/reinforcement-learning-cheatsheet/
tags: [gk, reinforcement_learning, cheatsheet]
gk_section: 機械学習の概要/代表的な手法/強化学習
gk_order: 2
---

## まず結論
- 強化学習は「**報酬を最大化する行動を学習する枠組み**」である。
- G検定では「**誰が何を学習しているか**」「**単一か複数か**」「**代表手法と例**」を正確に区別できるかが問われる。

## 直感的な説明
強化学習は、  
**正解を教えてもらうのではなく、行動の結果から学ぶ学習**です。

- 行動 → 結果 → ご褒美 or 罰
- 良かった行動は増やす
- 悪かった行動は減らす

👉 **教師あり・なしとは別枠**の学習方法。

## 定義・仕組み
### 基本構成（必須）
- エージェント：行動する主体
- 環境：エージェントの外界
- 状態（State）
- 行動（Action）
- 報酬（Reward）

👉 **報酬は環境から与えられる（重要）**

### 学習対象の違い
- 価値ベース：  
  👉 行動の「良さ」を数値で学習
- 方策ベース：  
  👉 行動の「選び方」を直接学習

## いつ使う？（得意・不得意）
### 向いている問題
- 試行錯誤が可能
- 明確な報酬が定義できる
- 逐次的な意思決定

### 向いていない問題
- 即座に正解が分かる分類問題
- 学習コストが許されない環境

## G検定ひっかけポイント（最重要）
### ① 報酬 vs 価値
- ❌ ネットワークが報酬を予測する  
- ⭕ ネットワークは **価値を推定**、報酬は **環境が与える**

### ② 単一エージェント vs マルチエージェント
- 単一：DQN / AlphaGo
- マルチ：AlphaStar

### ③ 学習対象で切る
- Q値 → DQN
- 方策 → Policy Gradient
- 両方 → Actor-Critic

---

## 手法まとめ（超重要）

### DQN（Deep Q-Network）
- 学習対象：**行動価値 Q(s,a)**
- 特徴：
  - 離散行動向き
  - Q値最大の行動を選択
- 派生：
  - Double DQN
  - Dueling DQN

---

### Policy Gradient
- 学習対象：**方策（行動確率）**
- 特徴：
  - 連続行動に強い
  - 学習が不安定

---

### Actor-Critic
- Actor：方策を学習
- Critic：価値を学習
- 特徴：
  - 安定性が高い
  - 実用でよく使われる

---

### デュエリングネットワーク
- 状態価値 V とアドバンテージ A を分離
- Q値を計算する構造
- **報酬は扱わない**

## 代表例まとめ（即答用）

| 用語 | 内容 |
|---|---|
| AlphaGo | 単一エージェント強化学習 |
| AlphaStar | マルチエージェント強化学習 |
| DQN | 価値ベース手法 |
| Policy Gradient | 方策ベース手法 |
| Actor-Critic | 価値＋方策 |

## まとめ（試験直前用）
- 強化学習＝報酬最大化
- 報酬は環境、価値は推定
- Q値 → DQN
- 方策 → Policy Gradient
- 両方 → Actor-Critic
- 複数同時 → マルチエージェント
- **迷ったら「何を学習しているか」で切る**

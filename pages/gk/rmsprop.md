---
layout: page
title: RMSpropとは？G検定対策
permalink: /gk/rmsprop/
tags: [gk, neural_network]
---

## まず結論

RMSprop（Root Mean Square Propagation）は、**AdaGradの欠点を改良し、過去の勾配を徐々に忘れながら学習率を調整する最適化手法**です。
G検定では、**「過去の勾配をすべては使わない」点**が問われます。

## 直感的な説明

RMSpropは、

> 「最近の動きを重視して、古い情報は忘れる」

という考え方の最適化手法です。

* AdaGrad：過去すべてを記憶 → 学習率が極端に小さくなる
* RMSprop：**過去を少しずつ忘れる** → 学習が止まりにくい

この違いが、RMSprop最大のポイントです。

## 定義・仕組み

RMSpropは、AdaGradを改良した最適化手法です。

ポイントは次の1点です。

* **勾配の二乗和を指数移動平均で管理する**

イメージ式は次の通りです。

```
G_t = \alpha G_{t-1} + (1-\alpha) g_t^2
```

* `g_t`：時刻tの勾配
* `G_t`：勾配二乗の移動平均
* `\alpha`：減衰率（0.9など）

👉 **古い勾配の影響は徐々に小さくなる** のが特徴です。

## いつ使う？（得意・不得意）

**得意な点**

* 非定常な問題（勾配が変化しやすい）
* AdaGradで学習が止まってしまう場合

**注意点**

* ハイパーパラメータ（減衰率）の設定が必要
* 学習率自体は別途設定する

## G検定ひっかけポイント

G検定では、次の混同が頻出です。

* **AdaGradとの違い**
* **Adamとの違い**

選択肢の判断基準は次の通りです。

* 「過去すべての勾配」→ AdaGrad
* 「過去を忘れる（移動平均）」→ **RMSprop**
* 「モメンタムも使う」→ Adam

## まとめ（試験直前用）

* RMSpropは**最適化手法**
* AdaGradの改良版
* 勾配の二乗を**指数移動平均**で管理
* 過去を忘れるのが特徴
* 学習が止まりにくい

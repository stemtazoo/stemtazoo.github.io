---
layout: page
title: データ窃取攻撃（Data Extraction Attack）とは？【G検定対策】
permalink: /gk/data-extraction-attack/
tags: [gk, security, privacy]
---

## まず結論
**データ窃取攻撃（Data Extraction Attack）**とは、学習済みAIモデルの出力結果を分析することで、**学習データの一部や特徴を推定・抽出しようとする攻撃**である。  
G検定では「学習データが漏れる攻撃はどれか？」という形で問われることが多い。

---

## 直感的な説明
AIに何度も質問を投げ続けると、  
「このAI、昔こういうデータで学習してない？」  
と**中身を逆算できてしまう**ことがある。

たとえば、
- 医療AIに大量の質問を投げる  
- 回答の傾向を細かく分析する  

すると、**特定の患者データや特徴が学習データに含まれていたかのような情報**が推測できてしまう。  
これがデータ窃取攻撃のイメージ。

---

## 定義・仕組み
**データ窃取攻撃（Data Extraction Attack）**とは、

> 学習済みモデルへの問い合わせ結果（出力）を利用して、  
> **学習データの一部や統計的特徴を復元・推定する攻撃**

を指す。

ポイントは以下の通り。

- モデル内部（重み）を直接盗むわけではない  
- API や推論結果だけを使う  
- 個々のデータ、またはデータ分布の特徴を狙う  

G検定では  
「**学習データの漏えいリスク**」  
という文脈で登場する。

---

## いつ使う？（得意・不得意）
### 得意な状況
- 外部に公開されたAPIモデル
- 回答回数に制限がない
- 出力が詳細（確率・スコアなど）に返ってくる場合

### 注意・不得意な状況
- 出力が単純（ラベルのみ）
- ノイズ付与や差分プライバシーを導入しているモデル
- 問い合わせ制限が厳しい場合

👉 **セキュリティ設計が甘いほど成立しやすい攻撃**。

---

## G検定ひっかけポイント
G検定では、**似た攻撃手法との違い**を混同させてくる。

### よくある混同
- ❌ **メンバーシップ推論攻撃**  
  → 「そのデータが学習に使われたか？」を判定する  
- ❌ **モデル窃取攻撃**  
  → モデル構造やパラメータを真似して再構築する  
- ❌ **推論最適化攻撃**  
  → 一般的な用語ではなく不適切

### 判断基準（超重要）
- **学習データそのもの／特徴を抜こうとしている？**  
  → ✅ データ窃取攻撃  
- **含まれていたかどうかの判定だけ？**  
  → メンバーシップ推論攻撃  
- **モデルをコピー？**  
  → モデル窃取攻撃  

選択肢に  
>「学習データの一部や特徴を推定・抽出」  
と書いてあったら、**即データ窃取攻撃**。

---

## まとめ（試験直前用）
- データ窃取攻撃＝**学習データを抜く攻撃**
- 出力結果を大量分析して推定する
- モデル内部を盗むわけではない
- メンバーシップ推論・モデル窃取と混同注意
- 「学習データが漏れる」→ データ窃取攻撃

---
layout: page
title: AdaBoundとは？（学習率に上下限を持つ最適化手法）【G検定対策】
permalink: /gk/adabound/
tags: [gk]
---

## まず結論

AdaBoundとは、**Adamのような高速収束と、SGDのような安定した汎化性能を両立するために、学習率に上限・下限（境界）を設ける最適化手法**であり、G検定では「学習率を制御する特徴」を正しく説明できるかが問われる。

## 直感的な説明

AdaBoundは、

> 最初は自由に学ばせて、
> だんだん“暴れない範囲”に縛っていく

というイメージです。

* 学習初期：

  * Adamのように大きく・速く更新
* 学習後半：

  * 学習率の範囲を狭め、SGDに近づく

これにより、

* 速く学習できる
* 途中で不安定になりにくい

というメリットがあります。

## 定義・仕組み

AdaBoundはAdam系の最適化手法で、

* 勾配の1次モーメント・2次モーメントを使用
* 学習率に**動的な上限・下限**を設定

します。

学習が進むにつれて、

* 学習率の上限は下がる
* 学習率の下限は上がる

結果として、

> 学習率が一定範囲に収束

し、SGDに近い挙動になります。

## いつ使う？（得意・不得意）

### 得意なケース

* 深層学習モデルの学習
* Adamの不安定さを避けたい場合
* ハイパーパラメータ調整を簡単にしたい場合

### 注意点

* 常にAdamより優れるわけではない
* 問題設定によっては効果が限定的

## G検定ひっかけポイント

G検定では、**Adam・RMSProp・AdaDeltaとの違い**が狙われます。

### よくある混同

* AdaBound＝学習率を自動調整するだけ → ✕
* AdaDeltaも学習率に上下限がある → ✕

### 正誤を切る判断基準

* **学習率に上限・下限？** → AdaBound
* **徐々にSGDに近づく？** → AdaBound
* **単に適応的学習率？** → Adam系全般

## まとめ（試験直前用）

* AdaBoundは学習率に境界を設ける
* Adamの速さ＋SGDの安定性
* 学習が進むとSGD的挙動になる
* 学習率の上下限がキーワード
* G検定ではAdamとの違いに注意

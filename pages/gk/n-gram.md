---
layout: page
title: N-gramとは？文字・単語を分割する基本手法【NLP基礎｜G検定対策】
permalink: /gk/n-gram/
tags: [gk, nlp]
---

## まず結論
- **N-gramとは、文章を連続するN個の文字または単語のまとまりに分割する手法**である。
- G検定では「**文章を任意の文字数（長さ）で分割する**」という定義が問われる。

## 直感的な説明
N-gramはとてもシンプルで、

> **文章を“n文字ずつ（またはn単語ずつ）スライドしながら切る”**

方法です。

例：「AIを学ぶ」

- 1-gram：  
  → A / I / を / 学 / ぶ
- 2-gram：  
  → AI / Iを / を学 / 学ぶ
- 3-gram：  
  → AIを / Iを学 / を学ぶ

👉 **nを変えるだけで粒度が変わる**のが特徴です。

## 定義・仕組み
### 定義
- 文章を **連続するN個の要素**に分割する手法
- 要素は以下のどちらか
  - **文字N-gram**
  - **単語N-gram**

### 仕組みのポイント
- 順序を保持する
- 局所的な文脈を捉えられる
- Nが大きいほど文脈は広がるが、次元が増える

## いつ使う？（得意・不得意）
### 得意な場面
- 文書分類
- 言語モデルの基礎
- スペル補正
- 日本語処理（形態素解析が不要な場合）

### 注意点
- Nを大きくすると特徴量が爆発
- 長距離依存関係は扱えない
- 意味理解は弱い

## G検定ひっかけポイント
ここは定番です。

### よくある誤解
- ❌「単語の意味をベクトル化する手法」
- ❌「文書を単語の集合として扱う」
- ❌「ニューラルネットワークのモデル」
- ❌「クラスタリング結果の可視化」

### 正しい判断基準
- **任意の文字数・単語数で分割 → N-gram**
- **順序を考慮しない → BoW**
- **単語分散表現 → Word2Vec / CBOW**
- **階層クラスタリング → デンドログラム**

問題文に  
「任意の文字数」「N個ずつ分割」  
とあれば **N-gram**。

## BoW / CBOW との違い（超重要）
- **N-gram**：分割方法  
- **BoW**：数え方（順序なし）  
- **CBOW**：単語ベクトル学習モデル  

👉 役割が全く違う。

## まとめ（試験直前用）
- N-gram＝N個ずつ切る
- 文字でも単語でもOK
- 順序を保つ
- 分割手法であってモデルではない
- 「任意の文字数」が決定打

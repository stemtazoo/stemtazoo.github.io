---
layout: page
title: KLダイバージェンス（Kullback-Leibler Divergence）とは？【G検定対策】
permalink: /gk/kl-divergence/
tags: [gk, metrics, probability]
gk_section: ディープラーニングの概要/誤差関数
gk_order: 4
---

## まず結論
- **KLダイバージェンス（Kullback-Leibler Divergence）とは、2つの確率分布の「ズレ」を表す指標**である。  
- G検定では「**相互情報量との違い**」や「**0になる条件**」が頻出で問われる。

---

## 直感的な説明
KLダイバージェンスは、  
**「本当の分布」と「近似した分布」が、どれだけ食い違っているか」**  
を測る指標です。

たとえば、
- 本当のテストの出題傾向（真の分布）
- 自分の予想した出題傾向（近似分布）

この2つがどれだけズレているかを数値で表します。

👉 **完全に同じ分布ならズレは0**  
👉 少しでも違えば **0より大きくなる**

---

## 定義・仕組み
KLダイバージェンス（KL Divergence）は、

- ある確率分布 **P** を  
- 別の確率分布 **Q** で近似したときの  
**情報の損失量** を表す指標

という位置づけです。

### 性質（G検定重要）
- KL ≥ 0  
- **KL = 0 ⇔ P と Q が完全に同一の分布**
- **対称ではない**
  - KL(P || Q) ≠ KL(Q || P)

※ 数式は試験ではほぼ不要  
→ **意味と性質を覚えるのが最優先**

---

## いつ使う？（得意・不得意）
### 得意な場面
- 確率モデルの評価
- 機械学習の損失関数（変分推論、VAEなど）
- 分布の近さを評価したいとき

### 注意が必要な点
- 非対称（距離ではない）
- 分布が完全一致しない限り 0 にならない
- 「変数同士の関係性」を見る指標ではない

---

## G検定ひっかけポイント
ここが **相互情報量との最大の混同ポイント** です。

### ひっかけ①  
**「KLダイバージェンスが0 → XとYは独立」**  
❌ 不正解  

→ KLは  
**分布PとQの比較**  
であり、  
**変数間の独立性は扱わない**

---

### ひっかけ②  
**「相互情報量とKLダイバージェンスは同じもの」**  
❌ 不正解  

- 相互情報量：  
  - 2つの確率変数の依存関係  
- KLダイバージェンス：  
  - 2つの確率分布のズレ  

👉 **対象が違う**

---

### ひっかけ③  
選択肢に  
- 「分布が同じとき0」  
とあれば → **KL**  
- 「独立のとき0」  
とあれば → **相互情報量**

この切り分けが超重要。

---

## まとめ（試験直前用）
- KLダイバージェンスは「2つの確率分布のズレ」を測る指標  
- **KL = 0 ⇔ 分布が完全に同じ**  
- 非対称なので距離ではない  
- 相互情報量とは目的・意味が違う  

👉 **「分布 vs 変数」どちらを比べているかで判断する**

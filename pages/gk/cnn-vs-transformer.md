---
layout: page
title: CNN vs Transformer（G検定対策）
permalink: /gk/cnn-vs-transformer/
tags: [gk, cnn, transformer, cheatsheet]
---

## まず結論

* **G検定で計算問題が出るのはCNN、出ないのがTransformer**
* CNNは「局所構造」、Transformerは「全体関係」を扱う
* 両者は競合ではなく **役割が違う**

---

## 直感的な違い

### CNN

> 近くを見るのが得意

* 画像の一部一部を少しずつ見る
* フィルタを滑らせて特徴抽出

---

### Transformer

> 全体を一気に見るのが得意

* 入力全体の関係性を同時に考慮
* どこを見るかを **Attention** で決める

---

## 構造の違い（ざっくり）

| 観点    | CNN            | Transformer          |
| ----- | -------------- | -------------------- |
| 基本構造  | Conv + Pooling | Self-Attention + FFN |
| 見る範囲  | 局所             | **全体**               |
| パラメータ | カーネル依存         | 行列サイズ依存              |
| 計算問題  | **出る**         | **ほぼ出ない**            |

---

## なぜCNNは計算問題が出る？

* カーネルサイズ
* ストライド
* パディング

👉 **数字が明確で式にできる**

---

## なぜTransformerは計算問題が出ない？

* Attentionは

  * 行列演算が中心
  * サイズは問題文に出にくい

👉 **概念理解が中心**

---

## 得意分野の違い

| タスク       | CNN | Transformer |
| --------- | --- | ----------- |
| 画像分類      | ◎   | ○           |
| 物体検出      | ◎   | ○           |
| セグメンテーション | ◎   | ○           |
| NLP       | △   | ◎           |
| 長距離依存     | △   | ◎           |

---

## G検定ひっかけポイント

* ❌「TransformerはCNNの改良版」→ **誤り**
* ❌「CNNでも全体関係が一瞬で分かる」→ **誤り**
* ✅ CNN：局所特徴
* ✅ Transformer：全体関係

---

## 試験直前の一言

* 計算する → **CNN**
* 概念を聞く → **Transformer**

---

## まとめ

* CNNとTransformerは役割が違う
* G検定では **使い分け理解** が最重要
* 計算問題が出るかどうかで即切り

---
layout: page
title: Attention（注意機構）
permalink: /gk/attention/
tags: [gk, neural_network, attention]
---

## まず結論

* **Attentionは「どこに注目するか」を学習する仕組み**
* 系列全体を見渡し、重要な部分に重みを置く
* **長期依存問題を緩和** し、RNNの弱点を補う

---

## 直感的な説明

Attentionは、

> 「全部を同じ強さで覚えるのではなく、重要なところだけ強く見る」

という考え方です。

文章を読むときに、

* 大事な単語に注目する
* あまり重要でない部分は流し読みする

のと同じイメージです。

---

## 定義・仕組み

### Attentionの基本アイデア

* 入力全体の中から
* **重要度（重み）** を計算
* 重み付き和として情報を集約

これにより、
**必要な情報を直接参照** できます。

---

### Query / Key / Value

Attentionは、次の3つで説明されます。

* **Query（Q）**：何を探しているか
* **Key（K）**：各情報の特徴
* **Value（V）**：実際の情報

> Q と K の類似度で、V の重みが決まる

---

### Self-Attention

* Q / K / V を **同じ系列** から作る
* 系列内の単語同士の関係を捉える

これが、
**Transformerの中核技術** です。

---

## いつ使う？（得意・不得意）

### 得意なこと

* 長距離の依存関係の把握
* 機械翻訳・文章理解
* 並列計算が可能

### 注意点

* 計算量が増えやすい
* 長い系列ではメモリ消費が大きい

---

## G検定ひっかけポイント

* ❌「AttentionはRNNの一種」→ **誤り**
* ❌「Attentionは過去情報だけを見る」→ **誤り**
* ✅ 系列全体を参照できる
* ✅ Self-Attentionは同一系列内の関係

---

## まとめ（試験直前用）

* Attentionは **注目点を学習**
* Q / K / V が基本
* 長期依存に強い

👉 次は **Transformer（概要）** に進むと全体像が完成します。

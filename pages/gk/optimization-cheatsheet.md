---
layout: page
title: 最適化手法まとめ（チートシート）
permalink: /gk/optimization-cheatsheet/
tags: [gk, neural_network, optimization, cheatsheet]
gk_section: ディープラーニングの概要/最適化手法
gk_order: 13
---

## まず結論（これだけ見ればOK）

| キーワードが出たら      | 答え             |
| -------------- | -------------- |
| 慣性・移動平均・1990年代 | モーメンタム         |
| 学習率を自動調整       | RMSProp / Adam |
| モーメント・最も使われる   | Adam           |
| 一番シンプル         | 最急降下法          |
| データをランダムに使う    | SGD            |
| 振動を抑えたい        | モーメンタム         |
| とりあえず強い        | Adam           |

---

## 直感マップ（G検定用）

```
最急降下法
   ↓（ミニバッチ）
SGD
   ↓（慣性）
モーメンタム
   ↓（学習率自動調整）
Adam
```

👉 **Adamは突然現れたわけではない**
👉 進化の積み重ね

---

## 各手法の役割（超要点）

### 最急降下法（Batch Gradient Descent）

* 全データを使って更新
* 安定だが遅い
* 実用性は低い

---

### 確率的勾配降下法（SGD）

* データをランダムに使用
* 計算は速いが不安定
* 振動しやすい

---

### モーメンタム

* **過去の勾配の移動平均** を使う
* 慣性を導入して振動を抑制
* **1990年代に提案**

---

### RMSProp

* 勾配の **二乗の移動平均** を利用
* 勾配が大きい → 学習率を下げる
* 勾配が小さい → 学習率を上げる

---

### Adam（最重要）

* **モーメンタム + RMSProp**
* 1次モーメント：平均（勢い）
* 2次モーメント：分散（スケール調整）
* デフォルト設定でも強い

---

## G検定ひっかけパターン集

### ① 「慣性」という言葉が出たら

* ❌ SGD
* ❌ 最急降下法
* ✅ **モーメンタム**

---

### ② 「Adamは全く新しい手法」

* ❌ 新理論
* ✅ **既存手法の組み合わせ**

---

### ③ 「学習率を自動で変える」

* ❌ SGD
* ❌ モーメンタム
* ✅ **RMSProp / Adam**

---

### ④ 「最も広く使われている」

* ❌ SGD
* ❌ RMSProp
* ✅ **Adam**

---

## 試験直前の一行暗記

* SGD：**ランダム**
* モーメンタム：**慣性**
* RMSProp：**学習率調整**
* Adam：**全部入り**

---

## まとめ（ここだけ覚える）

* Adam = **モーメンタム + RMSProp**
* 慣性 → モーメンタム
* 自動調整 → RMSProp / Adam
* G検定は **単語対応ゲー**

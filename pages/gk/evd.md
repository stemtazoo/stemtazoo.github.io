---
layout: page
title: 固有値分解（EVD, Eigenvalue Decomposition）とは？G検定対策
permalink: /gk/evd/
tags: [gk, linear_algebra]
gk_section: 機械学習の概要/代表的な手法/教師なし学習
gk_order: 13
---

## まず結論

* **固有値分解（EVD, Eigenvalue Decomposition）**とは、正方行列を**固有値と固有ベクトル**に分解し、行列の本質的な性質を理解・利用するための線形代数手法。
* G検定では**SVDやPCAとの違い（使える行列の条件・役割）**が問われる。

## 直感的な説明

* 行列を「向きを変える力」と考えると、

  * ある特別な方向（固有ベクトル）では
  * 向きは変わらず、**伸び縮み（固有値）だけ**が起こります。
* 固有値分解は、
  👉 **行列が本当にやっている“伸ばす方向と強さ”を取り出す**方法です。

## 定義・仕組み

* 固有値分解は、

  * 正方行列 A を
  * 固有ベクトル行列と固有値行列に分解する操作です。

* 成り立つ条件：

  * **正方行列であること**
  * （多くの場合）対角化可能であること

* 特徴：

  * 固有値がスケールの大きさ
  * 固有ベクトルが方向を表す

## いつ使う？（得意・不得意）

### 使われる場面（得意）

* 共分散行列の解析
* PCAの理論的理解
* 線形変換の性質解析

### 注意点・不得意

* 非正方行列には使えない
* 数値計算の安定性でSVDに劣る場合がある
* 非線形構造は扱えない

## G検定ひっかけポイント

* よくある誤解：

  * ❌ 任意の行列に適用できる
  * ❌ 非線形次元削減手法

* SVDとの違い：

  * **EVD：正方行列のみ**
  * **SVD：任意形状の行列OK**

* 判断基準：

  * **共分散行列 → 固有値分解**
  * **元データ行列 → SVD**

## まとめ（試験直前用）

* 固有値分解＝正方行列を分解
* 固有値＝強さ、固有ベクトル＝方向
* PCA理論の基礎
* 任意行列には使えない
* SVDとの適用条件の違いが重要

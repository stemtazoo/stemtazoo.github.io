---
layout: page
title: オンポリシー vs オフポリシーとは？（DQN系の位置づけ）【G検定対策】
permalink: /gk/on-policy-off-policy/
tags: [gk, neural_network]
gk_section: ディープラーニングの応用例/深層強化学習
gk_order: 14
---

## まず結論
- **オンポリシーは「今使っている方策で集めた経験だけで学習する」方式**。
- **オフポリシーは「別の方策で集めた過去の経験も使って学習する」方式**。
- **DQN 系はオフポリシー学習**である点が、G検定の重要ポイント。

## 直感的な説明
- オンポリシー：
  - 今のやり方で練習
  - 昨日の練習メモは捨てる
- オフポリシー：
  - 今のやり方で練習
  - **過去のノートも引っ張り出して復習**
- イメージ：
  - オンポリシー：実践重視
  - オフポリシー：復習重視

## 定義・仕組み
### オンポリシー（On-policy）
- 学習対象：
  - **現在の方策（policy）**
- 使用する経験：
  - その方策で集めたデータのみ
- 特徴：
  - 学習は安定しやすい
  - データ効率は低い
- 代表例：
  - Policy Gradient
  - REINFORCE
  - A2C / A3C（基本はオンポリシー）

### オフポリシー（Off-policy）
- 学習対象：
  - **目標方策**
- 使用する経験：
  - 過去の方策や別方策で集めたデータも利用
- 特徴：
  - データ効率が高い
  - 学習が不安定になりやすい
- 代表例：
  - **DQN**
  - Double DQN
  - Prioritized Experience Replay
  - **APE-X**

## いつ使う？（得意・不得意）
**オンポリシーが向いている**
- 安定性を重視したい
- 方策を直接学習したい場合
- 分布ずれを避けたいとき

**オフポリシーが向いている**
- 経験を再利用したい
- 大量データ・分散学習
- サンプル効率を上げたい場合

## G検定ひっかけポイント
- **Experience Replay が出たらオフポリシー**
- よくある誤解：
  - ❌ DQN はオンポリシー
  - ❌ オフポリシーは古い手法
- 正しい判断基準：
  - 「Replay」「過去の経験」→ **オフポリシー**
  - 「今の方策だけ」→ **オンポリシー**
- 選択肢の即断：
  - DQN / APE-X → **オフポリシー**
  - Policy Gradient → **オンポリシー**

## まとめ（試験直前用）
- オンポリシー：今の方策の経験だけで学習
- オフポリシー：過去・別方策の経験も使う
- DQN 系は **オフポリシー**
- Replay がキーワード
- **「経験を使い回すか？」で判断**

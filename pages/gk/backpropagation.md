---
layout: page
title: 誤差逆伝播法（Backpropagation）
permalink: /gk/backpropagation/
tags: [gk, neural_network, backprop]
---

## まず結論

* **誤差逆伝播法はニューラルネットワークの学習を支える中核アルゴリズム**
* **出力の誤差を後ろから前へ伝えて重みを更新する**
* **連鎖律（chain rule）に基づいて勾配を計算する**

---

## 直感的な説明

誤差逆伝播法は、

> 「結果がズレた原因を、あとからさかのぼって探す方法」

です。

* 最後の出力が間違っていた
* どの重みが、どれくらい影響したのかを
* 後ろ（出力層）から前（入力層）へ順に調べる

この考え方により、
**全ての重みを効率よく調整**できます。

---

## 定義・仕組み

### 学習の全体の流れ

ニューラルネットワークの学習は、次の3ステップを繰り返します。

1. **順伝播（Forward Propagation）**
   入力から出力を計算する
2. **誤差の計算**
   出力と正解との差（損失）を求める
3. **逆伝播（Backpropagation）**
   誤差を使って重みを更新する

---

### 逆伝播で何をしている？

* 出力層での誤差を計算
* その誤差を、

  * 活性化関数
  * 重み
  * 前の層

へと順番に伝える

このとき使われるのが **微分と連鎖律** です。

> 「この重みを少し変えたら、誤差はどれだけ変わるか？」

を計算しています。

---

### 重みの更新

重みは次のように更新されます。

[
w := w - \eta \frac{\partial L}{\partial w}
]

* (L)：損失関数
* (\eta)：学習率

誤差が小さくなる方向に、
少しずつ重みを動かします。

---

## いつ使う？（得意・不得意）

### 得意なこと

* 多層ニューラルネットワークの学習
* 大量のパラメータを効率よく最適化

### 苦手・注意点

* 勾配消失・勾配爆発が起こることがある
* 活性化関数や学習率の選び方に影響されやすい

> この問題への対策が、ReLU・正則化・最適化手法です。

---

## G検定ひっかけポイント

* ❌「誤差は前から後ろへ伝える」→ **誤り**
* ❌「微分を使わない」→ **誤り**
* ✅ **連鎖律を使って後ろから前へ伝える**
* ✅ 誤差逆伝播法 + 勾配降下法はセット
* ✅ 活性化関数は微分可能である必要がある

---

## まとめ（試験直前用）

* 誤差逆伝播法は **NN学習の中核**
* **順伝播 → 誤差計算 → 逆伝播** の流れ
* 連鎖律で勾配を計算
* 勾配問題への対策が重要

👉 次は **最適化手法（SGD / Adam など）** を理解すると学習の全体像が完成します。

---
layout: page
title: 差分プライバシー（Differential Privacy）とは？【G検定対策】
permalink: /gk/differential-privacy/
tags: [gk, security, privacy]
gk_section: AIの法律と倫理/AIの法律と倫理
gk_order: 32
---

## まず結論
**差分プライバシー（Differential Privacy）**とは、  
個々のデータが学習データに含まれていてもいなくても、**モデルの出力結果がほとんど変わらないようにするプライバシー保護の考え方**である。  
G検定では「プライバシー攻撃への代表的な防御策」として問われる。

---

## 直感的な説明
差分プライバシーは、  
**「1人分のデータが入っても、結果がバレないようにする」**仕組み。

たとえば、
- 100人のデータで学習したモデル
- そのうち1人を抜いたモデル

この2つのモデルの出力が  
**ほぼ同じになるようにノイズを加える**。

つまり、
> 「この人のデータが使われていたかどうか、分からない」

状態を作るのが差分プライバシー。

---

## 定義・仕組み
**差分プライバシー（Differential Privacy）**とは、

> 学習データに **特定の個人データが含まれていてもいなくても**、  
> モデルや分析結果の出力が統計的にほとんど変わらないことを保証する枠組み

を指す。

仕組みのポイント：

- 出力や学習過程に **ランダムなノイズ**を加える
- 個々のデータの影響を小さくする
- 数式的には ε（イプシロン）でプライバシー強度を表す（※暗記不要）

G検定では  
**「ノイズを加えることで個人情報を守る」**  
という理解で十分。

---

## いつ使う？（得意・不得意）
### 得意な場面
- 個人情報を含むデータ（医療・位置情報・購買履歴など）
- API公開モデル
- メンバーシップ推論攻撃への対策

### 注意が必要な点
- ノイズを増やしすぎると精度が下がる
- 完全な防御ではない（トレードオフあり）
- モデル設計が複雑になる

👉 **プライバシーと精度はトレードオフ**、は頻出考え方。

---

## G検定ひっかけポイント
### よくある誤解
- ❌ データを暗号化する技術  
- ❌ モデルを非公開にすること  
- ❌ データを削除すること  

👉 どれも **差分プライバシーではない**。

### 選択肢での判断基準
- 「ノイズを加えて個人の影響を分からなくする」  
  → **差分プライバシー**
- 「学習データが含まれていたか分からない」  
  → メンバーシップ推論攻撃への防御
- 「精度とプライバシーのトレードオフ」  
  → 差分プライバシーの説明

G検定では  
**防御策＝差分プライバシー** と結びつける。

---

## まとめ（試験直前用）
- 差分プライバシー＝**個人の影響を隠す仕組み**
- ノイズを加えて出力を安定させる
- メンバーシップ推論・データ窃取への対策
- 精度とのトレードオフがある
- 「入っていても分からない」→ 差分プライバシー

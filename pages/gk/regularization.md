---
layout: page
title: 正則化・過学習対策まとめ
permalink: /gk/regularization/
tags: [gk, neural_network, regularization]
---

## まず結論

* **正則化は過学習を防ぐための仕組み**
* モデルを「複雑にしすぎない」よう制御する
* G検定では **手法名と目的の対応** が頻出

---

## 直感的な説明

過学習とは、

> 「学習データは完璧だが、新しいデータでは当たらない状態」

です。

正則化は、

* 無理に細かく覚えすぎない
* 本質的な特徴だけを使う

ようにモデルへ **制約をかける** 考え方です。

---

## 定義・仕組み

### 過学習が起きる理由

* モデルが複雑すぎる
* 学習データが少ない
* ノイズまで学習してしまう

その結果、

* 学習誤差：小さい
* テスト誤差：大きい

という状態になります。

---

## 主な正則化・過学習対策

### L1正則化（Lasso）

* 重みの絶対値の和をペナルティとして加える

**特徴**

* 不要な重みを0にしやすい
* 特徴選択の効果

---

### L2正則化（Ridge）

* 重みの二乗和をペナルティとして加える

**特徴**

* 重みを全体的に小さくする
* 学習が安定しやすい

---

### Dropout

* 学習時にランダムでノードを無効化

**特徴**

* 特定のノードへの依存を防ぐ
* NNで非常によく使われる

---

### Early Stopping

* 検証誤差が悪化した時点で学習を停止

**特徴**

* 学習回数を制御
* 実装が簡単

---

### Data Augmentation

* 学習データを人工的に増やす

**特徴**

* 特に画像認識で有効
* モデルではなくデータ側の対策

---

## いつ使う？（得意・不得意）

| 手法                | 主な目的     |
| ----------------- | -------- |
| L1                | 特徴選択     |
| L2                | 重み抑制     |
| Dropout           | NNの過学習防止 |
| Early Stopping    | 学習回数制御   |
| Data Augmentation | データ不足対策  |

---

## G検定ひっかけポイント

* ❌「正則化は学習速度を上げる手法」→ **誤り**
* ❌「Dropoutは推論時もノードを落とす」→ **誤り**
* ✅ 正則化の目的は **汎化性能向上**
* ✅ L1とL2の違いを言葉で説明できる

---

## まとめ（試験直前用）

* 正則化 = **過学習対策**
* L1 / L2 は重みへのペナルティ
* Dropout は NN 定番手法
* Early Stopping は学習回数制御

👉 次は **バッチ学習・エポック・ミニバッチ** を整理すると用語が完成します。

---
layout: page
title: 破壊的忘却（Catastrophic Forgetting）とは？【連続学習の落とし穴｜G検定対策】
permalink: /gk/catastrophic-forgetting/
tags: [gk, neural_network]
---

## まず結論
- **破壊的忘却（Catastrophic Forgetting）**とは、**新しいタスクを学習すると、過去に学習した知識を急激に失ってしまう現象**である。
- G検定では「**連続学習（逐次学習）で起きる問題**」であることが問われる。

## 直感的な説明
破壊的忘却は、  
**「新しいことを覚えたら、前に覚えたことを全部忘れてしまうAI」**です。

たとえば、
- タスクAを学習 → うまくできる
- 次にタスクBを学習 → Bはできる
- でも **Aが急にできなくなる**

👉 人間なら「復習」で防げますが、  
ニューラルネットワークは **上書き学習** しやすいため、これが起こります。

## 定義・仕組み
破壊的忘却は、主に **ニューラルネットワークの重み更新** が原因です。

- 新しいタスクを学習すると
- 既存の重みが大きく更新され
- 過去タスクに重要だった情報が壊れる

特に問題になるのは：
- **連続学習（Continual Learning）**
- **逐次タスク学習**
- **マルチタスクを順番に学ぶ場合**

重要：
- データのノイズが原因ではない
- モデルの容量不足だけが原因でもない

## いつ使う？（得意・不得意）
### 問題になる場面
- ロボットが次々に新しい作業を覚える
- AIが継続的にタスク追加される
- 過去データをすべて保存できない状況

### 問題になりにくい場面
- 全タスクをまとめて学習（同時学習）
- マルチタスク学習（最初から複数タスク）

## G検定ひっかけポイント
G検定では、次の混同を狙ってきます。

### よくある誤解
- ❌「性能が向上し続ける現象」
- ❌「ノイズによる性能劣化」
- ❌「理想的な転移学習の状態」

### 正しい判断基準
- **新しい学習が原因で過去を忘れる → 破壊的忘却**
- **性能向上 → 正の転移（Positive Transfer）**
- **ノイズが原因 → 別問題**

選択肢に  
「新しいタスク」「過去の知識が失われる」  
があれば **破壊的忘却**。

## まとめ（試験直前用）
- 破壊的忘却＝新しい学習で過去を忘れる
- 連続学習で起きやすい
- ノイズや性能向上とは無関係
- マルチタスク同時学習とは対照的
- 「上書きされる」イメージで覚える

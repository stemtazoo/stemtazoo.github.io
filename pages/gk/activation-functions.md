---
layout: page
title: 活性化関数まとめ
permalink: /gk/activation-functions/
tags: [gk, neural_network, activation]
---

## まず結論

* **活性化関数はニューラルネットワークに非線形性を与える重要要素**
* 活性化関数がないと、層を重ねても **ただの線形モデル** になる
* G検定では **用途・特徴・欠点の対応関係** がよく問われる

---

## 直感的な説明

活性化関数は、

> 「入力された値を、そのまま出すか、変形して出すかを決めるフィルター」

のような存在です。

* 小さい値は無視する
* ある範囲に押し込める
* 強い反応だけを通す

といった役割を持ち、
これによって **複雑な判断** が可能になります。

---

## 定義・仕組み

ニューラルネットワークでは、各ノードで

[
y = f( \sum_i w_i x_i + b )
]

という計算が行われ、
この (f) が **活性化関数** です。

ここで重要なのは、

> **f が非線形関数であること**

です。

---

## 主な活性化関数

### ステップ関数（Step Function）

* 一定のしきい値で 0 / 1 を切り替える
* パーセプトロンで使用

**特徴**

* 実装は簡単

**欠点**

* 微分できない → 誤差逆伝播法が使えない

---

### シグモイド関数（Sigmoid）

* 出力範囲：0〜1
* 確率として解釈しやすい

**特徴**

* 出力がなめらか

**欠点**

* 勾配消失が起きやすい
* 深いネットワークには不向き

---

### tanh 関数

* 出力範囲：-1〜1
* Sigmoid を原点対称にした形

**特徴**

* 平均が0に近く、学習が安定しやすい

**欠点**

* 勾配消失は起こりうる

---

### ReLU（Rectified Linear Unit）

* 0以下は0、正の値はそのまま出力

**特徴**

* 計算が軽い
* 勾配消失が起きにくい
* 現在の主流

**欠点**

* **Dead ReLU 問題**（常に0になる）

---

### Leaky ReLU

* ReLU の改良版
* 負の領域にもわずかな傾きを持たせる

**特徴**

* Dead ReLU を軽減

---

### Softmax

* 出力を確率分布に変換
* 多クラス分類の出力層で使用

**特徴**

* 出力の合計が1

**注意点**

* 分類問題では **クロスエントロピー損失** とセットで使われる

---

## いつ使う？（得意・不得意）

| 使う場所      | 活性化関数             |
| --------- | ----------------- |
| パーセプトロン   | ステップ関数            |
| 中間層（一般）   | ReLU / Leaky ReLU |
| 出力層（二値分類） | Sigmoid           |
| 出力層（多クラス） | Softmax           |

---

## G検定ひっかけポイント

* ❌「活性化関数がなくても多層NNは非線形になる」→ **誤り**
* ❌「Sigmoidは勾配消失を起こさない」→ **誤り**
* ✅ **ReLUは勾配消失を起こしにくい**
* ✅ Softmax + クロスエントロピーは頻出セット
* ✅ ステップ関数は誤差逆伝播に不向き

---

## まとめ（試験直前用）

* 活性化関数は **非線形性のカギ**
* 現在の主流は **ReLU系**
* 出力層は **タスクに応じて選ぶ**
* Softmaxは多クラス分類で使用

👉 次は **誤差逆伝播法** を理解すると、学習の仕組みが完成します。

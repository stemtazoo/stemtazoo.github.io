---
layout: page
title: Dropoutとは？（暗黙的正則化）【G検定対策】
permalink: /gk/dropout/
tags: [gk, neural_network]
gk_section: ディープラーニングの概要/正則化
gk_order: 3
---

## まず結論

Dropoutは、**学習中に一部のニューロンをランダムに無効化することで過学習を防ぐ手法**で、G検定では「明示的な正則化（L1/L2）とは異なる暗黙的正則化」である点が問われる。

## 直感的な説明

Dropoutは、

> 毎回メンバーが少しずつ入れ替わるチームで練習する

ようなものです。

* いつも同じメンバー → 特定の人に依存する
* 毎回誰か休む → 全員が満遍なく力をつける

ニューラルネットワークでも同じで、

* 特定のニューロンに依存しすぎる
* 記憶丸暗記になる

のを防ぐ効果があります。

## 定義・仕組み

Dropoutは学習時に、

* 各ニューロンを**一定確率でランダムに無効化**

します。

重要なポイントは次の2つです。

* **学習時のみ適用**される
* **推論時（テスト時）は全ニューロンを使う**

これにより、

* 疑似的に多数のネットワークを学習した
* アンサンブルに近い効果

が得られます。

## いつ使う？（得意・不得意）

### 得意なケース

* 深層ニューラルネットワーク
* パラメータ数が多いモデル
* 過学習が起きやすい場合

### 注意点

* 適用しすぎると学習が進まない
* 小規模モデルでは効果が小さいこともある

## G検定ひっかけポイント

G検定では、**DropoutをL1/L2正則化と同列に扱わせる**選択肢が出ます。

### よくあるひっかけ

* Dropoutは損失関数に項を追加する → ✕
* Dropoutは重みを0にする → ✕

### 正誤を切る判断基準

* **重みに罰則項を加える？** → L1 / L2
* **学習中にニューロンを無効化？** → Dropout

## まとめ（試験直前用）

* Dropoutは過学習対策
* 学習中のみニューロンをランダム無効化
* 暗黙的正則化の代表例
* 推論時にはDropoutしない
* G検定ではL1/L2との違いを確認

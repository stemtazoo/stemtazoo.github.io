---
layout: page
title: GPT（自己回帰モデル）とは？G検定対策
permalink: /gk/gpt/
tags: [gk, nlp, transformer, attention]
---

## まず結論
- **GPT（Generative Pre-trained Transformer）**は、文を「左から右へ一方向」に処理し、次の単語を予測する自然言語生成モデルである。
- G検定では「**一方向（自己回帰）か双方向か**」「**生成向きか理解向きか**」がよく問われる。

## 直感的な説明
GPTは、**文章の続きを順番に書いていく作家**のようなモデルです。

たとえば  
> 「今日はとても天気がよいので」

という文があれば、  
GPTは **それまでに出てきた単語だけ**を見て、  
次に来そうな単語を予測します。

👉 **未来（右側）は見ない**  
👉 過去（左側）だけを頼りに続ける  
これが GPT の基本動作です。

## 定義・仕組み
- GPTは **Transformer の Decoder 構造**をベースにしたモデル
- **自己回帰（Autoregressive）モデル**であることが最大の特徴

### 自己回帰とは
- 「これまでの単語列」から「次の単語」を予測する方式
- 数式で書くと  
  次の単語 = 過去の単語に基づく確率最大の語

### GPTの事前学習
- ラベルの付いていない大量のテキストを使用
- 「次の単語を当てる」タスクのみで学習
- → **Masked Language Model や NSP は使わない**

## いつ使う？（得意・不得意）
### 得意
- 文章生成
- 要約
- チャットボット
- ストーリー作成

### 不得意・注意点
- 文全体の意味理解は BERT に劣る
- 文脈の誤解（もっともらしい嘘）を出すことがある
- 長文になると一貫性が崩れることがある

## G検定ひっかけポイント
G検定では、次の混同が頻出。

### ❌ 双方向Transformer
- 「文の前後を同時に考慮する」
→ **これはBERT**
→ GPTは **一方向のみ**

### ❌ Masked Language Model を使う
- GPTは単語を隠して当てない
→ **次の単語予測のみ**

### ⭕ 正しい判断基準
- **一方向・自己回帰** → GPT
- **文章生成が得意** → GPT
- **Decoderベース** → GPT
- **文理解特化** → BERT

## まとめ（試験直前用）
- GPTは **一方向（左→右）の自己回帰モデル**
- 次の単語を順番に予測して文章を生成する
- 事前学習は「次単語予測」のみ
- **双方向・MLMと書いてあったらGPTではない**
- 「生成特化」＝GPT と覚える

---
layout: page
title: CBOWモデルとは？（Word2Vec）【G検定対策】
permalink: /gk/cbow/
tags: [gk, nlp, embedding]
---

## まず結論
- **CBOW（Continuous Bag of Words）モデル**とは、  
  **周囲の単語（文脈語）を入力として、中央のターゲット単語を予測する Word2Vec の学習手法**である。  
- G検定では「**単語の順序を考慮するかどうか**」「**BoWとの違い**」が狙われる。

## 直感的な説明
CBOWは、  
**「まわりの単語をヒントに、真ん中の単語を当てる穴埋め問題」**。

例：
> 「I ___ to school」

- 入力（周辺単語）  
  →「I」「to」「school」
- 出力（予測する単語）  
  →「go」

👉 **文脈（Context） → 単語（Word）** が CBOW。

## 定義・仕組み
- 入力：ターゲット単語の**前後の単語（文脈語）**
- 出力：**中央のターゲット単語**
- 文脈語のベクトルは **平均（または和）** されて使用される
- **単語の並び順（順序）は考慮しない**

※  
「Bag of Words」とは  
👉 **出現単語の集合として扱い、順序を無視する**  
という意味。

## いつ使う？（得意・不得意）
### 得意
- 学習が高速
- 大規模コーパスに向いている
- 一般的な単語の分散表現学習

### 不得意・注意点
- 単語の順序情報は失われる
- 低頻度語の表現は弱くなりやすい

👉 高速・安定重視 → **CBOW**  
👉 希少語重視 → **Skip-gram**

## G検定ひっかけポイント（★重要）
ここは **今回の誤答ポイントをそのまま対策**👇

### ❌ CBOWは単語の並び順を重視する
- **誤り**
- 「Bag-of-Words」の考え方なので **順序は保持しない**
- 「並び順を考慮」するのは RNN / Transformer 系

### ❌ CBOWは文脈を考慮しない
- **誤り**
- **周辺単語（文脈）を使って予測するモデル**
- 「文脈を考慮しない」は **古典的BoW** の説明

### ❌ CBOWは文書全体を入力として分類する
- **誤り**
- 文書分類モデルではない（Doc2Vecなどとは別）

### ⭕ 正しい判断基準（試験用）
- 「周辺単語を入力」→ CBOW  
- 「中央単語を予測」→ CBOW  
- 「単語順序を無視」→ CBOW  

## Skip-gramとの最重要対比
| 観点 | CBOW | Skip-gram |
|---|---|---|
| 予測方向 | 文脈 → 単語 | 単語 → 文脈 |
| 学習速度 | 速い | 遅め |
| 低頻度語 | 弱い | 強い |
| 順序考慮 | しない | しない |

## まとめ（試験直前用）
- CBOW＝周辺単語から中央単語を予測
- Word2Vecの学習手法
- Bag-of-Wordsの考え方 → **順序は無視**
- 文脈は使うが、並び順は使わない
- **「周囲 → 中央」＝CBOW**
